{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "476e777378d1414393bd692ecd6b390e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91975aa4af38454c8d5d81516fcabde4",
              "IPY_MODEL_dc0a696a429a418d92b4dbe56abc5e3f",
              "IPY_MODEL_23ea085531944de2a1fcc9c99b10c1cd"
            ],
            "layout": "IPY_MODEL_2171ee4e23274680a133d84db77c5cb3"
          }
        },
        "91975aa4af38454c8d5d81516fcabde4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d0ab7c3a144dc28299d615f73c2b1e",
            "placeholder": "​",
            "style": "IPY_MODEL_86682fdc5e3c4a03b5a30b69bf3cf55d",
            "value": "config.json: 100%"
          }
        },
        "dc0a696a429a418d92b4dbe56abc5e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6600c64125e943c9863508681a0602d2",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d36147d028f44005a6cca219d63098d8",
            "value": 625
          }
        },
        "23ea085531944de2a1fcc9c99b10c1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff4019b67fd44bfa67f72015f26b547",
            "placeholder": "​",
            "style": "IPY_MODEL_8fca1823daa442ef9a719f1635757ce2",
            "value": " 625/625 [00:00&lt;00:00, 26.4kB/s]"
          }
        },
        "2171ee4e23274680a133d84db77c5cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d0ab7c3a144dc28299d615f73c2b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86682fdc5e3c4a03b5a30b69bf3cf55d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6600c64125e943c9863508681a0602d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d36147d028f44005a6cca219d63098d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fff4019b67fd44bfa67f72015f26b547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fca1823daa442ef9a719f1635757ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fccb115d53724681a6772b79a1aa4475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_facf57913e9f4453b56b156b61e3f7a5",
              "IPY_MODEL_3cf5f96a10ec44beadba9d4ef33e88f3",
              "IPY_MODEL_10e71e13f8274611a0c5685706212682"
            ],
            "layout": "IPY_MODEL_b21d11cc7e524963b555a20836ba83d9"
          }
        },
        "facf57913e9f4453b56b156b61e3f7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a4e4660c0ee43869f33a440354299b5",
            "placeholder": "​",
            "style": "IPY_MODEL_d4d556fd25ed4937941df83dd2b99fdb",
            "value": "model.safetensors: 100%"
          }
        },
        "3cf5f96a10ec44beadba9d4ef33e88f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75975b34e28e40ee98bd3fd4f9f5edc6",
            "max": 672247920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fa7de0e9f4c4bb5947745dc4a447933",
            "value": 672247920
          }
        },
        "10e71e13f8274611a0c5685706212682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_299e1805d142456aa3688a80283a5d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_3e987aa82a8143a7b52d1fd4038b829d",
            "value": " 672M/672M [00:12&lt;00:00, 93.4MB/s]"
          }
        },
        "b21d11cc7e524963b555a20836ba83d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4e4660c0ee43869f33a440354299b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d556fd25ed4937941df83dd2b99fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75975b34e28e40ee98bd3fd4f9f5edc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fa7de0e9f4c4bb5947745dc4a447933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "299e1805d142456aa3688a80283a5d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e987aa82a8143a7b52d1fd4038b829d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJVk6b1TvCbv",
        "outputId": "ce5cd0af-650e-4d4a-9bc0-8cb620519e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HmwpqzwnMyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2224bfa-df55-413c-e457-4b846a8d1d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'character-tokenizer'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 20 (delta 5), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (20/20), 5.89 KiB | 5.89 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/KuzmaKhrabrov/character-tokenizer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pynvml"
      ],
      "metadata": {
        "id": "hkVexA2nnRQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e08abd-3f85-4a76-a16c-052744444f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m660.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
      ],
      "metadata": {
        "id": "5uRJn1JovwRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Koziev/NLP_Datasets/raw/master/Stress/all_accents.zip\n",
        "!unzip all_accents.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMJqU4hCkQmm",
        "outputId": "96ac66ea-0095-4fb9-a62a-9bd4f540c549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-22 17:33:19--  https://github.com/Koziev/NLP_Datasets/raw/master/Stress/all_accents.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Koziev/NLP_Datasets/master/Stress/all_accents.zip [following]\n",
            "--2023-12-22 17:33:19--  https://raw.githubusercontent.com/Koziev/NLP_Datasets/master/Stress/all_accents.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10624775 (10M) [application/zip]\n",
            "Saving to: ‘all_accents.zip’\n",
            "\n",
            "all_accents.zip     100%[===================>]  10.13M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-12-22 17:33:20 (110 MB/s) - ‘all_accents.zip’ saved [10624775/10624775]\n",
            "\n",
            "Archive:  all_accents.zip\n",
            "  inflating: all_accents.tsv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('all_accents.tsv', sep = '\\t', header = None)"
      ],
      "metadata": {
        "id": "bKcIRRJfv7Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "pSzsKbGVv_6D",
        "outputId": "3c48dd7c-0660-4db6-a46b-59108a9acd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0             1\n",
              "0           -де          -д^е\n",
              "1           -ка          -к^а\n",
              "2         -либо        -л^ибо\n",
              "3       -нибудь      -ниб^удь\n",
              "4            -с            -с\n",
              "5         -таки        -так^и\n",
              "6           -то          -т^о\n",
              "7   ­вычеркнуть  ­в^ычеркнуть\n",
              "8             а            ^а\n",
              "9       а-конто      а-к^онто\n",
              "10         а-ля         а-л^я\n",
              "11    а-мольный    а-м^ольный\n",
              "12        а·ван        а·в^ан\n",
              "13        а·нал        а·н^ал\n",
              "14          аав          ^аав\n",
              "15         аава         ^аава\n",
              "16        аавам        ^аавам\n",
              "17       аавами       ^аавами\n",
              "18    аавасакса    ^аавасакса\n",
              "19        аавах        ^аавах"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb5666d5-284c-4699-b7f6-f949c28fbc90\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-де</td>\n",
              "      <td>-д^е</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-ка</td>\n",
              "      <td>-к^а</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-либо</td>\n",
              "      <td>-л^ибо</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-нибудь</td>\n",
              "      <td>-ниб^удь</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-с</td>\n",
              "      <td>-с</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-таки</td>\n",
              "      <td>-так^и</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-то</td>\n",
              "      <td>-т^о</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>­вычеркнуть</td>\n",
              "      <td>­в^ычеркнуть</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>а</td>\n",
              "      <td>^а</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>а-конто</td>\n",
              "      <td>а-к^онто</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>а-ля</td>\n",
              "      <td>а-л^я</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>а-мольный</td>\n",
              "      <td>а-м^ольный</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>а·ван</td>\n",
              "      <td>а·в^ан</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>а·нал</td>\n",
              "      <td>а·н^ал</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>аав</td>\n",
              "      <td>^аав</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>аава</td>\n",
              "      <td>^аава</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>аавам</td>\n",
              "      <td>^аавам</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>аавами</td>\n",
              "      <td>^аавами</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>аавасакса</td>\n",
              "      <td>^аавасакса</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>аавах</td>\n",
              "      <td>^аавах</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb5666d5-284c-4699-b7f6-f949c28fbc90')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb5666d5-284c-4699-b7f6-f949c28fbc90 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb5666d5-284c-4699-b7f6-f949c28fbc90');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-804b4fb6-ed92-4352-8c30-9954ad536507\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-804b4fb6-ed92-4352-8c30-9954ad536507')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-804b4fb6-ed92-4352-8c30-9954ad536507 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_index = data[0].map(lambda x: len(x)).argmax()\n",
        "max_len = len(data.iloc[max_index, 0])\n",
        "print(max_len)\n",
        "print(data.iloc[max_index,1])"
      ],
      "metadata": {
        "id": "BuTrbTD5xfPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01805d7-112a-46d0-a6b4-92f59265cad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56\n",
            "лланвайрпуллгуингиллгогерихуирндробуллллантисилиогогог^ох\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_accent_position(word):\n",
        "    return max(0, word.find(\"^\"))"
      ],
      "metadata": {
        "id": "l3N4qY-N0TQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = data[0].values\n",
        "labels = data[1].apply(find_accent_position).values"
      ],
      "metadata": {
        "id": "ZwtNewULwKnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import sys\n",
        "sys.path.append(\"/content/character-tokenizer\")\n",
        "from charactertokenizer import CharacterTokenizer\n",
        "\n",
        "chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
        "model_max_length = 64\n",
        "tokenizer = CharacterTokenizer(\n",
        "    chars,\n",
        "    model_max_length = model_max_length\n",
        ")"
      ],
      "metadata": {
        "id": "5FaCG9ajnS_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Приветик\"\n",
        "tokens = tokenizer(example)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "I5FSPMOSncpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85468dfa-3666-4868-ceae-e4c35fc7baae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [0, 39, 42, 26, 12, 18, 46, 26, 30, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "MAX_LENGTH = model_max_length\n",
        "for word in words:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        word,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', words[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwRn6I_7_UVu",
        "outputId": "b153ddfa-e695-4773-ae64-012282efff13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  -де\n",
            "Token IDs: tensor([ 0,  6, 16, 18,  1,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original: ', words[1])\n",
        "print('Token IDs:', input_ids[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBXx6ojdByA0",
        "outputId": "8ab667d2-9703-4a5a-d1a5-93d697f79029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  -ка\n",
            "Token IDs: tensor([ 0,  6, 30,  8,  1,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "test_size = int(0.5 * len(dataset))\n",
        "train_size = int(0.5 * 0.7 * len(dataset))\n",
        "val_size = len(dataset) - test_size - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEBJ2nBAkbdi",
        "outputId": "015fbc76-1c25-4d15-96d2-9542e1d18bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "588,187 training samples\n",
            "252,081 validation samples\n",
            "840,267 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch\n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "wa_MFxgMlk-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = MAX_LENGTH, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917,
          "referenced_widgets": [
            "476e777378d1414393bd692ecd6b390e",
            "91975aa4af38454c8d5d81516fcabde4",
            "dc0a696a429a418d92b4dbe56abc5e3f",
            "23ea085531944de2a1fcc9c99b10c1cd",
            "2171ee4e23274680a133d84db77c5cb3",
            "63d0ab7c3a144dc28299d615f73c2b1e",
            "86682fdc5e3c4a03b5a30b69bf3cf55d",
            "6600c64125e943c9863508681a0602d2",
            "d36147d028f44005a6cca219d63098d8",
            "fff4019b67fd44bfa67f72015f26b547",
            "8fca1823daa442ef9a719f1635757ce2",
            "fccb115d53724681a6772b79a1aa4475",
            "facf57913e9f4453b56b156b61e3f7a5",
            "3cf5f96a10ec44beadba9d4ef33e88f3",
            "10e71e13f8274611a0c5685706212682",
            "b21d11cc7e524963b555a20836ba83d9",
            "3a4e4660c0ee43869f33a440354299b5",
            "d4d556fd25ed4937941df83dd2b99fdb",
            "75975b34e28e40ee98bd3fd4f9f5edc6",
            "0fa7de0e9f4c4bb5947745dc4a447933",
            "299e1805d142456aa3688a80283a5d0f",
            "3e987aa82a8143a7b52d1fd4038b829d"
          ]
        },
        "id": "4wwU0XX5mBdW",
        "outputId": "a2b25619-afe3-4675-e3d5-28f4320eb80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "476e777378d1414393bd692ecd6b390e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fccb115d53724681a6772b79a1aa4475"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=64, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APzPXQLzFMY6",
        "outputId": "adfa8138-36d5-4630-a534-ffb4a95daea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (105879, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                          (64, 768)\n",
            "classifier.bias                                                (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x010BSAGFQRc",
        "outputId": "7b6169ff-4dc8-4bb9-fefa-2b387a541a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "V3GwuA0OFUS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "2rUlX_7VFaY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "metadata": {
        "id": "rPvy_9-lFcvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        output = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += output.loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        output.loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            output = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += output.loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c5zeXcOrFvJC",
        "outputId": "fd44c60b-f9b0-486d-b3b5-c1931474f336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  18,381.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  18,381.    Elapsed: 0:00:27.\n",
            "  Batch   120  of  18,381.    Elapsed: 0:00:40.\n",
            "  Batch   160  of  18,381.    Elapsed: 0:00:54.\n",
            "  Batch   200  of  18,381.    Elapsed: 0:01:08.\n",
            "  Batch   240  of  18,381.    Elapsed: 0:01:21.\n",
            "  Batch   280  of  18,381.    Elapsed: 0:01:36.\n",
            "  Batch   320  of  18,381.    Elapsed: 0:01:50.\n",
            "  Batch   360  of  18,381.    Elapsed: 0:02:05.\n",
            "  Batch   400  of  18,381.    Elapsed: 0:02:20.\n",
            "  Batch   440  of  18,381.    Elapsed: 0:02:34.\n",
            "  Batch   480  of  18,381.    Elapsed: 0:02:49.\n",
            "  Batch   520  of  18,381.    Elapsed: 0:03:03.\n",
            "  Batch   560  of  18,381.    Elapsed: 0:03:18.\n",
            "  Batch   600  of  18,381.    Elapsed: 0:03:32.\n",
            "  Batch   640  of  18,381.    Elapsed: 0:03:47.\n",
            "  Batch   680  of  18,381.    Elapsed: 0:04:02.\n",
            "  Batch   720  of  18,381.    Elapsed: 0:04:16.\n",
            "  Batch   760  of  18,381.    Elapsed: 0:04:31.\n",
            "  Batch   800  of  18,381.    Elapsed: 0:04:45.\n",
            "  Batch   840  of  18,381.    Elapsed: 0:05:00.\n",
            "  Batch   880  of  18,381.    Elapsed: 0:05:15.\n",
            "  Batch   920  of  18,381.    Elapsed: 0:05:29.\n",
            "  Batch   960  of  18,381.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  18,381.    Elapsed: 0:05:58.\n",
            "  Batch 1,040  of  18,381.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  18,381.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  18,381.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  18,381.    Elapsed: 0:06:57.\n",
            "  Batch 1,200  of  18,381.    Elapsed: 0:07:11.\n",
            "  Batch 1,240  of  18,381.    Elapsed: 0:07:26.\n",
            "  Batch 1,280  of  18,381.    Elapsed: 0:07:41.\n",
            "  Batch 1,320  of  18,381.    Elapsed: 0:07:55.\n",
            "  Batch 1,360  of  18,381.    Elapsed: 0:08:10.\n",
            "  Batch 1,400  of  18,381.    Elapsed: 0:08:24.\n",
            "  Batch 1,440  of  18,381.    Elapsed: 0:08:39.\n",
            "  Batch 1,480  of  18,381.    Elapsed: 0:08:54.\n",
            "  Batch 1,520  of  18,381.    Elapsed: 0:09:08.\n",
            "  Batch 1,560  of  18,381.    Elapsed: 0:09:23.\n",
            "  Batch 1,600  of  18,381.    Elapsed: 0:09:37.\n",
            "  Batch 1,640  of  18,381.    Elapsed: 0:09:52.\n",
            "  Batch 1,680  of  18,381.    Elapsed: 0:10:07.\n",
            "  Batch 1,720  of  18,381.    Elapsed: 0:10:21.\n",
            "  Batch 1,760  of  18,381.    Elapsed: 0:10:36.\n",
            "  Batch 1,800  of  18,381.    Elapsed: 0:10:50.\n",
            "  Batch 1,840  of  18,381.    Elapsed: 0:11:05.\n",
            "  Batch 1,880  of  18,381.    Elapsed: 0:11:20.\n",
            "  Batch 1,920  of  18,381.    Elapsed: 0:11:34.\n",
            "  Batch 1,960  of  18,381.    Elapsed: 0:11:49.\n",
            "  Batch 2,000  of  18,381.    Elapsed: 0:12:03.\n",
            "  Batch 2,040  of  18,381.    Elapsed: 0:12:18.\n",
            "  Batch 2,080  of  18,381.    Elapsed: 0:12:33.\n",
            "  Batch 2,120  of  18,381.    Elapsed: 0:12:47.\n",
            "  Batch 2,160  of  18,381.    Elapsed: 0:13:02.\n",
            "  Batch 2,200  of  18,381.    Elapsed: 0:13:16.\n",
            "  Batch 2,240  of  18,381.    Elapsed: 0:13:31.\n",
            "  Batch 2,280  of  18,381.    Elapsed: 0:13:45.\n",
            "  Batch 2,320  of  18,381.    Elapsed: 0:14:00.\n",
            "  Batch 2,360  of  18,381.    Elapsed: 0:14:15.\n",
            "  Batch 2,400  of  18,381.    Elapsed: 0:14:29.\n",
            "  Batch 2,440  of  18,381.    Elapsed: 0:14:44.\n",
            "  Batch 2,480  of  18,381.    Elapsed: 0:14:58.\n",
            "  Batch 2,520  of  18,381.    Elapsed: 0:15:13.\n",
            "  Batch 2,560  of  18,381.    Elapsed: 0:15:28.\n",
            "  Batch 2,600  of  18,381.    Elapsed: 0:15:42.\n",
            "  Batch 2,640  of  18,381.    Elapsed: 0:15:57.\n",
            "  Batch 2,680  of  18,381.    Elapsed: 0:16:11.\n",
            "  Batch 2,720  of  18,381.    Elapsed: 0:16:26.\n",
            "  Batch 2,760  of  18,381.    Elapsed: 0:16:41.\n",
            "  Batch 2,800  of  18,381.    Elapsed: 0:16:55.\n",
            "  Batch 2,840  of  18,381.    Elapsed: 0:17:10.\n",
            "  Batch 2,880  of  18,381.    Elapsed: 0:17:24.\n",
            "  Batch 2,920  of  18,381.    Elapsed: 0:17:39.\n",
            "  Batch 2,960  of  18,381.    Elapsed: 0:17:54.\n",
            "  Batch 3,000  of  18,381.    Elapsed: 0:18:08.\n",
            "  Batch 3,040  of  18,381.    Elapsed: 0:18:23.\n",
            "  Batch 3,080  of  18,381.    Elapsed: 0:18:37.\n",
            "  Batch 3,120  of  18,381.    Elapsed: 0:18:52.\n",
            "  Batch 3,160  of  18,381.    Elapsed: 0:19:07.\n",
            "  Batch 3,200  of  18,381.    Elapsed: 0:19:21.\n",
            "  Batch 3,240  of  18,381.    Elapsed: 0:19:36.\n",
            "  Batch 3,280  of  18,381.    Elapsed: 0:19:50.\n",
            "  Batch 3,320  of  18,381.    Elapsed: 0:20:05.\n",
            "  Batch 3,360  of  18,381.    Elapsed: 0:20:20.\n",
            "  Batch 3,400  of  18,381.    Elapsed: 0:20:34.\n",
            "  Batch 3,440  of  18,381.    Elapsed: 0:20:49.\n",
            "  Batch 3,480  of  18,381.    Elapsed: 0:21:03.\n",
            "  Batch 3,520  of  18,381.    Elapsed: 0:21:18.\n",
            "  Batch 3,560  of  18,381.    Elapsed: 0:21:32.\n",
            "  Batch 3,600  of  18,381.    Elapsed: 0:21:47.\n",
            "  Batch 3,640  of  18,381.    Elapsed: 0:22:02.\n",
            "  Batch 3,680  of  18,381.    Elapsed: 0:22:16.\n",
            "  Batch 3,720  of  18,381.    Elapsed: 0:22:31.\n",
            "  Batch 3,760  of  18,381.    Elapsed: 0:22:45.\n",
            "  Batch 3,800  of  18,381.    Elapsed: 0:23:00.\n",
            "  Batch 3,840  of  18,381.    Elapsed: 0:23:15.\n",
            "  Batch 3,880  of  18,381.    Elapsed: 0:23:29.\n",
            "  Batch 3,920  of  18,381.    Elapsed: 0:23:44.\n",
            "  Batch 3,960  of  18,381.    Elapsed: 0:23:58.\n",
            "  Batch 4,000  of  18,381.    Elapsed: 0:24:13.\n",
            "  Batch 4,040  of  18,381.    Elapsed: 0:24:28.\n",
            "  Batch 4,080  of  18,381.    Elapsed: 0:24:42.\n",
            "  Batch 4,120  of  18,381.    Elapsed: 0:24:57.\n",
            "  Batch 4,160  of  18,381.    Elapsed: 0:25:12.\n",
            "  Batch 4,200  of  18,381.    Elapsed: 0:25:26.\n",
            "  Batch 4,240  of  18,381.    Elapsed: 0:25:41.\n",
            "  Batch 4,280  of  18,381.    Elapsed: 0:25:55.\n",
            "  Batch 4,320  of  18,381.    Elapsed: 0:26:10.\n",
            "  Batch 4,360  of  18,381.    Elapsed: 0:26:25.\n",
            "  Batch 4,400  of  18,381.    Elapsed: 0:26:39.\n",
            "  Batch 4,440  of  18,381.    Elapsed: 0:26:54.\n",
            "  Batch 4,480  of  18,381.    Elapsed: 0:27:08.\n",
            "  Batch 4,520  of  18,381.    Elapsed: 0:27:23.\n",
            "  Batch 4,560  of  18,381.    Elapsed: 0:27:38.\n",
            "  Batch 4,600  of  18,381.    Elapsed: 0:27:52.\n",
            "  Batch 4,640  of  18,381.    Elapsed: 0:28:07.\n",
            "  Batch 4,680  of  18,381.    Elapsed: 0:28:21.\n",
            "  Batch 4,720  of  18,381.    Elapsed: 0:28:36.\n",
            "  Batch 4,760  of  18,381.    Elapsed: 0:28:51.\n",
            "  Batch 4,800  of  18,381.    Elapsed: 0:29:05.\n",
            "  Batch 4,840  of  18,381.    Elapsed: 0:29:20.\n",
            "  Batch 4,880  of  18,381.    Elapsed: 0:29:34.\n",
            "  Batch 4,920  of  18,381.    Elapsed: 0:29:49.\n",
            "  Batch 4,960  of  18,381.    Elapsed: 0:30:04.\n",
            "  Batch 5,000  of  18,381.    Elapsed: 0:30:18.\n",
            "  Batch 5,040  of  18,381.    Elapsed: 0:30:33.\n",
            "  Batch 5,080  of  18,381.    Elapsed: 0:30:47.\n",
            "  Batch 5,120  of  18,381.    Elapsed: 0:31:02.\n",
            "  Batch 5,160  of  18,381.    Elapsed: 0:31:16.\n",
            "  Batch 5,200  of  18,381.    Elapsed: 0:31:31.\n",
            "  Batch 5,240  of  18,381.    Elapsed: 0:31:46.\n",
            "  Batch 5,280  of  18,381.    Elapsed: 0:32:00.\n",
            "  Batch 5,320  of  18,381.    Elapsed: 0:32:15.\n",
            "  Batch 5,360  of  18,381.    Elapsed: 0:32:29.\n",
            "  Batch 5,400  of  18,381.    Elapsed: 0:32:44.\n",
            "  Batch 5,440  of  18,381.    Elapsed: 0:32:59.\n",
            "  Batch 5,480  of  18,381.    Elapsed: 0:33:13.\n",
            "  Batch 5,520  of  18,381.    Elapsed: 0:33:28.\n",
            "  Batch 5,560  of  18,381.    Elapsed: 0:33:42.\n",
            "  Batch 5,600  of  18,381.    Elapsed: 0:33:57.\n",
            "  Batch 5,640  of  18,381.    Elapsed: 0:34:12.\n",
            "  Batch 5,680  of  18,381.    Elapsed: 0:34:26.\n",
            "  Batch 5,720  of  18,381.    Elapsed: 0:34:41.\n",
            "  Batch 5,760  of  18,381.    Elapsed: 0:34:55.\n",
            "  Batch 5,800  of  18,381.    Elapsed: 0:35:10.\n",
            "  Batch 5,840  of  18,381.    Elapsed: 0:35:25.\n",
            "  Batch 5,880  of  18,381.    Elapsed: 0:35:39.\n",
            "  Batch 5,920  of  18,381.    Elapsed: 0:35:54.\n",
            "  Batch 5,960  of  18,381.    Elapsed: 0:36:08.\n",
            "  Batch 6,000  of  18,381.    Elapsed: 0:36:23.\n",
            "  Batch 6,040  of  18,381.    Elapsed: 0:36:38.\n",
            "  Batch 6,080  of  18,381.    Elapsed: 0:36:52.\n",
            "  Batch 6,120  of  18,381.    Elapsed: 0:37:07.\n",
            "  Batch 6,160  of  18,381.    Elapsed: 0:37:22.\n",
            "  Batch 6,200  of  18,381.    Elapsed: 0:37:36.\n",
            "  Batch 6,240  of  18,381.    Elapsed: 0:37:51.\n",
            "  Batch 6,280  of  18,381.    Elapsed: 0:38:05.\n",
            "  Batch 6,320  of  18,381.    Elapsed: 0:38:20.\n",
            "  Batch 6,360  of  18,381.    Elapsed: 0:38:34.\n",
            "  Batch 6,400  of  18,381.    Elapsed: 0:38:49.\n",
            "  Batch 6,440  of  18,381.    Elapsed: 0:39:04.\n",
            "  Batch 6,480  of  18,381.    Elapsed: 0:39:18.\n",
            "  Batch 6,520  of  18,381.    Elapsed: 0:39:33.\n",
            "  Batch 6,560  of  18,381.    Elapsed: 0:39:48.\n",
            "  Batch 6,600  of  18,381.    Elapsed: 0:40:02.\n",
            "  Batch 6,640  of  18,381.    Elapsed: 0:40:17.\n",
            "  Batch 6,680  of  18,381.    Elapsed: 0:40:31.\n",
            "  Batch 6,720  of  18,381.    Elapsed: 0:40:46.\n",
            "  Batch 6,760  of  18,381.    Elapsed: 0:41:01.\n",
            "  Batch 6,800  of  18,381.    Elapsed: 0:41:15.\n",
            "  Batch 6,840  of  18,381.    Elapsed: 0:41:30.\n",
            "  Batch 6,880  of  18,381.    Elapsed: 0:41:44.\n",
            "  Batch 6,920  of  18,381.    Elapsed: 0:41:59.\n",
            "  Batch 6,960  of  18,381.    Elapsed: 0:42:14.\n",
            "  Batch 7,000  of  18,381.    Elapsed: 0:42:28.\n",
            "  Batch 7,040  of  18,381.    Elapsed: 0:42:43.\n",
            "  Batch 7,080  of  18,381.    Elapsed: 0:42:57.\n",
            "  Batch 7,120  of  18,381.    Elapsed: 0:43:12.\n",
            "  Batch 7,160  of  18,381.    Elapsed: 0:43:27.\n",
            "  Batch 7,200  of  18,381.    Elapsed: 0:43:41.\n",
            "  Batch 7,240  of  18,381.    Elapsed: 0:43:56.\n",
            "  Batch 7,280  of  18,381.    Elapsed: 0:44:10.\n",
            "  Batch 7,320  of  18,381.    Elapsed: 0:44:25.\n",
            "  Batch 7,360  of  18,381.    Elapsed: 0:44:40.\n",
            "  Batch 7,400  of  18,381.    Elapsed: 0:44:54.\n",
            "  Batch 7,440  of  18,381.    Elapsed: 0:45:09.\n",
            "  Batch 7,480  of  18,381.    Elapsed: 0:45:23.\n",
            "  Batch 7,520  of  18,381.    Elapsed: 0:45:38.\n",
            "  Batch 7,560  of  18,381.    Elapsed: 0:45:53.\n",
            "  Batch 7,600  of  18,381.    Elapsed: 0:46:07.\n",
            "  Batch 7,640  of  18,381.    Elapsed: 0:46:22.\n",
            "  Batch 7,680  of  18,381.    Elapsed: 0:46:36.\n",
            "  Batch 7,720  of  18,381.    Elapsed: 0:46:51.\n",
            "  Batch 7,760  of  18,381.    Elapsed: 0:47:06.\n",
            "  Batch 7,800  of  18,381.    Elapsed: 0:47:20.\n",
            "  Batch 7,840  of  18,381.    Elapsed: 0:47:35.\n",
            "  Batch 7,880  of  18,381.    Elapsed: 0:47:49.\n",
            "  Batch 7,920  of  18,381.    Elapsed: 0:48:04.\n",
            "  Batch 7,960  of  18,381.    Elapsed: 0:48:19.\n",
            "  Batch 8,000  of  18,381.    Elapsed: 0:48:33.\n",
            "  Batch 8,040  of  18,381.    Elapsed: 0:48:48.\n",
            "  Batch 8,080  of  18,381.    Elapsed: 0:49:02.\n",
            "  Batch 8,120  of  18,381.    Elapsed: 0:49:17.\n",
            "  Batch 8,160  of  18,381.    Elapsed: 0:49:32.\n",
            "  Batch 8,200  of  18,381.    Elapsed: 0:49:46.\n",
            "  Batch 8,240  of  18,381.    Elapsed: 0:50:01.\n",
            "  Batch 8,280  of  18,381.    Elapsed: 0:50:15.\n",
            "  Batch 8,320  of  18,381.    Elapsed: 0:50:30.\n",
            "  Batch 8,360  of  18,381.    Elapsed: 0:50:45.\n",
            "  Batch 8,400  of  18,381.    Elapsed: 0:50:59.\n",
            "  Batch 8,440  of  18,381.    Elapsed: 0:51:14.\n",
            "  Batch 8,480  of  18,381.    Elapsed: 0:51:28.\n",
            "  Batch 8,520  of  18,381.    Elapsed: 0:51:43.\n",
            "  Batch 8,560  of  18,381.    Elapsed: 0:51:58.\n",
            "  Batch 8,600  of  18,381.    Elapsed: 0:52:12.\n",
            "  Batch 8,640  of  18,381.    Elapsed: 0:52:27.\n",
            "  Batch 8,680  of  18,381.    Elapsed: 0:52:41.\n",
            "  Batch 8,720  of  18,381.    Elapsed: 0:52:56.\n",
            "  Batch 8,760  of  18,381.    Elapsed: 0:53:11.\n",
            "  Batch 8,800  of  18,381.    Elapsed: 0:53:25.\n",
            "  Batch 8,840  of  18,381.    Elapsed: 0:53:40.\n",
            "  Batch 8,880  of  18,381.    Elapsed: 0:53:54.\n",
            "  Batch 8,920  of  18,381.    Elapsed: 0:54:09.\n",
            "  Batch 8,960  of  18,381.    Elapsed: 0:54:23.\n",
            "  Batch 9,000  of  18,381.    Elapsed: 0:54:38.\n",
            "  Batch 9,040  of  18,381.    Elapsed: 0:54:53.\n",
            "  Batch 9,080  of  18,381.    Elapsed: 0:55:07.\n",
            "  Batch 9,120  of  18,381.    Elapsed: 0:55:22.\n",
            "  Batch 9,160  of  18,381.    Elapsed: 0:55:37.\n",
            "  Batch 9,200  of  18,381.    Elapsed: 0:55:51.\n",
            "  Batch 9,240  of  18,381.    Elapsed: 0:56:06.\n",
            "  Batch 9,280  of  18,381.    Elapsed: 0:56:20.\n",
            "  Batch 9,320  of  18,381.    Elapsed: 0:56:35.\n",
            "  Batch 9,360  of  18,381.    Elapsed: 0:56:50.\n",
            "  Batch 9,400  of  18,381.    Elapsed: 0:57:04.\n",
            "  Batch 9,440  of  18,381.    Elapsed: 0:57:19.\n",
            "  Batch 9,480  of  18,381.    Elapsed: 0:57:34.\n",
            "  Batch 9,520  of  18,381.    Elapsed: 0:57:48.\n",
            "  Batch 9,560  of  18,381.    Elapsed: 0:58:03.\n",
            "  Batch 9,600  of  18,381.    Elapsed: 0:58:17.\n",
            "  Batch 9,640  of  18,381.    Elapsed: 0:58:32.\n",
            "  Batch 9,680  of  18,381.    Elapsed: 0:58:47.\n",
            "  Batch 9,720  of  18,381.    Elapsed: 0:59:01.\n",
            "  Batch 9,760  of  18,381.    Elapsed: 0:59:16.\n",
            "  Batch 9,800  of  18,381.    Elapsed: 0:59:31.\n",
            "  Batch 9,840  of  18,381.    Elapsed: 0:59:45.\n",
            "  Batch 9,880  of  18,381.    Elapsed: 1:00:00.\n",
            "  Batch 9,920  of  18,381.    Elapsed: 1:00:14.\n",
            "  Batch 9,960  of  18,381.    Elapsed: 1:00:29.\n",
            "  Batch 10,000  of  18,381.    Elapsed: 1:00:43.\n",
            "  Batch 10,040  of  18,381.    Elapsed: 1:00:58.\n",
            "  Batch 10,080  of  18,381.    Elapsed: 1:01:13.\n",
            "  Batch 10,120  of  18,381.    Elapsed: 1:01:27.\n",
            "  Batch 10,160  of  18,381.    Elapsed: 1:01:42.\n",
            "  Batch 10,200  of  18,381.    Elapsed: 1:01:56.\n",
            "  Batch 10,240  of  18,381.    Elapsed: 1:02:11.\n",
            "  Batch 10,280  of  18,381.    Elapsed: 1:02:26.\n",
            "  Batch 10,320  of  18,381.    Elapsed: 1:02:40.\n",
            "  Batch 10,360  of  18,381.    Elapsed: 1:02:55.\n",
            "  Batch 10,400  of  18,381.    Elapsed: 1:03:09.\n",
            "  Batch 10,440  of  18,381.    Elapsed: 1:03:24.\n",
            "  Batch 10,480  of  18,381.    Elapsed: 1:03:39.\n",
            "  Batch 10,520  of  18,381.    Elapsed: 1:03:53.\n",
            "  Batch 10,560  of  18,381.    Elapsed: 1:04:08.\n",
            "  Batch 10,600  of  18,381.    Elapsed: 1:04:22.\n",
            "  Batch 10,640  of  18,381.    Elapsed: 1:04:37.\n",
            "  Batch 10,680  of  18,381.    Elapsed: 1:04:52.\n",
            "  Batch 10,720  of  18,381.    Elapsed: 1:05:06.\n",
            "  Batch 10,760  of  18,381.    Elapsed: 1:05:21.\n",
            "  Batch 10,800  of  18,381.    Elapsed: 1:05:35.\n",
            "  Batch 10,840  of  18,381.    Elapsed: 1:05:50.\n",
            "  Batch 10,880  of  18,381.    Elapsed: 1:06:05.\n",
            "  Batch 10,920  of  18,381.    Elapsed: 1:06:19.\n",
            "  Batch 10,960  of  18,381.    Elapsed: 1:06:34.\n",
            "  Batch 11,000  of  18,381.    Elapsed: 1:06:48.\n",
            "  Batch 11,040  of  18,381.    Elapsed: 1:07:03.\n",
            "  Batch 11,080  of  18,381.    Elapsed: 1:07:18.\n",
            "  Batch 11,120  of  18,381.    Elapsed: 1:07:32.\n",
            "  Batch 11,160  of  18,381.    Elapsed: 1:07:47.\n",
            "  Batch 11,200  of  18,381.    Elapsed: 1:08:01.\n",
            "  Batch 11,240  of  18,381.    Elapsed: 1:08:16.\n",
            "  Batch 11,280  of  18,381.    Elapsed: 1:08:31.\n",
            "  Batch 11,320  of  18,381.    Elapsed: 1:08:45.\n",
            "  Batch 11,360  of  18,381.    Elapsed: 1:09:00.\n",
            "  Batch 11,400  of  18,381.    Elapsed: 1:09:15.\n",
            "  Batch 11,440  of  18,381.    Elapsed: 1:09:29.\n",
            "  Batch 11,480  of  18,381.    Elapsed: 1:09:44.\n",
            "  Batch 11,520  of  18,381.    Elapsed: 1:09:58.\n",
            "  Batch 11,560  of  18,381.    Elapsed: 1:10:13.\n",
            "  Batch 11,600  of  18,381.    Elapsed: 1:10:28.\n",
            "  Batch 11,640  of  18,381.    Elapsed: 1:10:42.\n",
            "  Batch 11,680  of  18,381.    Elapsed: 1:10:57.\n",
            "  Batch 11,720  of  18,381.    Elapsed: 1:11:11.\n",
            "  Batch 11,760  of  18,381.    Elapsed: 1:11:26.\n",
            "  Batch 11,800  of  18,381.    Elapsed: 1:11:41.\n",
            "  Batch 11,840  of  18,381.    Elapsed: 1:11:55.\n",
            "  Batch 11,880  of  18,381.    Elapsed: 1:12:10.\n",
            "  Batch 11,920  of  18,381.    Elapsed: 1:12:25.\n",
            "  Batch 11,960  of  18,381.    Elapsed: 1:12:39.\n",
            "  Batch 12,000  of  18,381.    Elapsed: 1:12:54.\n",
            "  Batch 12,040  of  18,381.    Elapsed: 1:13:08.\n",
            "  Batch 12,080  of  18,381.    Elapsed: 1:13:23.\n",
            "  Batch 12,120  of  18,381.    Elapsed: 1:13:38.\n",
            "  Batch 12,160  of  18,381.    Elapsed: 1:13:52.\n",
            "  Batch 12,200  of  18,381.    Elapsed: 1:14:07.\n",
            "  Batch 12,240  of  18,381.    Elapsed: 1:14:21.\n",
            "  Batch 12,280  of  18,381.    Elapsed: 1:14:36.\n",
            "  Batch 12,320  of  18,381.    Elapsed: 1:14:51.\n",
            "  Batch 12,360  of  18,381.    Elapsed: 1:15:05.\n",
            "  Batch 12,400  of  18,381.    Elapsed: 1:15:20.\n",
            "  Batch 12,440  of  18,381.    Elapsed: 1:15:34.\n",
            "  Batch 12,480  of  18,381.    Elapsed: 1:15:49.\n",
            "  Batch 12,520  of  18,381.    Elapsed: 1:16:04.\n",
            "  Batch 12,560  of  18,381.    Elapsed: 1:16:18.\n",
            "  Batch 12,600  of  18,381.    Elapsed: 1:16:33.\n",
            "  Batch 12,640  of  18,381.    Elapsed: 1:16:48.\n",
            "  Batch 12,680  of  18,381.    Elapsed: 1:17:02.\n",
            "  Batch 12,720  of  18,381.    Elapsed: 1:17:17.\n",
            "  Batch 12,760  of  18,381.    Elapsed: 1:17:31.\n",
            "  Batch 12,800  of  18,381.    Elapsed: 1:17:46.\n",
            "  Batch 12,840  of  18,381.    Elapsed: 1:18:01.\n",
            "  Batch 12,880  of  18,381.    Elapsed: 1:18:15.\n",
            "  Batch 12,920  of  18,381.    Elapsed: 1:18:30.\n",
            "  Batch 12,960  of  18,381.    Elapsed: 1:18:44.\n",
            "  Batch 13,000  of  18,381.    Elapsed: 1:18:59.\n",
            "  Batch 13,040  of  18,381.    Elapsed: 1:19:14.\n",
            "  Batch 13,080  of  18,381.    Elapsed: 1:19:28.\n",
            "  Batch 13,120  of  18,381.    Elapsed: 1:19:43.\n",
            "  Batch 13,160  of  18,381.    Elapsed: 1:19:58.\n",
            "  Batch 13,200  of  18,381.    Elapsed: 1:20:12.\n",
            "  Batch 13,240  of  18,381.    Elapsed: 1:20:27.\n",
            "  Batch 13,280  of  18,381.    Elapsed: 1:20:41.\n",
            "  Batch 13,320  of  18,381.    Elapsed: 1:20:56.\n",
            "  Batch 13,360  of  18,381.    Elapsed: 1:21:10.\n",
            "  Batch 13,400  of  18,381.    Elapsed: 1:21:25.\n",
            "  Batch 13,440  of  18,381.    Elapsed: 1:21:40.\n",
            "  Batch 13,480  of  18,381.    Elapsed: 1:21:54.\n",
            "  Batch 13,520  of  18,381.    Elapsed: 1:22:09.\n",
            "  Batch 13,560  of  18,381.    Elapsed: 1:22:24.\n",
            "  Batch 13,600  of  18,381.    Elapsed: 1:22:38.\n",
            "  Batch 13,640  of  18,381.    Elapsed: 1:22:53.\n",
            "  Batch 13,680  of  18,381.    Elapsed: 1:23:08.\n",
            "  Batch 13,720  of  18,381.    Elapsed: 1:23:22.\n",
            "  Batch 13,760  of  18,381.    Elapsed: 1:23:37.\n",
            "  Batch 13,800  of  18,381.    Elapsed: 1:23:51.\n",
            "  Batch 13,840  of  18,381.    Elapsed: 1:24:06.\n",
            "  Batch 13,880  of  18,381.    Elapsed: 1:24:21.\n",
            "  Batch 13,920  of  18,381.    Elapsed: 1:24:35.\n",
            "  Batch 13,960  of  18,381.    Elapsed: 1:24:50.\n",
            "  Batch 14,000  of  18,381.    Elapsed: 1:25:05.\n",
            "  Batch 14,040  of  18,381.    Elapsed: 1:25:19.\n",
            "  Batch 14,080  of  18,381.    Elapsed: 1:25:34.\n",
            "  Batch 14,120  of  18,381.    Elapsed: 1:25:48.\n",
            "  Batch 14,160  of  18,381.    Elapsed: 1:26:03.\n",
            "  Batch 14,200  of  18,381.    Elapsed: 1:26:18.\n",
            "  Batch 14,240  of  18,381.    Elapsed: 1:26:32.\n",
            "  Batch 14,280  of  18,381.    Elapsed: 1:26:47.\n",
            "  Batch 14,320  of  18,381.    Elapsed: 1:27:02.\n",
            "  Batch 14,360  of  18,381.    Elapsed: 1:27:16.\n",
            "  Batch 14,400  of  18,381.    Elapsed: 1:27:31.\n",
            "  Batch 14,440  of  18,381.    Elapsed: 1:27:45.\n",
            "  Batch 14,480  of  18,381.    Elapsed: 1:28:00.\n",
            "  Batch 14,520  of  18,381.    Elapsed: 1:28:15.\n",
            "  Batch 14,560  of  18,381.    Elapsed: 1:28:29.\n",
            "  Batch 14,600  of  18,381.    Elapsed: 1:28:44.\n",
            "  Batch 14,640  of  18,381.    Elapsed: 1:28:58.\n",
            "  Batch 14,680  of  18,381.    Elapsed: 1:29:13.\n",
            "  Batch 14,720  of  18,381.    Elapsed: 1:29:28.\n",
            "  Batch 14,760  of  18,381.    Elapsed: 1:29:42.\n",
            "  Batch 14,800  of  18,381.    Elapsed: 1:29:57.\n",
            "  Batch 14,840  of  18,381.    Elapsed: 1:30:12.\n",
            "  Batch 14,880  of  18,381.    Elapsed: 1:30:26.\n",
            "  Batch 14,920  of  18,381.    Elapsed: 1:30:41.\n",
            "  Batch 14,960  of  18,381.    Elapsed: 1:30:55.\n",
            "  Batch 15,000  of  18,381.    Elapsed: 1:31:10.\n",
            "  Batch 15,040  of  18,381.    Elapsed: 1:31:25.\n",
            "  Batch 15,080  of  18,381.    Elapsed: 1:31:39.\n",
            "  Batch 15,120  of  18,381.    Elapsed: 1:31:54.\n",
            "  Batch 15,160  of  18,381.    Elapsed: 1:32:08.\n",
            "  Batch 15,200  of  18,381.    Elapsed: 1:32:23.\n",
            "  Batch 15,240  of  18,381.    Elapsed: 1:32:38.\n",
            "  Batch 15,280  of  18,381.    Elapsed: 1:32:52.\n",
            "  Batch 15,320  of  18,381.    Elapsed: 1:33:07.\n",
            "  Batch 15,360  of  18,381.    Elapsed: 1:33:22.\n",
            "  Batch 15,400  of  18,381.    Elapsed: 1:33:36.\n",
            "  Batch 15,440  of  18,381.    Elapsed: 1:33:51.\n",
            "  Batch 15,480  of  18,381.    Elapsed: 1:34:05.\n",
            "  Batch 15,520  of  18,381.    Elapsed: 1:34:20.\n",
            "  Batch 15,560  of  18,381.    Elapsed: 1:34:35.\n",
            "  Batch 15,600  of  18,381.    Elapsed: 1:34:49.\n",
            "  Batch 15,640  of  18,381.    Elapsed: 1:35:04.\n",
            "  Batch 15,680  of  18,381.    Elapsed: 1:35:18.\n",
            "  Batch 15,720  of  18,381.    Elapsed: 1:35:33.\n",
            "  Batch 15,760  of  18,381.    Elapsed: 1:35:48.\n",
            "  Batch 15,800  of  18,381.    Elapsed: 1:36:02.\n",
            "  Batch 15,840  of  18,381.    Elapsed: 1:36:17.\n",
            "  Batch 15,880  of  18,381.    Elapsed: 1:36:31.\n",
            "  Batch 15,920  of  18,381.    Elapsed: 1:36:46.\n",
            "  Batch 15,960  of  18,381.    Elapsed: 1:37:01.\n",
            "  Batch 16,000  of  18,381.    Elapsed: 1:37:15.\n",
            "  Batch 16,040  of  18,381.    Elapsed: 1:37:30.\n",
            "  Batch 16,080  of  18,381.    Elapsed: 1:37:45.\n",
            "  Batch 16,120  of  18,381.    Elapsed: 1:37:59.\n",
            "  Batch 16,160  of  18,381.    Elapsed: 1:38:14.\n",
            "  Batch 16,200  of  18,381.    Elapsed: 1:38:28.\n",
            "  Batch 16,240  of  18,381.    Elapsed: 1:38:43.\n",
            "  Batch 16,280  of  18,381.    Elapsed: 1:38:58.\n",
            "  Batch 16,320  of  18,381.    Elapsed: 1:39:12.\n",
            "  Batch 16,360  of  18,381.    Elapsed: 1:39:27.\n",
            "  Batch 16,400  of  18,381.    Elapsed: 1:39:42.\n",
            "  Batch 16,440  of  18,381.    Elapsed: 1:39:56.\n",
            "  Batch 16,480  of  18,381.    Elapsed: 1:40:11.\n",
            "  Batch 16,520  of  18,381.    Elapsed: 1:40:26.\n",
            "  Batch 16,560  of  18,381.    Elapsed: 1:40:40.\n",
            "  Batch 16,600  of  18,381.    Elapsed: 1:40:55.\n",
            "  Batch 16,640  of  18,381.    Elapsed: 1:41:09.\n",
            "  Batch 16,680  of  18,381.    Elapsed: 1:41:24.\n",
            "  Batch 16,720  of  18,381.    Elapsed: 1:41:39.\n",
            "  Batch 16,760  of  18,381.    Elapsed: 1:41:53.\n",
            "  Batch 16,800  of  18,381.    Elapsed: 1:42:08.\n",
            "  Batch 16,840  of  18,381.    Elapsed: 1:42:23.\n",
            "  Batch 16,880  of  18,381.    Elapsed: 1:42:37.\n",
            "  Batch 16,920  of  18,381.    Elapsed: 1:42:52.\n",
            "  Batch 16,960  of  18,381.    Elapsed: 1:43:06.\n",
            "  Batch 17,000  of  18,381.    Elapsed: 1:43:21.\n",
            "  Batch 17,040  of  18,381.    Elapsed: 1:43:36.\n",
            "  Batch 17,080  of  18,381.    Elapsed: 1:43:50.\n",
            "  Batch 17,120  of  18,381.    Elapsed: 1:44:05.\n",
            "  Batch 17,160  of  18,381.    Elapsed: 1:44:20.\n",
            "  Batch 17,200  of  18,381.    Elapsed: 1:44:34.\n",
            "  Batch 17,240  of  18,381.    Elapsed: 1:44:49.\n",
            "  Batch 17,280  of  18,381.    Elapsed: 1:45:03.\n",
            "  Batch 17,320  of  18,381.    Elapsed: 1:45:18.\n",
            "  Batch 17,360  of  18,381.    Elapsed: 1:45:33.\n",
            "  Batch 17,400  of  18,381.    Elapsed: 1:45:47.\n",
            "  Batch 17,440  of  18,381.    Elapsed: 1:46:02.\n",
            "  Batch 17,480  of  18,381.    Elapsed: 1:46:17.\n",
            "  Batch 17,520  of  18,381.    Elapsed: 1:46:31.\n",
            "  Batch 17,560  of  18,381.    Elapsed: 1:46:46.\n",
            "  Batch 17,600  of  18,381.    Elapsed: 1:47:00.\n",
            "  Batch 17,640  of  18,381.    Elapsed: 1:47:15.\n",
            "  Batch 17,680  of  18,381.    Elapsed: 1:47:30.\n",
            "  Batch 17,720  of  18,381.    Elapsed: 1:47:44.\n",
            "  Batch 17,760  of  18,381.    Elapsed: 1:47:59.\n",
            "  Batch 17,800  of  18,381.    Elapsed: 1:48:14.\n",
            "  Batch 17,840  of  18,381.    Elapsed: 1:48:28.\n",
            "  Batch 17,880  of  18,381.    Elapsed: 1:48:43.\n",
            "  Batch 17,920  of  18,381.    Elapsed: 1:48:58.\n",
            "  Batch 17,960  of  18,381.    Elapsed: 1:49:12.\n",
            "  Batch 18,000  of  18,381.    Elapsed: 1:49:27.\n",
            "  Batch 18,040  of  18,381.    Elapsed: 1:49:41.\n",
            "  Batch 18,080  of  18,381.    Elapsed: 1:49:56.\n",
            "  Batch 18,120  of  18,381.    Elapsed: 1:50:11.\n",
            "  Batch 18,160  of  18,381.    Elapsed: 1:50:25.\n",
            "  Batch 18,200  of  18,381.    Elapsed: 1:50:40.\n",
            "  Batch 18,240  of  18,381.    Elapsed: 1:50:54.\n",
            "  Batch 18,280  of  18,381.    Elapsed: 1:51:09.\n",
            "  Batch 18,320  of  18,381.    Elapsed: 1:51:24.\n",
            "  Batch 18,360  of  18,381.    Elapsed: 1:51:38.\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epoch took: 1:51:46\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:16:19\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  18,381.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  18,381.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  18,381.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  18,381.    Elapsed: 0:00:59.\n",
            "  Batch   200  of  18,381.    Elapsed: 0:01:13.\n",
            "  Batch   240  of  18,381.    Elapsed: 0:01:28.\n",
            "  Batch   280  of  18,381.    Elapsed: 0:01:42.\n",
            "  Batch   320  of  18,381.    Elapsed: 0:01:57.\n",
            "  Batch   360  of  18,381.    Elapsed: 0:02:12.\n",
            "  Batch   400  of  18,381.    Elapsed: 0:02:26.\n",
            "  Batch   440  of  18,381.    Elapsed: 0:02:41.\n",
            "  Batch   480  of  18,381.    Elapsed: 0:02:55.\n",
            "  Batch   520  of  18,381.    Elapsed: 0:03:10.\n",
            "  Batch   560  of  18,381.    Elapsed: 0:03:25.\n",
            "  Batch   600  of  18,381.    Elapsed: 0:03:39.\n",
            "  Batch   640  of  18,381.    Elapsed: 0:03:54.\n",
            "  Batch   680  of  18,381.    Elapsed: 0:04:09.\n",
            "  Batch   720  of  18,381.    Elapsed: 0:04:23.\n",
            "  Batch   760  of  18,381.    Elapsed: 0:04:38.\n",
            "  Batch   800  of  18,381.    Elapsed: 0:04:52.\n",
            "  Batch   840  of  18,381.    Elapsed: 0:05:07.\n",
            "  Batch   880  of  18,381.    Elapsed: 0:05:22.\n",
            "  Batch   920  of  18,381.    Elapsed: 0:05:36.\n",
            "  Batch   960  of  18,381.    Elapsed: 0:05:51.\n",
            "  Batch 1,000  of  18,381.    Elapsed: 0:06:05.\n",
            "  Batch 1,040  of  18,381.    Elapsed: 0:06:20.\n",
            "  Batch 1,080  of  18,381.    Elapsed: 0:06:35.\n",
            "  Batch 1,120  of  18,381.    Elapsed: 0:06:49.\n",
            "  Batch 1,160  of  18,381.    Elapsed: 0:07:04.\n",
            "  Batch 1,200  of  18,381.    Elapsed: 0:07:18.\n",
            "  Batch 1,240  of  18,381.    Elapsed: 0:07:33.\n",
            "  Batch 1,280  of  18,381.    Elapsed: 0:07:48.\n",
            "  Batch 1,320  of  18,381.    Elapsed: 0:08:02.\n",
            "  Batch 1,360  of  18,381.    Elapsed: 0:08:17.\n",
            "  Batch 1,400  of  18,381.    Elapsed: 0:08:31.\n",
            "  Batch 1,440  of  18,381.    Elapsed: 0:08:46.\n",
            "  Batch 1,480  of  18,381.    Elapsed: 0:09:01.\n",
            "  Batch 1,520  of  18,381.    Elapsed: 0:09:15.\n",
            "  Batch 1,560  of  18,381.    Elapsed: 0:09:30.\n",
            "  Batch 1,600  of  18,381.    Elapsed: 0:09:45.\n",
            "  Batch 1,640  of  18,381.    Elapsed: 0:09:59.\n",
            "  Batch 1,680  of  18,381.    Elapsed: 0:10:14.\n",
            "  Batch 1,720  of  18,381.    Elapsed: 0:10:28.\n",
            "  Batch 1,760  of  18,381.    Elapsed: 0:10:43.\n",
            "  Batch 1,800  of  18,381.    Elapsed: 0:10:58.\n",
            "  Batch 1,840  of  18,381.    Elapsed: 0:11:12.\n",
            "  Batch 1,880  of  18,381.    Elapsed: 0:11:27.\n",
            "  Batch 1,920  of  18,381.    Elapsed: 0:11:41.\n",
            "  Batch 1,960  of  18,381.    Elapsed: 0:11:56.\n",
            "  Batch 2,000  of  18,381.    Elapsed: 0:12:11.\n",
            "  Batch 2,040  of  18,381.    Elapsed: 0:12:25.\n",
            "  Batch 2,080  of  18,381.    Elapsed: 0:12:40.\n",
            "  Batch 2,120  of  18,381.    Elapsed: 0:12:54.\n",
            "  Batch 2,160  of  18,381.    Elapsed: 0:13:09.\n",
            "  Batch 2,200  of  18,381.    Elapsed: 0:13:24.\n",
            "  Batch 2,240  of  18,381.    Elapsed: 0:13:38.\n",
            "  Batch 2,280  of  18,381.    Elapsed: 0:13:53.\n",
            "  Batch 2,320  of  18,381.    Elapsed: 0:14:08.\n",
            "  Batch 2,360  of  18,381.    Elapsed: 0:14:22.\n",
            "  Batch 2,400  of  18,381.    Elapsed: 0:14:37.\n",
            "  Batch 2,440  of  18,381.    Elapsed: 0:14:52.\n",
            "  Batch 2,480  of  18,381.    Elapsed: 0:15:06.\n",
            "  Batch 2,520  of  18,381.    Elapsed: 0:15:21.\n",
            "  Batch 2,560  of  18,381.    Elapsed: 0:15:35.\n",
            "  Batch 2,600  of  18,381.    Elapsed: 0:15:50.\n",
            "  Batch 2,640  of  18,381.    Elapsed: 0:16:05.\n",
            "  Batch 2,680  of  18,381.    Elapsed: 0:16:19.\n",
            "  Batch 2,720  of  18,381.    Elapsed: 0:16:34.\n",
            "  Batch 2,760  of  18,381.    Elapsed: 0:16:49.\n",
            "  Batch 2,800  of  18,381.    Elapsed: 0:17:03.\n",
            "  Batch 2,840  of  18,381.    Elapsed: 0:17:18.\n",
            "  Batch 2,880  of  18,381.    Elapsed: 0:17:33.\n",
            "  Batch 2,920  of  18,381.    Elapsed: 0:17:47.\n",
            "  Batch 2,960  of  18,381.    Elapsed: 0:18:02.\n",
            "  Batch 3,000  of  18,381.    Elapsed: 0:18:16.\n",
            "  Batch 3,040  of  18,381.    Elapsed: 0:18:31.\n",
            "  Batch 3,080  of  18,381.    Elapsed: 0:18:46.\n",
            "  Batch 3,120  of  18,381.    Elapsed: 0:19:00.\n",
            "  Batch 3,160  of  18,381.    Elapsed: 0:19:15.\n",
            "  Batch 3,200  of  18,381.    Elapsed: 0:19:30.\n",
            "  Batch 3,240  of  18,381.    Elapsed: 0:19:44.\n",
            "  Batch 3,280  of  18,381.    Elapsed: 0:19:59.\n",
            "  Batch 3,320  of  18,381.    Elapsed: 0:20:13.\n",
            "  Batch 3,360  of  18,381.    Elapsed: 0:20:28.\n",
            "  Batch 3,400  of  18,381.    Elapsed: 0:20:43.\n",
            "  Batch 3,440  of  18,381.    Elapsed: 0:20:57.\n",
            "  Batch 3,480  of  18,381.    Elapsed: 0:21:12.\n",
            "  Batch 3,520  of  18,381.    Elapsed: 0:21:26.\n",
            "  Batch 3,560  of  18,381.    Elapsed: 0:21:41.\n",
            "  Batch 3,600  of  18,381.    Elapsed: 0:21:55.\n",
            "  Batch 3,640  of  18,381.    Elapsed: 0:22:10.\n",
            "  Batch 3,680  of  18,381.    Elapsed: 0:22:25.\n",
            "  Batch 3,720  of  18,381.    Elapsed: 0:22:39.\n",
            "  Batch 3,760  of  18,381.    Elapsed: 0:22:54.\n",
            "  Batch 3,800  of  18,381.    Elapsed: 0:23:08.\n",
            "  Batch 3,840  of  18,381.    Elapsed: 0:23:23.\n",
            "  Batch 3,880  of  18,381.    Elapsed: 0:23:38.\n",
            "  Batch 3,920  of  18,381.    Elapsed: 0:23:52.\n",
            "  Batch 3,960  of  18,381.    Elapsed: 0:24:07.\n",
            "  Batch 4,000  of  18,381.    Elapsed: 0:24:22.\n",
            "  Batch 4,040  of  18,381.    Elapsed: 0:24:36.\n",
            "  Batch 4,080  of  18,381.    Elapsed: 0:24:51.\n",
            "  Batch 4,120  of  18,381.    Elapsed: 0:25:05.\n",
            "  Batch 4,160  of  18,381.    Elapsed: 0:25:20.\n",
            "  Batch 4,200  of  18,381.    Elapsed: 0:25:35.\n",
            "  Batch 4,240  of  18,381.    Elapsed: 0:25:49.\n",
            "  Batch 4,280  of  18,381.    Elapsed: 0:26:04.\n",
            "  Batch 4,320  of  18,381.    Elapsed: 0:26:18.\n",
            "  Batch 4,360  of  18,381.    Elapsed: 0:26:33.\n",
            "  Batch 4,400  of  18,381.    Elapsed: 0:26:48.\n",
            "  Batch 4,440  of  18,381.    Elapsed: 0:27:02.\n",
            "  Batch 4,480  of  18,381.    Elapsed: 0:27:17.\n",
            "  Batch 4,520  of  18,381.    Elapsed: 0:27:31.\n",
            "  Batch 4,560  of  18,381.    Elapsed: 0:27:46.\n",
            "  Batch 4,600  of  18,381.    Elapsed: 0:28:01.\n",
            "  Batch 4,640  of  18,381.    Elapsed: 0:28:15.\n",
            "  Batch 4,680  of  18,381.    Elapsed: 0:28:30.\n",
            "  Batch 4,720  of  18,381.    Elapsed: 0:28:45.\n",
            "  Batch 4,760  of  18,381.    Elapsed: 0:28:59.\n",
            "  Batch 4,800  of  18,381.    Elapsed: 0:29:14.\n",
            "  Batch 4,840  of  18,381.    Elapsed: 0:29:28.\n",
            "  Batch 4,880  of  18,381.    Elapsed: 0:29:43.\n",
            "  Batch 4,920  of  18,381.    Elapsed: 0:29:58.\n",
            "  Batch 4,960  of  18,381.    Elapsed: 0:30:12.\n",
            "  Batch 5,000  of  18,381.    Elapsed: 0:30:27.\n",
            "  Batch 5,040  of  18,381.    Elapsed: 0:30:41.\n",
            "  Batch 5,080  of  18,381.    Elapsed: 0:30:56.\n",
            "  Batch 5,120  of  18,381.    Elapsed: 0:31:11.\n",
            "  Batch 5,160  of  18,381.    Elapsed: 0:31:25.\n",
            "  Batch 5,200  of  18,381.    Elapsed: 0:31:40.\n",
            "  Batch 5,240  of  18,381.    Elapsed: 0:31:54.\n",
            "  Batch 5,280  of  18,381.    Elapsed: 0:32:09.\n",
            "  Batch 5,320  of  18,381.    Elapsed: 0:32:24.\n",
            "  Batch 5,360  of  18,381.    Elapsed: 0:32:38.\n",
            "  Batch 5,400  of  18,381.    Elapsed: 0:32:53.\n",
            "  Batch 5,440  of  18,381.    Elapsed: 0:33:08.\n",
            "  Batch 5,480  of  18,381.    Elapsed: 0:33:22.\n",
            "  Batch 5,520  of  18,381.    Elapsed: 0:33:37.\n",
            "  Batch 5,560  of  18,381.    Elapsed: 0:33:51.\n",
            "  Batch 5,600  of  18,381.    Elapsed: 0:34:06.\n",
            "  Batch 5,640  of  18,381.    Elapsed: 0:34:21.\n",
            "  Batch 5,680  of  18,381.    Elapsed: 0:34:35.\n",
            "  Batch 5,720  of  18,381.    Elapsed: 0:34:50.\n",
            "  Batch 5,760  of  18,381.    Elapsed: 0:35:05.\n",
            "  Batch 5,800  of  18,381.    Elapsed: 0:35:19.\n",
            "  Batch 5,840  of  18,381.    Elapsed: 0:35:34.\n",
            "  Batch 5,880  of  18,381.    Elapsed: 0:35:48.\n",
            "  Batch 5,920  of  18,381.    Elapsed: 0:36:03.\n",
            "  Batch 5,960  of  18,381.    Elapsed: 0:36:18.\n",
            "  Batch 6,000  of  18,381.    Elapsed: 0:36:32.\n",
            "  Batch 6,040  of  18,381.    Elapsed: 0:36:47.\n",
            "  Batch 6,080  of  18,381.    Elapsed: 0:37:01.\n",
            "  Batch 6,120  of  18,381.    Elapsed: 0:37:16.\n",
            "  Batch 6,160  of  18,381.    Elapsed: 0:37:31.\n",
            "  Batch 6,200  of  18,381.    Elapsed: 0:37:45.\n",
            "  Batch 6,240  of  18,381.    Elapsed: 0:38:00.\n",
            "  Batch 6,280  of  18,381.    Elapsed: 0:38:15.\n",
            "  Batch 6,320  of  18,381.    Elapsed: 0:38:29.\n",
            "  Batch 6,360  of  18,381.    Elapsed: 0:38:44.\n",
            "  Batch 6,400  of  18,381.    Elapsed: 0:38:58.\n",
            "  Batch 6,440  of  18,381.    Elapsed: 0:39:13.\n",
            "  Batch 6,480  of  18,381.    Elapsed: 0:39:28.\n",
            "  Batch 6,520  of  18,381.    Elapsed: 0:39:42.\n",
            "  Batch 6,560  of  18,381.    Elapsed: 0:39:57.\n",
            "  Batch 6,600  of  18,381.    Elapsed: 0:40:12.\n",
            "  Batch 6,640  of  18,381.    Elapsed: 0:40:26.\n",
            "  Batch 6,680  of  18,381.    Elapsed: 0:40:41.\n",
            "  Batch 6,720  of  18,381.    Elapsed: 0:40:55.\n",
            "  Batch 6,760  of  18,381.    Elapsed: 0:41:10.\n",
            "  Batch 6,800  of  18,381.    Elapsed: 0:41:25.\n",
            "  Batch 6,840  of  18,381.    Elapsed: 0:41:39.\n",
            "  Batch 6,880  of  18,381.    Elapsed: 0:41:54.\n",
            "  Batch 6,920  of  18,381.    Elapsed: 0:42:09.\n",
            "  Batch 6,960  of  18,381.    Elapsed: 0:42:23.\n",
            "  Batch 7,000  of  18,381.    Elapsed: 0:42:38.\n",
            "  Batch 7,040  of  18,381.    Elapsed: 0:42:52.\n",
            "  Batch 7,080  of  18,381.    Elapsed: 0:43:07.\n",
            "  Batch 7,120  of  18,381.    Elapsed: 0:43:22.\n",
            "  Batch 7,160  of  18,381.    Elapsed: 0:43:36.\n",
            "  Batch 7,200  of  18,381.    Elapsed: 0:43:51.\n",
            "  Batch 7,240  of  18,381.    Elapsed: 0:44:06.\n",
            "  Batch 7,280  of  18,381.    Elapsed: 0:44:20.\n",
            "  Batch 7,320  of  18,381.    Elapsed: 0:44:35.\n",
            "  Batch 7,360  of  18,381.    Elapsed: 0:44:49.\n",
            "  Batch 7,400  of  18,381.    Elapsed: 0:45:04.\n",
            "  Batch 7,440  of  18,381.    Elapsed: 0:45:19.\n",
            "  Batch 7,480  of  18,381.    Elapsed: 0:45:33.\n",
            "  Batch 7,520  of  18,381.    Elapsed: 0:45:48.\n",
            "  Batch 7,560  of  18,381.    Elapsed: 0:46:03.\n",
            "  Batch 7,600  of  18,381.    Elapsed: 0:46:17.\n",
            "  Batch 7,640  of  18,381.    Elapsed: 0:46:32.\n",
            "  Batch 7,680  of  18,381.    Elapsed: 0:46:46.\n",
            "  Batch 7,720  of  18,381.    Elapsed: 0:47:01.\n",
            "  Batch 7,760  of  18,381.    Elapsed: 0:47:16.\n",
            "  Batch 7,800  of  18,381.    Elapsed: 0:47:30.\n",
            "  Batch 7,840  of  18,381.    Elapsed: 0:47:45.\n",
            "  Batch 7,880  of  18,381.    Elapsed: 0:48:00.\n",
            "  Batch 7,920  of  18,381.    Elapsed: 0:48:14.\n",
            "  Batch 7,960  of  18,381.    Elapsed: 0:48:29.\n",
            "  Batch 8,000  of  18,381.    Elapsed: 0:48:43.\n",
            "  Batch 8,040  of  18,381.    Elapsed: 0:48:58.\n",
            "  Batch 8,080  of  18,381.    Elapsed: 0:49:13.\n",
            "  Batch 8,120  of  18,381.    Elapsed: 0:49:27.\n",
            "  Batch 8,160  of  18,381.    Elapsed: 0:49:42.\n",
            "  Batch 8,200  of  18,381.    Elapsed: 0:49:57.\n",
            "  Batch 8,240  of  18,381.    Elapsed: 0:50:11.\n",
            "  Batch 8,280  of  18,381.    Elapsed: 0:50:26.\n",
            "  Batch 8,320  of  18,381.    Elapsed: 0:50:40.\n",
            "  Batch 8,360  of  18,381.    Elapsed: 0:50:55.\n",
            "  Batch 8,400  of  18,381.    Elapsed: 0:51:10.\n",
            "  Batch 8,440  of  18,381.    Elapsed: 0:51:24.\n",
            "  Batch 8,480  of  18,381.    Elapsed: 0:51:39.\n",
            "  Batch 8,520  of  18,381.    Elapsed: 0:51:54.\n",
            "  Batch 8,560  of  18,381.    Elapsed: 0:52:08.\n",
            "  Batch 8,600  of  18,381.    Elapsed: 0:52:23.\n",
            "  Batch 8,640  of  18,381.    Elapsed: 0:52:37.\n",
            "  Batch 8,680  of  18,381.    Elapsed: 0:52:52.\n",
            "  Batch 8,720  of  18,381.    Elapsed: 0:53:07.\n",
            "  Batch 8,760  of  18,381.    Elapsed: 0:53:21.\n",
            "  Batch 8,800  of  18,381.    Elapsed: 0:53:36.\n",
            "  Batch 8,840  of  18,381.    Elapsed: 0:53:50.\n",
            "  Batch 8,880  of  18,381.    Elapsed: 0:54:05.\n",
            "  Batch 8,920  of  18,381.    Elapsed: 0:54:20.\n",
            "  Batch 8,960  of  18,381.    Elapsed: 0:54:34.\n",
            "  Batch 9,000  of  18,381.    Elapsed: 0:54:49.\n",
            "  Batch 9,040  of  18,381.    Elapsed: 0:55:04.\n",
            "  Batch 9,080  of  18,381.    Elapsed: 0:55:18.\n",
            "  Batch 9,120  of  18,381.    Elapsed: 0:55:33.\n",
            "  Batch 9,160  of  18,381.    Elapsed: 0:55:47.\n",
            "  Batch 9,200  of  18,381.    Elapsed: 0:56:02.\n",
            "  Batch 9,240  of  18,381.    Elapsed: 0:56:17.\n",
            "  Batch 9,280  of  18,381.    Elapsed: 0:56:31.\n",
            "  Batch 9,320  of  18,381.    Elapsed: 0:56:46.\n",
            "  Batch 9,360  of  18,381.    Elapsed: 0:57:00.\n",
            "  Batch 9,400  of  18,381.    Elapsed: 0:57:15.\n",
            "  Batch 9,440  of  18,381.    Elapsed: 0:57:30.\n",
            "  Batch 9,480  of  18,381.    Elapsed: 0:57:44.\n",
            "  Batch 9,520  of  18,381.    Elapsed: 0:57:59.\n",
            "  Batch 9,560  of  18,381.    Elapsed: 0:58:14.\n",
            "  Batch 9,600  of  18,381.    Elapsed: 0:58:28.\n",
            "  Batch 9,640  of  18,381.    Elapsed: 0:58:43.\n",
            "  Batch 9,680  of  18,381.    Elapsed: 0:58:57.\n",
            "  Batch 9,720  of  18,381.    Elapsed: 0:59:12.\n",
            "  Batch 9,760  of  18,381.    Elapsed: 0:59:27.\n",
            "  Batch 9,800  of  18,381.    Elapsed: 0:59:41.\n",
            "  Batch 9,840  of  18,381.    Elapsed: 0:59:56.\n",
            "  Batch 9,880  of  18,381.    Elapsed: 1:00:10.\n",
            "  Batch 9,920  of  18,381.    Elapsed: 1:00:25.\n",
            "  Batch 9,960  of  18,381.    Elapsed: 1:00:40.\n",
            "  Batch 10,000  of  18,381.    Elapsed: 1:00:54.\n",
            "  Batch 10,040  of  18,381.    Elapsed: 1:01:09.\n",
            "  Batch 10,080  of  18,381.    Elapsed: 1:01:23.\n",
            "  Batch 10,120  of  18,381.    Elapsed: 1:01:38.\n",
            "  Batch 10,160  of  18,381.    Elapsed: 1:01:53.\n",
            "  Batch 10,200  of  18,381.    Elapsed: 1:02:07.\n",
            "  Batch 10,240  of  18,381.    Elapsed: 1:02:22.\n",
            "  Batch 10,280  of  18,381.    Elapsed: 1:02:37.\n",
            "  Batch 10,320  of  18,381.    Elapsed: 1:02:51.\n",
            "  Batch 10,360  of  18,381.    Elapsed: 1:03:06.\n",
            "  Batch 10,400  of  18,381.    Elapsed: 1:03:20.\n",
            "  Batch 10,440  of  18,381.    Elapsed: 1:03:35.\n",
            "  Batch 10,480  of  18,381.    Elapsed: 1:03:50.\n",
            "  Batch 10,520  of  18,381.    Elapsed: 1:04:04.\n",
            "  Batch 10,560  of  18,381.    Elapsed: 1:04:19.\n",
            "  Batch 10,600  of  18,381.    Elapsed: 1:04:33.\n",
            "  Batch 10,640  of  18,381.    Elapsed: 1:04:48.\n",
            "  Batch 10,680  of  18,381.    Elapsed: 1:05:03.\n",
            "  Batch 10,720  of  18,381.    Elapsed: 1:05:17.\n",
            "  Batch 10,760  of  18,381.    Elapsed: 1:05:32.\n",
            "  Batch 10,800  of  18,381.    Elapsed: 1:05:46.\n",
            "  Batch 10,840  of  18,381.    Elapsed: 1:06:01.\n",
            "  Batch 10,880  of  18,381.    Elapsed: 1:06:16.\n",
            "  Batch 10,920  of  18,381.    Elapsed: 1:06:30.\n",
            "  Batch 10,960  of  18,381.    Elapsed: 1:06:45.\n",
            "  Batch 11,000  of  18,381.    Elapsed: 1:06:59.\n",
            "  Batch 11,040  of  18,381.    Elapsed: 1:07:14.\n",
            "  Batch 11,080  of  18,381.    Elapsed: 1:07:29.\n",
            "  Batch 11,120  of  18,381.    Elapsed: 1:07:43.\n",
            "  Batch 11,160  of  18,381.    Elapsed: 1:07:58.\n",
            "  Batch 11,200  of  18,381.    Elapsed: 1:08:12.\n",
            "  Batch 11,240  of  18,381.    Elapsed: 1:08:27.\n",
            "  Batch 11,280  of  18,381.    Elapsed: 1:08:42.\n",
            "  Batch 11,320  of  18,381.    Elapsed: 1:08:56.\n",
            "  Batch 11,360  of  18,381.    Elapsed: 1:09:11.\n",
            "  Batch 11,400  of  18,381.    Elapsed: 1:09:25.\n",
            "  Batch 11,440  of  18,381.    Elapsed: 1:09:40.\n",
            "  Batch 11,480  of  18,381.    Elapsed: 1:09:55.\n",
            "  Batch 11,520  of  18,381.    Elapsed: 1:10:09.\n",
            "  Batch 11,560  of  18,381.    Elapsed: 1:10:24.\n",
            "  Batch 11,600  of  18,381.    Elapsed: 1:10:38.\n",
            "  Batch 11,640  of  18,381.    Elapsed: 1:10:53.\n",
            "  Batch 11,680  of  18,381.    Elapsed: 1:11:08.\n",
            "  Batch 11,720  of  18,381.    Elapsed: 1:11:22.\n",
            "  Batch 11,760  of  18,381.    Elapsed: 1:11:37.\n",
            "  Batch 11,800  of  18,381.    Elapsed: 1:11:51.\n",
            "  Batch 11,840  of  18,381.    Elapsed: 1:12:06.\n",
            "  Batch 11,880  of  18,381.    Elapsed: 1:12:21.\n",
            "  Batch 11,920  of  18,381.    Elapsed: 1:12:35.\n",
            "  Batch 11,960  of  18,381.    Elapsed: 1:12:50.\n",
            "  Batch 12,000  of  18,381.    Elapsed: 1:13:04.\n",
            "  Batch 12,040  of  18,381.    Elapsed: 1:13:19.\n",
            "  Batch 12,080  of  18,381.    Elapsed: 1:13:34.\n",
            "  Batch 12,120  of  18,381.    Elapsed: 1:13:48.\n",
            "  Batch 12,160  of  18,381.    Elapsed: 1:14:03.\n",
            "  Batch 12,200  of  18,381.    Elapsed: 1:14:17.\n",
            "  Batch 12,240  of  18,381.    Elapsed: 1:14:32.\n",
            "  Batch 12,280  of  18,381.    Elapsed: 1:14:47.\n",
            "  Batch 12,320  of  18,381.    Elapsed: 1:15:01.\n",
            "  Batch 12,360  of  18,381.    Elapsed: 1:15:16.\n",
            "  Batch 12,400  of  18,381.    Elapsed: 1:15:30.\n",
            "  Batch 12,440  of  18,381.    Elapsed: 1:15:45.\n",
            "  Batch 12,480  of  18,381.    Elapsed: 1:16:00.\n",
            "  Batch 12,520  of  18,381.    Elapsed: 1:16:14.\n",
            "  Batch 12,560  of  18,381.    Elapsed: 1:16:29.\n",
            "  Batch 12,600  of  18,381.    Elapsed: 1:16:44.\n",
            "  Batch 12,640  of  18,381.    Elapsed: 1:16:58.\n",
            "  Batch 12,680  of  18,381.    Elapsed: 1:17:13.\n",
            "  Batch 12,720  of  18,381.    Elapsed: 1:17:27.\n",
            "  Batch 12,760  of  18,381.    Elapsed: 1:17:42.\n",
            "  Batch 12,800  of  18,381.    Elapsed: 1:17:57.\n",
            "  Batch 12,840  of  18,381.    Elapsed: 1:18:11.\n",
            "  Batch 12,880  of  18,381.    Elapsed: 1:18:26.\n",
            "  Batch 12,920  of  18,381.    Elapsed: 1:18:40.\n",
            "  Batch 12,960  of  18,381.    Elapsed: 1:18:55.\n",
            "  Batch 13,000  of  18,381.    Elapsed: 1:19:10.\n",
            "  Batch 13,040  of  18,381.    Elapsed: 1:19:24.\n",
            "  Batch 13,080  of  18,381.    Elapsed: 1:19:39.\n",
            "  Batch 13,120  of  18,381.    Elapsed: 1:19:53.\n",
            "  Batch 13,160  of  18,381.    Elapsed: 1:20:08.\n",
            "  Batch 13,200  of  18,381.    Elapsed: 1:20:23.\n",
            "  Batch 13,240  of  18,381.    Elapsed: 1:20:37.\n",
            "  Batch 13,280  of  18,381.    Elapsed: 1:20:52.\n",
            "  Batch 13,320  of  18,381.    Elapsed: 1:21:06.\n",
            "  Batch 13,360  of  18,381.    Elapsed: 1:21:21.\n",
            "  Batch 13,400  of  18,381.    Elapsed: 1:21:36.\n",
            "  Batch 13,440  of  18,381.    Elapsed: 1:21:50.\n",
            "  Batch 13,480  of  18,381.    Elapsed: 1:22:05.\n",
            "  Batch 13,520  of  18,381.    Elapsed: 1:22:20.\n",
            "  Batch 13,560  of  18,381.    Elapsed: 1:22:34.\n",
            "  Batch 13,600  of  18,381.    Elapsed: 1:22:49.\n",
            "  Batch 13,640  of  18,381.    Elapsed: 1:23:03.\n",
            "  Batch 13,680  of  18,381.    Elapsed: 1:23:18.\n",
            "  Batch 13,720  of  18,381.    Elapsed: 1:23:33.\n",
            "  Batch 13,760  of  18,381.    Elapsed: 1:23:47.\n",
            "  Batch 13,800  of  18,381.    Elapsed: 1:24:02.\n",
            "  Batch 13,840  of  18,381.    Elapsed: 1:24:16.\n",
            "  Batch 13,880  of  18,381.    Elapsed: 1:24:31.\n",
            "  Batch 13,920  of  18,381.    Elapsed: 1:24:46.\n",
            "  Batch 13,960  of  18,381.    Elapsed: 1:25:00.\n",
            "  Batch 14,000  of  18,381.    Elapsed: 1:25:15.\n",
            "  Batch 14,040  of  18,381.    Elapsed: 1:25:29.\n",
            "  Batch 14,080  of  18,381.    Elapsed: 1:25:44.\n",
            "  Batch 14,120  of  18,381.    Elapsed: 1:25:59.\n",
            "  Batch 14,160  of  18,381.    Elapsed: 1:26:13.\n",
            "  Batch 14,200  of  18,381.    Elapsed: 1:26:28.\n",
            "  Batch 14,240  of  18,381.    Elapsed: 1:26:43.\n",
            "  Batch 14,280  of  18,381.    Elapsed: 1:26:57.\n",
            "  Batch 14,320  of  18,381.    Elapsed: 1:27:12.\n",
            "  Batch 14,360  of  18,381.    Elapsed: 1:27:26.\n",
            "  Batch 14,400  of  18,381.    Elapsed: 1:27:41.\n",
            "  Batch 14,440  of  18,381.    Elapsed: 1:27:56.\n",
            "  Batch 14,480  of  18,381.    Elapsed: 1:28:10.\n",
            "  Batch 14,520  of  18,381.    Elapsed: 1:28:25.\n",
            "  Batch 14,560  of  18,381.    Elapsed: 1:28:40.\n",
            "  Batch 14,600  of  18,381.    Elapsed: 1:28:54.\n",
            "  Batch 14,640  of  18,381.    Elapsed: 1:29:09.\n",
            "  Batch 14,680  of  18,381.    Elapsed: 1:29:23.\n",
            "  Batch 14,720  of  18,381.    Elapsed: 1:29:38.\n",
            "  Batch 14,760  of  18,381.    Elapsed: 1:29:53.\n",
            "  Batch 14,800  of  18,381.    Elapsed: 1:30:07.\n",
            "  Batch 14,840  of  18,381.    Elapsed: 1:30:22.\n",
            "  Batch 14,880  of  18,381.    Elapsed: 1:30:36.\n",
            "  Batch 14,920  of  18,381.    Elapsed: 1:30:51.\n",
            "  Batch 14,960  of  18,381.    Elapsed: 1:31:06.\n",
            "  Batch 15,000  of  18,381.    Elapsed: 1:31:20.\n",
            "  Batch 15,040  of  18,381.    Elapsed: 1:31:35.\n",
            "  Batch 15,080  of  18,381.    Elapsed: 1:31:49.\n",
            "  Batch 15,120  of  18,381.    Elapsed: 1:32:04.\n",
            "  Batch 15,160  of  18,381.    Elapsed: 1:32:19.\n",
            "  Batch 15,200  of  18,381.    Elapsed: 1:32:33.\n",
            "  Batch 15,240  of  18,381.    Elapsed: 1:32:48.\n",
            "  Batch 15,280  of  18,381.    Elapsed: 1:33:03.\n",
            "  Batch 15,320  of  18,381.    Elapsed: 1:33:17.\n",
            "  Batch 15,360  of  18,381.    Elapsed: 1:33:32.\n",
            "  Batch 15,400  of  18,381.    Elapsed: 1:33:46.\n",
            "  Batch 15,440  of  18,381.    Elapsed: 1:34:01.\n",
            "  Batch 15,480  of  18,381.    Elapsed: 1:34:16.\n",
            "  Batch 15,520  of  18,381.    Elapsed: 1:34:30.\n",
            "  Batch 15,560  of  18,381.    Elapsed: 1:34:45.\n",
            "  Batch 15,600  of  18,381.    Elapsed: 1:34:59.\n",
            "  Batch 15,640  of  18,381.    Elapsed: 1:35:14.\n",
            "  Batch 15,680  of  18,381.    Elapsed: 1:35:29.\n",
            "  Batch 15,720  of  18,381.    Elapsed: 1:35:43.\n",
            "  Batch 15,760  of  18,381.    Elapsed: 1:35:58.\n",
            "  Batch 15,800  of  18,381.    Elapsed: 1:36:13.\n",
            "  Batch 15,840  of  18,381.    Elapsed: 1:36:27.\n",
            "  Batch 15,880  of  18,381.    Elapsed: 1:36:42.\n",
            "  Batch 15,920  of  18,381.    Elapsed: 1:36:56.\n",
            "  Batch 15,960  of  18,381.    Elapsed: 1:37:11.\n",
            "  Batch 16,000  of  18,381.    Elapsed: 1:37:26.\n",
            "  Batch 16,040  of  18,381.    Elapsed: 1:37:40.\n",
            "  Batch 16,080  of  18,381.    Elapsed: 1:37:55.\n",
            "  Batch 16,120  of  18,381.    Elapsed: 1:38:10.\n",
            "  Batch 16,160  of  18,381.    Elapsed: 1:38:24.\n",
            "  Batch 16,200  of  18,381.    Elapsed: 1:38:39.\n",
            "  Batch 16,240  of  18,381.    Elapsed: 1:38:54.\n",
            "  Batch 16,280  of  18,381.    Elapsed: 1:39:08.\n",
            "  Batch 16,320  of  18,381.    Elapsed: 1:39:23.\n",
            "  Batch 16,360  of  18,381.    Elapsed: 1:39:37.\n",
            "  Batch 16,400  of  18,381.    Elapsed: 1:39:52.\n",
            "  Batch 16,440  of  18,381.    Elapsed: 1:40:07.\n",
            "  Batch 16,480  of  18,381.    Elapsed: 1:40:21.\n",
            "  Batch 16,520  of  18,381.    Elapsed: 1:40:36.\n",
            "  Batch 16,560  of  18,381.    Elapsed: 1:40:50.\n",
            "  Batch 16,600  of  18,381.    Elapsed: 1:41:05.\n",
            "  Batch 16,640  of  18,381.    Elapsed: 1:41:20.\n",
            "  Batch 16,680  of  18,381.    Elapsed: 1:41:34.\n",
            "  Batch 16,720  of  18,381.    Elapsed: 1:41:49.\n",
            "  Batch 16,760  of  18,381.    Elapsed: 1:42:04.\n",
            "  Batch 16,800  of  18,381.    Elapsed: 1:42:18.\n",
            "  Batch 16,840  of  18,381.    Elapsed: 1:42:33.\n",
            "  Batch 16,880  of  18,381.    Elapsed: 1:42:47.\n",
            "  Batch 16,920  of  18,381.    Elapsed: 1:43:02.\n",
            "  Batch 16,960  of  18,381.    Elapsed: 1:43:16.\n",
            "  Batch 17,000  of  18,381.    Elapsed: 1:43:31.\n",
            "  Batch 17,040  of  18,381.    Elapsed: 1:43:46.\n",
            "  Batch 17,080  of  18,381.    Elapsed: 1:44:00.\n",
            "  Batch 17,120  of  18,381.    Elapsed: 1:44:15.\n",
            "  Batch 17,160  of  18,381.    Elapsed: 1:44:30.\n",
            "  Batch 17,200  of  18,381.    Elapsed: 1:44:44.\n",
            "  Batch 17,240  of  18,381.    Elapsed: 1:44:59.\n",
            "  Batch 17,280  of  18,381.    Elapsed: 1:45:14.\n",
            "  Batch 17,320  of  18,381.    Elapsed: 1:45:28.\n",
            "  Batch 17,360  of  18,381.    Elapsed: 1:45:43.\n",
            "  Batch 17,400  of  18,381.    Elapsed: 1:45:57.\n",
            "  Batch 17,440  of  18,381.    Elapsed: 1:46:12.\n",
            "  Batch 17,480  of  18,381.    Elapsed: 1:46:27.\n",
            "  Batch 17,520  of  18,381.    Elapsed: 1:46:41.\n",
            "  Batch 17,560  of  18,381.    Elapsed: 1:46:56.\n",
            "  Batch 17,600  of  18,381.    Elapsed: 1:47:11.\n",
            "  Batch 17,640  of  18,381.    Elapsed: 1:47:25.\n",
            "  Batch 17,680  of  18,381.    Elapsed: 1:47:40.\n",
            "  Batch 17,720  of  18,381.    Elapsed: 1:47:54.\n",
            "  Batch 17,760  of  18,381.    Elapsed: 1:48:09.\n",
            "  Batch 17,800  of  18,381.    Elapsed: 1:48:24.\n",
            "  Batch 17,840  of  18,381.    Elapsed: 1:48:38.\n",
            "  Batch 17,880  of  18,381.    Elapsed: 1:48:53.\n",
            "  Batch 17,920  of  18,381.    Elapsed: 1:49:08.\n",
            "  Batch 17,960  of  18,381.    Elapsed: 1:49:22.\n",
            "  Batch 18,000  of  18,381.    Elapsed: 1:49:37.\n",
            "  Batch 18,040  of  18,381.    Elapsed: 1:49:51.\n",
            "  Batch 18,080  of  18,381.    Elapsed: 1:50:06.\n",
            "  Batch 18,120  of  18,381.    Elapsed: 1:50:21.\n",
            "  Batch 18,160  of  18,381.    Elapsed: 1:50:35.\n",
            "  Batch 18,200  of  18,381.    Elapsed: 1:50:50.\n",
            "  Batch 18,240  of  18,381.    Elapsed: 1:51:05.\n",
            "  Batch 18,280  of  18,381.    Elapsed: 1:51:19.\n",
            "  Batch 18,320  of  18,381.    Elapsed: 1:51:34.\n",
            "  Batch 18,360  of  18,381.    Elapsed: 1:51:48.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 1:51:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:16:17\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  18,381.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  18,381.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  18,381.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  18,381.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  18,381.    Elapsed: 0:01:13.\n",
            "  Batch   240  of  18,381.    Elapsed: 0:01:28.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9883a7adeac8>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# The optimizer dictates the \"update rule\"--how the parameters are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# modified based on their gradients, the learning rate, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Update the learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accent(input):\n",
        "    model.eval()\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        input,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "    ).to(device)\n",
        "    output = model(encoded_dict['input_ids'],\n",
        "                token_type_ids=encoded_dict['token_type_ids'],\n",
        "                attention_mask=encoded_dict['attention_mask'])\n",
        "    pos = torch.argmax(output.logits)\n",
        "    answer = input[:pos] + \"^\" + input[pos:]\n",
        "    return answer"
      ],
      "metadata": {
        "id": "9rG4AwIeZZHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_accent('привет'))\n",
        "print(get_accent('особенный'))\n",
        "print(get_accent('революция'))\n",
        "print(get_accent('геншин'))\n",
        "print(get_accent('старость'))\n",
        "print(get_accent('нейросеть'))\n",
        "print(get_accent('евгений'))\n",
        "print(get_accent('измученный'))\n",
        "print(get_accent('православие'))\n",
        "print(get_accent('абстракция'))\n",
        "print(get_accent('поттер'))\n",
        "print(get_accent('конструктивизм'))\n",
        "print(get_accent('металлика'))"
      ],
      "metadata": {
        "id": "BHzCIVNoWeqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58990cb1-9407-406d-848b-beeb7ecdd5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "прив^ет\n",
            "ос^обенный\n",
            "револ^юция\n",
            "г^еншин\n",
            "ст^арость\n",
            "нейрос^еть\n",
            "евг^ений\n",
            "изм^ученный\n",
            "правосл^авие\n",
            "абстр^акция\n",
            "п^оттер\n",
            "конструктив^изм\n",
            "мет^аллика\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "# pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "GMEjzkL3Izyq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "8e6a5cce-c0a9-4ce2-c458-919661ff726a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1           0.845599      0.46891       0.815326       1:51:46         0:16:19\n",
              "2           0.450438      0.35385       0.862784       1:51:56         0:16:17"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ac0d635-f4ad-4983-b603-60ad8e1cb23f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.845599</td>\n",
              "      <td>0.46891</td>\n",
              "      <td>0.815326</td>\n",
              "      <td>1:51:46</td>\n",
              "      <td>0:16:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.450438</td>\n",
              "      <td>0.35385</td>\n",
              "      <td>0.862784</td>\n",
              "      <td>1:51:56</td>\n",
              "      <td>0:16:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ac0d635-f4ad-4983-b603-60ad8e1cb23f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ac0d635-f4ad-4983-b603-60ad8e1cb23f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ac0d635-f4ad-4983-b603-60ad8e1cb23f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b15f40c2-0f45-43be-8c6f-eaf876a3b5ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b15f40c2-0f45-43be-8c6f-eaf876a3b5ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b15f40c2-0f45-43be-8c6f-eaf876a3b5ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hu7JT4UgI_ck",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "0043c696-7ba6-4213-864b-a8b407824320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABAwAAAI/CAYAAADz3UtPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACZk0lEQVR4nOzdd3hUVf7H8fedmUx6hYQQem+BhKKiYkNUEBREioiuZRUFO7qrq6vrWtDfqliwF0BRXEBpopRFRRGlk5DQe4cE0nsyM78/2MwSkyCpN5N8Xs/jo84999zvhOSQ+5lzzzFcLpcLEREREREREZEzWMwuQERERERERETqHgUGIiIiIiIiIlKKAgMRERERERERKUWBgYiIiIiIiIiUosBAREREREREREpRYCAiIiIiIiIipSgwEBEREREREZFSFBiIiIiIiIiISCkKDERERERERESkFAUGIiLisdasWUOnTp3o1KlTtfc9d+5cOnXqRP/+/au9b6ket956K506dWLKlCkVOlbVvmtD//796dSpE3PnzjXl+iIiIgA2swsQEZG6rSo34y+99BLDhw+vxmqkouLj45k6dSobNmwgLS2N4OBgmjZtysUXX8ygQYPo3Llzpfo9duwY/fv3x+l08te//pU///nP53Te/Pnzefzxx4HToUy3bt0qdX1PNXfuXI4cOcL555/PBRdcYHY51e6JJ55g3rx5NGvWjB9++MHsckREpIoUGIiIyFk1bty4zNdzcnLIyck5axsfH58aqwvA19eXNm3a1EjfgYGBtGnThiZNmtRI/7Xhq6++4umnn8bpdAKnv145OTkkJCSQkJDAxo0bmTFjRqX6btq0KRdddBG//PILc+fOPefA4OuvvwagS5cuNRoWNG3alDZt2hAaGlpj16iMefPmsXbtWu6///6zBgYtWrTAbrcTGBhYi9WJiIiUpMBARETOatWqVWW+PmXKFN5+++2ztqlpPXr0YMmSJTXS91VXXcVVV11VI33XhpSUFJ577jmcTiddunThhRdeIDo6GoBDhw7xww8/sHfv3ipdY8SIEfzyyy/s3r2b+Ph4YmJiztr+0KFDrFu3DoAbb7yxStf+I//6179qtP+a9umnn5pdgoiIiAIDERGR+mj9+vXk5+cD8Morr9ChQwf3sRYtWnDbbbdV+RpXXnklISEhpKWl8fXXX/9hYDB37lxcLhd2u53rrruuytcXERGRmqXAQEREakTx2gefffYZ7du358MPP2TFihUcP36cvLw8duzYAUBubi7ff/89P//8Mzt27ODEiRNkZWUREhJCjx49GD16NJdddlmZ11izZg1/+tOfANz9FZs7dy5/+9vf3M9SJyYm8tFHH7mf5W/SpAkDBgxgwoQJBAcHl+r79+efqXh2xfnnn8+MGTP47bffmDZtGps3byY7O5vmzZszePBg7r77bry9vcv9Gi1fvpzPPvuMrVu34nA4aNGiBddddx23334777//folrVJTVanX/d009VmG32xk6dCiffvop3377LU8++WS5j6E4nU7mz58PnJ69ERISAsDOnTtZunQp69at4+jRoyQlJWGz2WjZsiWXXXYZt912G2FhYRWu7dZbb3VP/X/ggQdKHXc4HMycOZO5c+eyb98+7HY7nTp1YuzYsQwcOPCsfR86dIjFixezZs0aDh8+zIkTJzAMw702xB133EFUVFSJc4q/n4q9/fbb7hk6xb7//nuaN28OnF708MiRI+WuA+JwOJg3bx4LFy5kx44dZGdnExoaSs+ePRk7dmy5jzuc+XW5//77mTNnDnPmzGHPnj24XC46duzIzTffzNChQ8/6NagJycnJTJ06lZ9//pkjR44A0KxZMy677DLuvPPOch99Sk9PZ/r06axYsYIDBw5QUFBAcHAwYWFh9OzZk0GDBnHhhReWOCcvL48vvviCZcuWsXfvXnJycggMDCQsLIzu3bvTv39/rrnmmhp/zyIidZ0CAxERqVEHDx5k4sSJnDx5Em9vb2y2kn/1LF682H0jZRgGAQEB2Gw2kpOT+f777/n++++588473QvlVcY333zD3/72NwoLCwkMDMThcHD48GGmT5/OqlWrmDVrFv7+/pXq++OPP+bVV18FTq97UFhYyN69e5kyZQpr165l2rRpJW7ei/3f//0fU6dOdf9/UFAQe/bs4dVXX+Wnn36id+/elXuz/3XhhRcSFhZGSkoKn332Gffff3+V+ivPiBEj+PTTT8nKymLp0qXl3mj+9ttvHD16FCj5OMK9997rvjn09vbG19eX9PR0tm3bxrZt25g3bx7Tp0+nbdu21VZzQUEB48eP55dffgHAYrHg5eXFunXrWLt2LXffffdZz3/yySdZu3YtAF5eXvj7+5ORkcGePXvYs2cP8+bN4/3336dPnz7uc3x8fGjcuDHp6ekUFhbi5+eHn59fiX7L+j4pS2ZmJhMmTHDXYLVa8ff3Jzk5maVLl7J06dI//JlxOBzcd999fP/999hsNnx8fMjOziYuLo64uDgOHDjAgw8+eE71VIe1a9dy3333kZGRAeD+2uzevZvdu3fz1Vdf8e6775b4mgIcP36cMWPGuL+3LBYLgYGBpKamcvLkSXbu3Mm+fftKBAZZWVmMHTuW7du3A6fHncDAQDIzM0lNTWXPnj2sW7dOgYGICNpWUUREatikSZMIDAxk+vTpxMXFsXHjxhLrDgQFBXHnnXcyc+ZMNm3axPr164mLi2PlypU88MADeHl5MXXqVL7//vtKXT8lJYUnn3ySYcOGsWLFCtavX8/GjRt55pln8PLyYteuXXz88ceV6nv79u289tprjBs3jl9//ZV169axfv167rvvPuD0DIh58+aVOu/bb791hwVDhgzh559/Zt26dWzcuJHnn3+ezZs38+WXX1aqpmJ+fn7uG8Z33nmHhQsXVqm/8nTs2JEePXoA/1vQsCzFx5o1a1bi5u28887j5Zdf5scff2Tz5s2sWbOGzZs3M336dHr06MGJEyd47LHHqrXm1157jV9++QXDMHj44YdZt24d69atY9WqVYwZM4aPPvqIbdu2lXt+586deeaZZ1i6dKm75oSEBObMmcMll1xCZmYmjzzyCHl5ee5zrr32WlatWkXPnj0BuPPOO1m1alWJf5o2bXpO9T/11FOsXbsWLy8v/v73v7NhwwbWrVvHypUr3WHM1KlTz/o9NHPmTNauXcvLL7/Mhg0b2LBhAz/99BNXXHEFAO+99x779+8/p3qq6tixY+6woH379u6xYNOmTXzxxRe0adOG9PR07rvvPk6cOFHi3ClTpnD06FGaNWvG9OnTSUxMZO3atSQkJPDDDz/w7LPPlnpU5rPPPmP79u2EhIQwZcoUNm/ezLp160hISODnn3/m//7v/7j44otr5b2LiNR1CgxERKRGWSwWpk+fzoUXXojFcvqvnTN3NhgwYACPP/44vXv3xtfX1/16REQE999/P4888ghApVfzz83NZfDgwbzwwgvuGzJfX1/Gjh3LLbfcApy+ga+MjIwMJkyYwMSJE93T5gMCAnjwwQe5+uqry+zb5XLx5ptvAnDxxRfz6quvuh8Z8Pb2ZtSoUTz77LOkp6dXqqZiR44ccQchTqeTJ5544qw39FUxYsQI4PSnxIcOHSp1PD09neXLlwMwfPhw9/cBnJ5pccMNN5SYwm+327nwwguZPn06jRs3ZsuWLaxfv75aaj1x4gSff/45AOPHj2f8+PEEBAQA0KhRI5599lmGDBlCZmZmuX089dRTjB07ltatW7vfi81mo0ePHnzwwQd06tSJpKQkli5dWi01nyk+Pt7d79NPP82tt97q/rkJDw9n0qRJ7k/G33zzTfc6Fr+Xnp7O22+/zQ033OB+jCQyMpK33nqLiIgInE4nixcvrvb6y/L++++TkZFBcHAw06dPLzG7pk+fPkyfPp2AgADS0tL44IMPSpy7adMmACZOnMiFF17onqVhtVpp1qwZY8aMKRU4FZ9z5513cvXVV2O324HTY1WTJk0YNmwYzz//fI29XxERT6LAQEREatTQoUOJjIys9PmXX345AHFxcTgcjkr1MX78+DJfv/LKKwE4cOAAubm5Fe7Xbrdz5513nrXv36+tsG3bNg4cOADAPffcg2EYpc79/Q10RaWnp3Pbbbexa9cuxowZw5tvvolhGDz11FPlBi9ffPEFnTp1qtQ07MGDB+Pr64vL5SpzRsWiRYvIz8/HYrFwww03nHO//v7+nHfeeQBs3LixwnWVZenSpRQVFeHj41PuVpBVeXzDarVyySWXALBhw4ZK91Oe7777Djh9cz9y5Mgy2zz00EMApKamlruDSa9evejbt2+p1+12O/369QNKf+/WBJfL5Z5xdNNNNxEeHl6qTWRkJDfddBNQOoALCgoCTq9/cK4qc46ISEOlNQxERKRG9erV6w/bnDx5kpkzZ7Jq1Sr2799PZmZmqXAgNzeX9PT0Ci+AFxISQqtWrco8FhER4f7vjIyMEjMczkWHDh3KXfuguO/fzxTYsmULcPrZ9+Lp6b9nGAbnnXceCxYsqFA9xV544QUOHTpEbGwsTz/9NFarFYfDwV/+8hdeeOEFcnJyuOeee0qcUzzVu0uXLhW+XkBAANdccw3z589n/vz53H///SVmERTPbLjwwgtp1qxZqfN//PFHFixYQEJCAqdOnSozvDl+/HiF6ypLYmIiANHR0e6ZBb/Xpk0bmjRpUmr6+5nWr1/PV199RVxcHCdOnCAnJ6dUm7OdX1nF9V9wwQUlvsZnateunbv+xMRE+vfvX6rN2Xa0KO97tyYcPnyYtLQ0gFILE57p4osv5uOPPyYtLY1Dhw7RokUL4HSguGnTJl577TX27t3LVVddRa9evcr9sy0+Z9GiRXz++eekpKRw7bXX0qtXr0otrikiUt8pMBARkRrVqFGjsx7ftGkT48aNcy92Bqefv/f19cUwDBwOB6mpqQCVmgVwtsUMz1xkrrCwsEb6LioqKvF68XsJCQlxT4UuS2V3NkhOTnZ/Cj1hwgR3HYMHD6awsJC//e1vTJ48mezsbCZOnOg+b926dQDuZ9grasSIEcyfP58jR47w22+/uZ8B3759uzskKX50oZjT6eQvf/kLixYtcr9ms9kIDg7Gy8sLOL3AX35+fqX+7Mty6tQp4I+/vpGRkeXe8L/yyisl1r2wWq0las7JyXH/U90qWn9x+9872/du8cKkv//erQln1ne293TmsZSUFHdg8Oc//5nt27ezePFiZs+ezezZszEMgw4dOtCvXz9GjhxZasHM6667js2bN/P555/z7bffumcttGrViosvvpgbb7yR6Ojo6nybIiIeS48kiIhIjSrvU1A4fUPy6KOPkpGRQZcuXfjwww/ZsGEDmzZt4tdff2XVqlXMnj3b3d7lctVGyR5t69at7hu93++0MGzYMF544QUMw+CDDz7ghRdewOVysXfvXjZt2kRwcDADBgyo1HXPO+88WrduDZzeQrBY8X+HhISU6vurr75i0aJFWK1W7rvvPpYtW0ZCQgJr1651LwRY/IhEXfmzX7VqlTssuPnmm/nmm29K1XzbbbeZXGXD4eXlxRtvvMGCBQu477776Nu3L76+vuzcuZOpU6cyZMiQEruRFHvqqadYsmQJEydO5NJLLyUoKIgDBw4wc+ZMbrzxRl588UUT3o2ISN2jGQYiImKauLg4jhw5gtVq5YMPPijzE8b69pxxaGgoAGlpaRQUFJQ7y6Cy09mzs7PPevzGG2+kqKiIf/zjH8yYMYPs7GwyMjJwuVzcdtttld5esrjv1157jf/85z/uRzyKd2e47rrrSr3X4k92R4wYUe4WfidPnqx0PWUpnvHyR1/f8o4X19yvXz/+8Y9/lNmmums+U6NGjdi3b98fPqJRfPyPZviY7cz6Tpw4Ue72mWf+eZT16EDnzp3p3LkzcDqIXLduHe+88w7r1q3jX//6FxdddJH7eLFWrVpxzz33cM899+B0Otm8eTMfffQRy5cv57PPPqNv377utUhERBoqzTAQERHTHDt2DDh9A1DedOTffvutNkuqcd26dQNOPwJRvFr777lcrkrvClA8VRtg9erVZbYZPXo0Tz/9NHB6BsDy5ctp06YNd911V6WuWWzYsGFYrVby8/P55ptv+OGHH9yPYPz+cQT4301t165dy+wvOzub+Pj4KtX0e8VTzRMTE8sNV/bv31/uDfkf1exyucr9ugPuRS4rO2OiuP41a9bgdDrLbLNnzx73DXb37t0rdZ3a0rx5c0JCQoCz/6z/+uuvwOmZKmd+j5fFZrNx4YUX8sEHH2C323G5XO7zy2OxWIiNjeWtt95yLzj6R+eIiDQECgxERMQ0gYGBwOlPZMv6VPb48eOV3k6xrurSpYt7EcYPP/ywzBvHBQsWcOTIkUr1Hx0dTcuWLYHTz9oX37D/3tixYxk0aJD7/zt37oy3t3elrlksIiKCSy+9FDgdRBQ/jtCtW7dSn+4C7oXptm/fXmZ/77777h/OmKioa665BqvVSl5eXplT1QHeeeedcs//o5q//PLLMreW/P35Z67ZURGDBw8GTn/iPmfOnDLbvPXWW8Dp2SwXXXRRpa5TWwzDcH8fzpo1q8wZRSdOnGDWrFkADBkypMSxgoKCcvu22+3uNTzOfDTqbOdYrVb3WhRl7WAiItLQKDAQERHT9O7dGz8/P1wuFw8//DD79u0DwOFwsHLlSm699VaTK6x+hmHwwAMPAPDLL7/w+OOPuz8Nzs/PZ86cOfzjH/8gODi40v0/88wzWK1W9u/fz8iRI1m6dCn5+fnA6a/txo0befDBB1m8eLH7pmjx4sW8/vrrVX5/xTMJEhMT+fnnn4HTjyqUpXj7wTlz5jBr1iz3jVxycjKTJk3i448/dn/6XF2aNGnCzTffDJwOJD744AOysrKA04vpPffccyxcuNAdZpVX888//8w777zjXtgwIyOD999/nxdeeOGsNXfo0MF9fmUeO+nRo4d7XYfnn3+ezz//3L0gZHJyMn//+9/d2xQ+9NBDVQ6BKsvpdJKSknLWf4q/7vfeey9BQUGkpaVxxx13lNhCc8OGDdxxxx1kZGQQEhLCuHHjSlzniiuu4LXXXiMuLq5EEHDgwAEee+wxcnNzsVgs7q0iAUaOHMkLL7zAmjVrSixMeeLECZ5//nn3tqeXXXZZjXxtREQ8idYwEBER0wQGBvLXv/6VZ599lnXr1jFw4ED8/PxwOBzk5+cTGhrKSy+9xPjx480utVpdd911JCQk8Omnn7JgwQIWLlxIUFAQOTk5FBYW0rdvX2JiYtxTqivqkksuYfLkyTz11FMcOnSIBx98EJvNRkBAANnZ2e4dIaKiopg0aRI///wzU6dO5f333yc8PJxbbrml0u/t8ssvp3Hjxpw8eRKn04m3tzfXXXddmW3vvPNOli5dyt69e3nmmWd49tlnCQgIIDMzE5fLxejRoykoKGDevHmVrqcsf/nLX9izZw+//vorkydP5s033yQgIMC9lsPdd99NfHw8a9euLXXusGHDmD9/PuvXr+ett95iypQpBAUFkZmZidPp5PLLL6dLly689957ZV77hhtuYNq0aRw4cIDLL7+csLAw9039zJkziYyM/MP6X3zxRVJTU1m7di3PP/88L730Ev7+/u764fTXdsyYMVX4KlXNsWPHzrpNIsCVV17Ju+++S2RkJO+88w4TJkxg165djBkzBj8/PwD3DX1QUBDvvPNOqUeXTp48yYcffsiHH36IxWIhMDCQvLw8d0BmGAaPP/447du3d5+TmZnJjBkzmDFjBoZhEBgYSFFRUYnw4Pbbb3eHQyIiDZkCAxERMdWYMWOIiori448/JjExEYfDQZMmTbjsssu4++67K7XdoSd48sknOe+88/jss8/YunUrBQUFtG3blqFDh3Lbbbfx8ssvA6dvlCpj4MCB9OrVi5kzZ/Lzzz9z4MABsrOzCQkJoVu3blx11VVcf/312O12LrjgAvbv388PP/zAiy++SKNGjUo8rlARNpuNYcOGuXcSuOqqq8p9D0FBQfz73//mnXfeYfny5SQlJWG1Wjn//PMZPXo0gwcP5oknnqhUHWfj7e3NRx99xMyZM5k7dy779u3D5XLRp08f96Ma5c1u8fLyYurUqXz44YcsWrSII0eO4HK56NGjB8OGDWP06NFnfaShdevWfPbZZ3zwwQds3ryZtLQ0964W57qNYWBgINOnT2fevHksWLCAHTt2kJOTQ+PGjenVqxdjx47lggsuqPgXxkTnn38+3333HdOmTeOnn37iyJEjGIZBu3btuOyyy7jzzjsJDw8vdd7UqVNZs2YNGzZs4NixY+5Hm1q1akXv3r0ZO3ZsqS0SJ0+ezC+//ML69es5fPgwJ0+epKioiGbNmhETE8OoUaP+MOwQEWkoDFdd2adIRERE3G666SY2bdrEgw8+yH333Wd2OSIiItIAaQ0DERGROmbt2rXuHRQ0LVpERETMosBARETEBP/85z+ZO3cuycnJ7ufOMzIy+Pe//82ECRMA6Nu3Lz169DCzTBEREWnA9EiCiIiICYYOHerems9ut+Pr61ti0br27dszderUUou8iYiIiNQWBQYiIiIm+P7771m+fDmbN2/m5MmTZGVlERAQQPv27bnqqqsYPXo0vr6+ZpcpIiIiDZgCAxEREREREREpRWsYiIiIiIiIiEgpCgxEREREREREpBSb2QU0BC6XC6ezck9+WCxGpc8VETGbxjAR8WQaw0TEU1ksBoZhVLkfBQa1wOl0kZKSXeHzbDYLoaH+ZGTkUFTkrIHKRERqjsYwEfFkGsNExJOFhfljtVY9MNAjCSIiIiIiIiJSigIDERERERERESlFgYGIiIiIiIiIlKLAQERERERERERKUWAgIiIiIiIiIqUoMBARERERERGRUhQYiIiIiIiIiEgpCgxEREREREREpBQFBiIiIiIiIiJSis3sAkRERERERM6Vw1GE0+k0uwyRWmEYBlarDcMwTLm+AgMREREREanzcnOzyc7OoKiowOxSRGqVYViw230IDAzBZvOq1WsrMBARERERkTotNzeb9PST2O2+hISEY7VaAXM+cRWpPS6cTieFhfnk5mZz6tRxQkMjsNu9a60CBQYiIiIiIlKnZWdnYLf7EhoabtrUbBGzeHv74ucXRErKCbKy0ggLa1Jr19aihyIiIiIiUmc5HEUUFRXg5xegsEAaLIvFgr9/IAUFeTgcjlq7rmYY1FFOp4tt+1Mo3JeKl+GiXVQwFosGSBERERFpWIoXODz9GIJIw2W1nl6/wOl01NrPgwKDOmjDjiRmLt9Fama++7XQQG9uHtCB3p0iTKxMRERERMQs+vBMGjYzZtjokYQ6ZsOOJN6Zl1giLABIzcznnXmJbNiRZFJlIiIiIiIi0pAoMKhDnE4XM5fvOmubL5fvwul01VJFIiIiIiIi0lApMKhDdh5KKzWz4PdSMvPZeSitdgoSERERERGRBkuBQR2Sln32sKCi7URERERERMzw4ovP0q9fH7777ptq6/P++8fRr18fNm5cX219ytlp0cM6JMTfu1rbiYiIiIhIw9CvX59KnTdnzkKaNo2q5mqkvlBgUId0bBFCaKD3WR9LCAmw07FFSO0VJSIiIiIidV737jGlXissLGT79q0AdO7cFS8vr1Jt7HZ7jdTTqFFjWrZshb9/QLX12aRJJC1btsLHx6fa+pSzM1wul1bQq2EOh5OUlOxzalu8S0J5wkN8+Oed5+NjV9YjInWbzWYhNNSf1NRsioqcZpcjIlIhGsPqjsLCAk6dOkajRk3x8qqZm9v66tixo4wceT2gmQT1QUV+FsLC/LFaq74CgdYwqGN6d4rgvhuiCQ0s+dhBsL8dH7uV5LQ83pmXSJFDf3GJiIiIiIhIzdHH1HVQ704R9OwQzp6j6RS6DLwMF+2igtl3PINXvtzEln0pTP1uG3cN6YrFMMwuV0RERESkXnI6Xew8lEZadj4h/t50bBGCxVI/fv8+c/bBL7+s56effmTOnC/Zs2c3mZkZTJv2BR06dOLUqZOsWPEDv/32CwcPHuDkyZPYbDZatWpF//5Xc+ONo8p8rOHFF59l8eJFPPnkP7j22uvcr3/33TdMmvRPYmN7MWXKByxY8DULFszl4MED2O3exMb25O67J9C2bbtSfd5//zji4jby1lvv06vX/9Zs+OSTD5g27SMGDRrC44//nS+/nMGSJd9y7NhR/Pz8ueCCvowbdx9NmkSW+bVITk7i44/fZ/XqX8nMzCAioglXXnk1f/rTnbz66ktlvo+GQoFBHWWxGHRpHVZiKly7qGAmDOvOlK83s3rLCYL97Yzu38HsUkVERERE6p0NO5KYuXxXifXFQgO9uXlAB3p3ijCxsur3xRef8t57UwgJCaV58+YkJZ1wH/vmm/l8/PH72O3eNGrUmHbt2pGens7OnTvYtm0rP//8I2+99X6Z6yP8kRde+AdLl35H06ZRtGzZigMHDrBy5U9s2rSBjz+eQfPmLSrUX1FREY8++gAbNqyjRYuWNG/egoMHD7B06WI2bdrI9OkzCQoKLnHOwYMHuO++u0lNTcFms9G2bTvy8/P59NNPWL9+bYN/jEOBgYfp0a4Rtw/qzCffbmPp2kME+3sz8IKWZpclIiIiIlJvlLeuWGpmPu/MS+S+G6LrVWjw8cfvM3Hi4wwbdiMWiwWn04nD4QCgZ88+vP76O/Ts2Rub7X+3j0lJJ3j99VdYuXIF//7359x66x0VumZi4mYOHNjP229/SGxsLwAyMtL5298eIz5+E5988gH/+McLFerzxx+XExkZxaef/pt27doDcPz4cR577AH279/Hl19+zj333Odu73K5eO65p0lNTaF79x48//z/0bhxOAA7d27nr399hB07tlWohvpGaxh4oIu7N2XkFaen6Mz+cTe/Jh4zuSIREREREfO4XC7yCxzV8k9uXhFf/GfnWa83c/kucvOKquV6dWEN+uuuG8bw4SOxWE7fHlosFveMgZiYWM4774ISYQFAREQT/vGPF7DZbCxZ8m2Fr1lUVMTDDz/mDgsAgoKCeeihRwH47bdVlerz73//pzssAIiMjOTuuyeU2efGjevZvn0rPj4+PP/8v9xhAUDHjp156ql/UFRUVOE66hPNMPBQA89vSXpWAcvWHWLad9sJ9LPTvW0js8sSEREREalVLpeLlz7fyO4j6bV2zdTMfO574+dq6at982D+NrYXholrk/3Rs/n5+Xn8+OP3xMdv4sSJE+Tl5bqDDovFwsGDB8jPz8Pb+9y3OwwICOTKK68u9XrHjp2x2+1kZWWSnp5GcHDIOffZvn1HoqO7l3q9W7fTrx05crjE62vW/ApA374X07hx41LnnXdeXyIjm3L8eMP9gFaBgYcyDINR/duTkV3A6q0neHdeIn8Z05O2UUFmlyYiIiIiUrvqxzqEpmnVqk25x/bu3cPjjz/CsWNHz9pHRkYG4eHnHhicbX2CkJBQkpJOkJubW6HAoLw+w8LCAMjNzSnx+qFDBwFo3778deHat++gwEA8k8UwuHNwFzJzCtiyP5U35sTz5K29iQzzM7s0EREREZFaYRgGfxvbi4LC6tl2fOehNF6fE/+H7R4ZGUPHFiFVvp7dy2Lq7AIAX1/fMl93OBw8/fTjHDt2lN69z+eWW26jffsOBAYGuR9RGD58MElJJyo8dd/Hp/xwofjRiIo+rlHe+yju7/dycnIB8PPzL7fPsx1rCBQYeDib1cKEG7rzry83ceB4JpNnxfHkrb0JCfA2uzQRERERkVphGAbedmu19NWtTRihgd4ldkf4vbBAb7q1Cas3WyyWZ9u2rRw4sJ+IiCb861+TSz1y4HK5yMzMNKm6qvPzOx0w5ORkl9vmbMcaAi16WA/4ett4ZGQMEaG+nEzP4/XZ8eTkNezFOUREREREKsNiMbh5wNm3Lh8zoEO9DwsAjh07AkCXLl3LXJ9g7949pab5e5IWLU7vNrdnz+5y25ztWEOgwKCeCPK3M3F0LEH+dg4lZfH23M0UFjnMLktERERExOP07hTBfTdEExpYctZuWKB3vdtS8WyKHxs4depUmcdnzvysNsupdhdccBEAq1evIiWl9Htcv37tH67dUN8pMKhHIkJ8eWRkDD52K9sPpvHhN1txOs3fpkVERERExNP07hTBK+Mv4q9jejLu+q78dUxP/jX+ogYTFsDp3QVsNhuJiZtZsGCu+/XCwkI++ug9li1b7N5+0RP16tWHLl26kpuby9///jgnT550H9u1aweTJv2z1HaSDY0Cg3qmVWQgDwzvjs1qsGFHMl8s31kn9nYVEREREfE0FotB51ah9O0aSedWoQ3iMYQzhYU1YsyYWwF45ZVJDBs2iLvu+hPXXXcVn376CXfeOY5GjUpvR+gpDMPg6aefJzQ0jM2b4xgxYgh33jmWW24ZxR13jKVx43Auv/xKoPyFE+u7hvmu67kurcO4a0hXDODHjUdY9Ot+s0sSEREREREPdM899/HYY3+jXbv2pKencfjwIdq378jzz7/MHXfcbXZ5VdayZSs++WQGgwdfT3BwMPv376OgIJ9bbrmdt9563737g79/w9wtwXDp4+ca53A4SUmp+OqaNpuF0FB/UlOzKSqq+DYxy9cfYubyXQDcPqgzl8ZEVbgPEZHKquoYJiJiJo1hdUdhYQGnTh2jUaOmeHnZzS5HGphbbx3Fvn17mTZtJh06dDS1lor8LISF+WO1Vn1+gGYY1GMD+rRg8IWtAPh0yXY27Uw2uSIRERERERHPsGVLIvv27SUoKJg2bdqaXY4pFBjUc8MvbUu/Hk1xueD9hVvYdTjN7JJERERERETqhEOHDjJnzr/JzMws8frmzXE888wTAFx//Q0NdvHDhvmuGxDDMLhtYCcyswuI33OKN+ds5m+39KJZeIDZpYmIiIiIiJgqOzuLN998lbfffp0WLVri5+fPyZPJJCWdAKB79x7cccddJldpHs0waACsFgv3DoumXbMgcvKLmDw7nlPpeWaXJSIiIiIiYqqoqOb86U930rFjJ9LT09m5czvZ2Vl069adBx98lDfffB9vbx+zyzSNFj2sBWYtevh7WbmFvPT5Bo6dyqFpIz/+dktvAnw9d99UEanbtGCYiHgyjWF1hxY9FDlNix5KjQrw9eLR0bGEBnpz7FQOb34VT36hw+yyREREREREpA5SYNDAhAX5MHFUDH7eNvYcyeD9+Yk4nErNRUREREREpCQFBg1Qs/AAHhzRAy+bhfg9p/h0yQ70ZIqIiIiIiIicSYFBA9WxRQj3Du2GYcAvm48x9+e9ZpckIiIiIiIidYgCgwasZ4dwbhvYGYBvfzvA8vWHTK5IRERERERE6goFBg3cpTFRDLukDQBfLt/F2m0nTK5IRERERERE6gIFBsJ1F7Xmil7NcAEfL9rKtv0pZpckIiIiIiIiJrOZXUBFrV69mmnTphEfH09OTg5RUVEMHDiQcePG4efnV+H+jh49ytSpU/nll184duwYTqeT8PBwLrjgAm6//XY6depUA++ibjEMg7EDOpKZXcD6HclMmZvA4zf3olVkoNmliYiIiIiIiEk8aobBjBkzuP3221mxYgXe3t60a9eOI0eO8N577zFixAjS0tIq1N+mTZsYMmQIM2bM4PDhwzRt2pTWrVtz6tQp5s6dy/Dhw1m8eHHNvJk6xmIxuPu6rnRqEUJegYPX58STlJZrdlkiIiIiIiJiEo8JDBITE5k0aRIAzz33HCtWrGDevHksX76cbt26sWfPHp5++ulz7s/lcvH444+TnZ1Nz549WbZsGUuWLOGbb77hl19+YciQIRQVFfH3v/+dzMzMmnpbdYqXzcoDN/agRUQAGdkFTJ4VR0Z2gdlliYiIiIiIiAk8JjB49913cTqdDB06lNGjR2MYBgBNmjRh8uTJWCwWli1bxvbt28+pv927d3PgwAEAnn32WaKiotzHAgMDeemll/Dz8yMrK4v169dX/xuqo/x8bDwyKobGwT4kpeby+px4cvOLzC5LRERERETqmH79+tCvX59Sr99//zj69evDxo0Vu4/auHE9/fr14f77x1VXiX/o2LGj9OvXhxEjrqu1a3oSjwgMsrOzWblyJQCjRo0qdbx169b07dsXgCVLlpxTn3l5ee7/btGiRanjdrudJk2aAFBU1LBumEMCvJk4OpYAXy8OHM/k3XkJFDmcZpclIiIiIiLlePHFZ+nXrw+PPvrgObVPSTnFZZddQL9+fVi3bk0NV2eeTz75gE8++aDBzBqvbh4RGGzbto2CggLsdjs9evQos03v3r0BiI+PP6c+27Rpg4+PD3B6LYPfS0pK4vDhw1itVrp27VrJyj1XZJgfD4+Mwe5lYcv+VD75dhtOl8vsskREREREpAyDBg0BYP36NZw6dfIP2y9bthiHw0FERBN69z6vWmtp0iSSli1bue+3zDRt2kdMm/YRWVllBwY2m42WLVvRrFnzWq7MM3hEYLBv3z4AoqKi8PLyKrNNy5YtS7T9IwEBAUyYMAGAv/3tbyxZsoTU1FSysrJYvXo148aNo7CwkHHjxtGsWbNqeBeep21UEPfd0B2rxWDN1hPM/mE3LoUGIiIiIiJ1Ts+evWnaNAqHw8GyZX8863rx4m8BGDhwMBZL9d4WPv30c8yc+TVdu0ZXa781ITw8gpkzv+bNN98zu5Q6ySO2VUxPTwcgODi43DbFx4rbnot77rmH8PBwPvnkEx566KESx1q3bs3rr7/OtddeW4mKS7PZKv5DaLVaSvzbDD07hnPXdV35YMEWlq07RGiQN4MvbG1aPSLiOerCGCYiUlkaw+oOp9MwuwSPYBgGAwcOZtq0j1iy5FvGjLml3La7du1gz55dwP9mJojnsFqNP7y/NKrpx8YjAoP8/HyAcmcXwOk1B85sey4KCws5dOgQ6enp2Gw2mjdvjpeXFwcOHODAgQN89dVX9OrVi8jIyCrVb7EYhIb6V/r8oCDfKl2/qoZc2p4CB0xbtIVZ3+8mKiKQ/n1amlqTiHgOs8cwEZGq0Bhmvrw8KydPWs7pJqm6OV1OdqXuJT0/k2DvQDqEtsVi1N0QaciQ65g+/WP27NnF3r276NixU5ntli79DoDu3WNo06Y1iYkJ/PTTj2zYsI4TJ06Qnp5GUFAw3bpFM3r0GPr0Of+s1/39n8v48XezadMG3nnnQ3r3LrkootPpZO7cOcyfP5dDhw7i5+dHjx6x3HXXPe6AzjBK/1lXtMaPPnqfTz750P3/I0deX+J4cW1Hjx5l+PAhREY2Zf78b0u9t+zsLL788gtWrPiBw4cPYRgGzZu34PLL+3PTTWPx9y99nzds2GCOHz/GO+98SEREBB999AHr168lKyuTpk2juO66odx8860VntnhdBpYLBaCg/1q7XEPjwgMvL29gdM3+OUpKCgo0fZc3H///axYsYJLL72UF154wb3IYXp6Oi+88AILFy5k9OjRfPvttwQEBFS6fqfTRUZGToXPs1otBAX5kpGRi8PkRQeviG3K8ZOZLF59kDf/HYfF5SKmfWNTaxKRuq0ujWEiIhWlMazuKCjIx+l04nC4KCqqvT+LuKQE5uxaSFr+/2Ywh3gHM7LD9cRGdK+1OiqiSZMoevSIJT5+E4sWLeTBBx8t1aaoqIilSxcDpx9HKCpy8o9/PMWRI4cJDAyiUaPGNGrUmOTkJFau/IlffvmZhx9+jBtvHF3udX//51L8GLPD4SxxzOVy8eyzT/H998sAiIxsSnBwCKtX/8pvv/3KHXfc5W73+z4rWmN4eBO6d48hIeH0GnedO3ct8QG0r68fRUXOEj/fv7/m8ePHefjhCRw+fBCLxUKbNm0B2LNnN7t27WTp0sW88ca7REQ0KfPrsn37Nv7610coKiqideu22Gw2DhzYz9tvv8nRo0eZOPHxcr+mZXE4XDidTtLTc8jNdZy1bXCwb7U8auIRgcG5PG5wLo8tnOmHH35gxYoVhIaGMnnyZAIDA0tcb9KkSSQmJrJ3715mzpzJuHFV29qjKoPb73/QzHLjZe1Izcxn9ZYTTPl6M38Z05N2Uef29RaRhquujGEiIpWhMcx8Dkftr6EVl5TAR4kzSr2elp/OR4kzuDv61jobGgwaNIT4+E385z9LmTDhIWy2krd8a9b8RmpqCna7N1deeTUAt99+F926dadly1Yl2m7YsI5nn32KKVNe5+KLL6vyzOuFC+fx/ffLsNu9+ec/X+SSSy4HICsrixdffJZPPvmg3HMrWuOQIUMZMmSoe9vH559/maZNoypU7z//+RSHDx+kffuOvPjiv9wLIx46dJAnn3yMffv28txzT/P22x+Wef57701h0KAhPPDARPz8/AD4/vv/8OyzTzJv3leMGHFTqfdzLs4lPKuupefq7nyaM7Ru3RqAo0ePljvL4ODBgyXa/pH160/vCdqjR48SYUExLy8vLrjgAgASExMrWHH9ZDEM7ry2C93ahFFQ6OTNOZs5dirb7LJEREREpIFzuVzkOwqq5Z/cojxm71xw1uvN2bWQ3KK8arledS8q3r//AHx8fEhNTWHNmt9KHV+8eBEAl1xymXsW9aBBQ8q8ce3d+zzGjZtAUVERy5ef2/b15XG5XHz++acAjB37J3dYAKcXpH/mmefLnN5frDZqPNOmTRtISIjHYrHwz39OKrGLQosWLXn22UkYhkFc3Ebi4jaW2UeLFi157LG/ucMCgCuvvIqLL74El8vF6tWrqq3emuIRMwy6dOmCl5cXBQUFbN682b2F4pk2bNgAQGxs7Dn1mZ197je6FVkXob6zWS3cd0M0/5q5if3HM5k8K54nb+1NaOC5PwoiIiIiIlJdXC4Xkze+y970A7V2zbT8dB77+Zlq6attcGsm9hqPUU2r1Pn5+XPZZf1ZuvQ7liz5losvvsR9LCMjg19/XQnAtddeV+K8o0ePsHz5Unbt2kl6epr7g9rs7Czg9EKJVXHw4AGOHTsCUObjDb6+vgwePJSZMz8rt4+arvFMq1f/CsD55/elVavWpY63a9ee8867gLVrV7NmzW/ExvYq1ea664ZhtVpLvd6tW3d++eVnjhw5XG311hSPCAwCAgLo168fP/74I7Nnzy4VGOzfv5/Vq1cDMHDgwHPqs02bNgBs3ryZzMzMUrMMCgsLWbNmTYm2cpqP3cbDI2N46fMNnEjN5fXZcTwxthd+PuUvSikiIiIiUnO0k8KZBg0awtKl37Fq1c8l7nV++GEZBQUFNG4cXmKRwNmzZ/Luu29RVFRUbp8V2Y2uLAcO7AcgNDSMkJCQMtsUrxFQltqo8UwHD54OoNq2bVdum7Zt27N27Wr3e/u95s3LXig+NDQMgNzc3KoVWQs8IjAAmDBhAitWrGDBggX06tWLUaNGYRgGSUlJTJw4EafTyYABA+jcuXOJ8/r37w/AX//61xJhwsCBA3nttddITU1l4sSJZS56uHfvXgzD4PrrS66oKRDkb2fi6FgmzdjA4eRs3vo6gUdHx+BlK52giYiIiIjUFMMwmNhrPAXO8hdIr4jdaXt5N37qH7abEHMn7UPKv8E9V3aLV7XNLijWu/d5NGkSyYkTx/n++2UMG3YjAIsXn94F4JprrnV/8p2QEM9bb03GYrFwxx13c9ll/YmKisLH5/SieRs2rOOhh8af9Ub9XOTmnl4EPjQ0tNw2xTfSv1dbNZ4pJ6e43kbltgkLa/TftmXPXi9vJ4PixQir+3GUmuAxgUGPHj144oknePnll3nmmWd47733CA0NZffu3RQUFNCmTRuef/75UucdOXJ62kvxH3ixyMhInn/+eZ566il+/vln+vfvX2JbxYKCAgzD4LHHHqNr16618h49TXiIL4+MiuHlLzay81AaHy7cyvhh0VgsSnhFREREpPYYhoG31V4tfXUJ60iId3CJ3RF+L9Q7mC5hHevsFouGYTBw4GA+/fQTliz5lmHDbuTgwQNs2ZIAnJ6BUGzJktMhwujRY/nzn+8p1Vd1fWrv63v6Of7U1NRy26SmppT5em3VeKbidQdSU0+V2yYl5dR/25a/9oKnq5vf4eW4/fbbmTZtGpdeeim5ubns3r2bqKgo7r33Xr7++mvCwspOpMozbNgwvv76a4YPH05UVBRHjx5l//79NG7cmMGDB/PFF19w11131dC7qR9aNgnkgRt7YLMabNiZzOf/2ekRSZmIiIiISFkshoWRHc4+w3hEh+vrbFhQrDgUSEzczKFDB9033V26dKN16/89cn3s2FEAYmJ6ltlPcchQVcXrAKSlpZKWllZmm3379pb5em3VeKbiBRb37t1TbpviY2WtcVBfeMwMg2IXXnghF1544Tm337Hj7AtfdO7cmZdeeqmqZTVoXVqFMu66brw3P5EVm44Q4m/n+n5a90FEREREPFNsRHfujr6VObsWlphpEOodzIgO19fZLRXP1Lx5C7p3jyEhIZ7FixexdOl3QMnZBQDe3qenzZ86dbJUH6mpqe5dFaqqZctWNG3ajGPHjjBv3hzuuOPuEsfz8vL47ruFZZ5blRq9vb3Jz8+v8EL2fftexBdffOpeo+D3ocDevXtYt261u219VbdjMfEYfTpHcPNVHQGY/8s+VsQdMbkiEREREZHKi43ozvMX/Y2Het7DHV3H8FDPe3juor95RFhQrHgnhFmzvuDEiePY7XYGDLimRJvY2NOf2s+YMc290B+c3pHgr399mLy8vGqpxTAMbr75VgC++OJTfvnlZ/ex7Owsnn/+abKysso8tyo1Fm+HGBe3oUL19uzZmx49YnE6nTz77JMldjQ4cuQw//znU7hcLmJje5U786E+8LgZBlJ3Xdm7OenZ+Sz69QAzlu4gyM9Or47hZpclIiIiIlIpFsNCx9DyV8mv6/r3H8Cbb77qvqG+6KJLCAoKKtHmuutuYMGCuRw8eIBbbx1FixatsFot7Nu3F19fXyZMeIA33ni1WuoZNuxGNm5cz48/LueJJybStGkUwcEh7N+/F6fTxZ//fA8ffPBOqfOqUuOAAdfw4Yfv8uqrLzN37hyCgoIBeOihR+nQodNZ633mmRd4+OHx7Nq1kzFjhtOmTTvAxb59e3E6nbRo0ZJnnim9jl59ohkGUq1uuKQtl/RoissF7y/Yws5DaWaXJCIiIiLSIPn7B3DppVe4/794xsGZ/Pz8eOedj7n++hsICQnh8OGDZGRkcPXVg5g69Qvatm1fbfUYhsGzz77Iww8/Rrt27Tl16iTHjx/lvPP68sEH0+jWrezZG1Wp8eab/8Rdd91L69ZtOHz4MHFxG4mL20hmZuYf1hsZGcknn8zgjjvupnXrNhw5cogjRw7Tpk1b/vzne/jkkxlERDSp9NfDExgurVBX4xwOJykpZW+1cTY2m4XQUH9SU7MpKnLWQGU1w+F08s7cROJ2n8TP28YTt/SieXiA2WWJSC3z1DFMRAQ0htUlhYUFnDp1jEaNmuLlVT07IYh4oor8LISF+WO1Vn1+gGYYSLWzWizcM7Qb7ZsFk5NfxORZcZxMzzW7LBEREREREakABQZSI7y9rDw4ogdRjf1Jyypg8qx4snILzS5LREREREREzpECA6kxAb5eTBwVQ2igN8dTcnhzTjz5BQ6zyxIREREREZFzoMBAalRYkA8TR8fi72Njz9EM3luQSJFDzwGKiIiIiIjUdQoMpMY1a+zPQyNi8LJZ2LznFJ8u3o7W2hQREREREanbFBhIrWjfPJjxQ6OxGAarEo/z9U97zS5JREREREREzkKBgdSa2A6N+dPATgB8t/oA/1l/yOSKREREREREpDwKDKRWXRoTxQ2XtgXg38t3sWbrCZMrEhERERERkbIoMJBaN+TCVlzZqzku4ONFW9myP8XskkREREREROR3FBhIrTMMgzEDOtCncwQOp4u35yZw4Him2WWJiIiISJ2mRbOloav9nwEFBmIKi8Xg7iFd6dwyhPwCB6/PjiMpNcfsskRERESkjjEMAwCnU1tzS8Pm+O/29IZRe7fxCgzENF42C/cP70GLiAAycgp5bVYc6dkFZpclIiIiInWI1WrDMCwUFuabXYqIqfLzc7FYbFit1lq7pgIDMZWfj41HRsXQONiH5LQ83pgdT25+kdlliYiIiEgdYRgGdrsPubnZmmUgDVZhYT55edn4+Pi5Z93UBsPlculhoBrmcDhJScmu8Hk2m4XQUH9SU7MpKqrfg+OJlBwmfb6BzJxCurYO5eGRMdisyrNEPFlDGsNEpP7RGFa3FBUVcurUcaxWG/7+gVitXrV60yRiDhcOh5P8/Fzy8rKx2bwIDY3AYvnj+6SwMH+s1XA/pcCgFigwODf7jmXwr5mbyC90cH6XCMZd3w2L/iIQ8VgNbQwTkfpFY1jdU1CQT1ZWGgUFeWaXIlKrLBYbPj5+BAQEn1NYANUXGNiq3ININWnTNIj7hkfz5pzNrN2WRJC/nTFXdlB6LCIiIiLY7d6EhTXB4XDgdDrMLkekVhiGBavVato9kQIDqVOi2zTizsFd+OibrSxff5iQAG+u7dvK7LJEREREpI6wWq21uuibSEOmh8SlzrmwWySj+7cH4KsVe/hl8zGTKxIREREREWl4FBhInXTN+S0ZeH5LAKYv3k787pMmVyQiIiIiItKwKDCQOmvEFe24sFskTpeL9+YnsudIutkliYiIiIiINBgKDKTOshgGd1zbmei2YRQUOXljTjzHTlV8twkRERERERGpOAUGUqfZrBYmDIumTdNAsvOKmDwrjtTMfLPLEhERERERqfcUGEid52O38dDIGJqE+nIqI5/Js+PIzis0uywREREREZF6TYGBeIQgPzuPjo4lOMDOkeRspny1mYJC7b8rIiIiIiJSUxQYiMdoHOLLxFGx+Hpb2Xk4nQ+/2YrT6TK7LBERERERkXpJgYF4lBYRATwwvAc2q8HGncnMWLYDl0uhgYiIiIiISHVTYCAep3OrUMZd1w0D+CnuKAt+2Wd2SSIiIiIiIvWOAgPxSH06R3DL1R0BWLhqPz9uOmJyRSIiIiIiIvWLAgPxWFf0as51F7UG4PNlO9iwI8ncgkREREREROoRBQbi0YZd0oZLY6JwueCDhVvZcTDV7JJERERERETqBQUG4tEMw+DWazrSs0NjihxO3vo6gcNJWWaXJSIiIiIi4vEUGIjHs1os3HN9Nzo0DyY3v4jJs+M4mZ5rdlkiIiIiIiIeTYGB1At2LysPjuhBs8b+pGUVMHlWPJk5BWaXJSIiIiIi4rEUGEi94e/jxSOjYggL8uZ4Sg5vzNlMfoHD7LJEREREREQ8kgIDqVfCgnyYOCoWfx8b+45l8O78RIocTrPLEhERERER8TgKDKTeiWrsz0MjY7DbLCTsPcX0xdtxuVxmlyUiIiIiIuJRFBhIvdS+WTD3DovGYhj8mnicr1bsMbskERERERERj6LAQOqt2PaNuW1QJwAWrznIsnWHTK5IRERERETEcygwkHrtkh5R3HhZWwD+/f0uVm89bnJFIiIiIiIinkGBgdR71/ZtxZW9mwPwyaJtbNmXYnJFIiIiIiIidZ8CA6n3DMNgzIAOnNc5AofTxdvzEth/PMPsskREREREROo0BQbSIFgMg7uGdKVLq1DyCxy8PjueE6k5ZpclIiIiIiJSZykwkAbDy2bh/uHdadkkgMycQibPiiM9K9/sskREREREROokBQbSoPh623hkZAyNg31ITsvj9dnx5OYXmV2WiIiIiIhInaPAQBqc4ABvHr0plkA/Lw4mZfH23AQKi5xmlyUiIiIiIlKnKDCQBqlJqB+PjIrB225l24FUPl60FafLZXZZIiIiIiIidYYCA2mwWkcGcf8N3bFaDNZtT+LL5btwKTQQEREREREBFBhIA9etTRh/HtIFgO83HOa71QdMrkhERERERKRuUGAgDV7frpHcdGUHAL7+aS8rNx81uSIRERERERHzKTAQAa4+rwWDLmgJwKeLdxC3+6TJFYmIiIiIiJhLgYHIf424vB0XRUfidLl4f34iu4+km12SiIiIiIiIaRQYiPyXYRjcPqgz3ds2oqDIyZtz4jl6MtvsskREREREREyhwEDkDDarhQnDomkbFUR2XhGTZ8eRkpFndlkiIiIiIiK1ToGByO942608NKIHkWF+pGTk8/rseLLzCs0uS0REREREpFYpMBApQ6CfnYmjYwgJsHPkZDZvfbWZgkKH2WWJiIiIiIjUGgUGIuVoHOzLxFGx+Hrb2HU4nQ8WbsHhdJpdloiIiIiISK1QYCByFs0jAnjwxu7YrBY27TrJjKU7cblcZpclIiIiIiJS4xQYiPyBTi1Duef6rhgG/Bx/lPkr95ldkoiIiIiISI1TYCByDnp3iuCWqzsB8M2v+/lx42GTKxIREREREalZCgxEztEVPZtx/cWtAfh82U7Wb08ytyAREREREZEapMBApAKG9mvDZbFRuIAPv9nC9gOpZpckIiIiIiJSIxQYiFSAYRjcenUnenUMp8jhYsrczRw8kWl2WSIiIiIiItVOgYFIBVksBvdc35WOzYPJzXfw+ux4Tqblml2WiIiIiIhItVJgIFIJXjYrD47oQbNwf9KzC3htdjwZOQVmlyUiIiIiIlJtbGYXUFGrV69m2rRpxMfHk5OTQ1RUFAMHDmTcuHH4+fmdcz9r1qzhT3/60zm1feCBB7j//vsrW7LUU34+XkwcFcukGes5kZLDm3Pi+cuYnvjYPe7HSkREREREpBSPurOZMWMGL774Ii6Xi8jISJo2bcru3bt57733WLZsGTNnziQkJOSc+goMDKRXr17lHs/KymLnzp0A9OzZszrKl3ooNNCbiaNjmTRjA/uOZfLuvEQeHNEDm1WTd0RERERExLMZLpfLZXYR5yIxMZGRI0ficrn45z//yahRozAMgxMnTjB+/Hi2bNnC1VdfzZQpU6rlem+//TZTpkyhadOm/PDDD1gslb8BdDicpKRkV/g8m81CaKg/qanZFBU5K319qXl7jqTzypebKChycmG3SP48pAsWwzC7LBFTaQwTEU+mMUxEPFlYmD/WavgQ02M+Bn333XdxOp0MHTqU0aNHY/z3ZqxJkyZMnjwZi8XCsmXL2L59e5Wv5XK5mD9/PgBDhw6tUlggDUO7ZsGMHxaNxTD4bctxvlqxx+ySREREREREqsQj7oSzs7NZuXIlAKNGjSp1vHXr1vTt2xeAJUuWVPl669at49ChQwAMHz68yv1JwxDTvjF3XNsZgCVrDrJ07UGTKxIREREREak8jwgMtm3bRkFBAXa7nR49epTZpnfv3gDEx8dX+Xrz5s1z99mqVasq9ycNx8XdmzLi8nYAzPphN79tOW5yRSIiIiIiIpXjEYHBvn37AIiKisLLy6vMNi1btizRtrJycnLcsxRuuOGGKvUlDdOgC1oyoE9zAKZ+u43EfadMrkhERERERKTiPGKXhPT0dACCg4PLbVN8rLhtZS1ZsoScnBx8fX0ZNGhQlfo6k81W8WymeJGK6lisQmrXLdd0IiunkNVbT/DO3ET+dmsv2kaV//0rUh9pDBMRT6YxTEQ8WXWtv+4RgUF+fj5AubMLAOx2e4m2lVX8OMLVV19NQEBAlfoqZrEYhIb6V/r8oCDfaqlDatdfbzuP5z5eQ9yuZCbPiueVBy4hKrx6vqdEPInGMBHxZBrDRKQh84jAwNvbG4DCwsJy2xQUFJRoWxmHDh1i3bp1QPU+juB0usjIyKnweVarhaAgXzIycnE4tJ2PJxo/rBsvzdjA/uOZ/P39X3n69j6EBFT+e1TEk2gMExFPpjFMRDxZcLBvtez25xGBwbk8bnAujy38kfnz5+NyuWjWrJl714XqUpX9ex0Op/b/9VBeVgsPjYzhpRkbSErL5ZWZm3j85l74+XjEj55ItdAYJiKeTGOYiHgil6t6+vGIh7Jat24NwNGjR8udZXDw4MESbSvK5XIxf/58AIYNG4ZRXQ99SIMX7G9n4ugYgvy8OJSUxdtzN1OoXzxERERERKSO84jAoEuXLnh5eVFQUMDmzZvLbLNhwwYAYmNjK3WNtWvXcvjwYQzD0O4IUu0iQv14ZFQs3nYr2w+m8dGirTid1RT7iYiIiIiI1ACPCAwCAgLo168fALNnzy51fP/+/axevRqAgQMHVuoaxYsd9unThxYtWlSyUpHytYoM5P7h3bFaDNZvT2Lm8p24qmuukIiIiIiISDXziMAAYMKECRiGwYIFC5g1a5b7RispKYmJEyfidDoZMGAAnTt3LnFe//796d+/P0uWLCm37+zsbJYuXQrA8OHDa+5NSIPXrXUYdw3pCsAPG4+w6LcDJlckIiIiIiJSNo8JDHr06METTzwBwDPPPMMVV1zBDTfcwJVXXsmWLVto06YNzz//fKnzjhw5wpEjR8jJKX+XgqVLl5KTk4Ofnx/XXHNNjb0HEYALujZhzIAOAMz7eS8/xx81uSIREREREZHSPGqp9ttvv51OnToxdepUNm/ezKlTp4iKimLgwIGMGzcOf3//SvVb/DjCNddcU+k+RCriqj4tSM8q4LvVB/h0yXaC/OzEdmhsdlkiIiIiIiJuhksPUdc4h8NJSkp2hc+z2SyEhvqTmpqt7XzqIZfLxdTvtrEq4TheNgt/uakn7ZtXfltQkbpGY5iIeDKNYSLiycLC/LFaq/5Agcc8kiBS3xiGwW0DO9OjXSMKi5y8+VU8R5KzzC5LREREREQEUGAgYiqb1cL4odG0iwoiO6+IybPjScnIM7ssERERERERBQYiZvO2W3loZAxNG/mRmpnP5NnxZOUWml2WiIiIiIg0cAoMROqAAF8vJo6KJSTAztGT2bz19WbyCx1mlyUiIiIiIg2YAgOROqJRsA8TR8fi521j9+F0PliwBYdTiyyJiIiIiIg5FBiI1CHNwwN4cEQPbFYLcbtP8tmSHWgjExERERERMYMCA5E6pmOLEO4d2g3DgJWbjzFv5T6zSxIRERERkQZIgYFIHdSrYzi3XtMJgEW/7uf7DYdNrkhERERERBoaBQYiddTlsc0Y1q8NADP/s5N125NMrkhERERERBoSBQYiddh1F7fm8p7NcAEffbOFbQdSzS5JREREREQaCAUGInWYYRjcclVHencMp8jh4u25mzl4ItPsskREREREpAFQYCBSx1ksBuOu70rHFiHk5jt4fXY8yWm5ZpclIiIiIiL1nAIDEQ/gZbPy4I3daR7uT3p2AZNnxZGRU2B2WSIiIiIiUo8pMBDxEH4+XjwyKpZGQT6cSM3ljdnx5BUUmV2WiIiIiIjUUwoMRDxIaKA3E0fHEODrxf7jmbwzL5Eih9PsskREREREpB5SYCDiYZo28uehkT2we1nYsi+Fqd9tw+lymV2WiIiIiIjUMwoMRDxQu6hgJgzrjtVisHrLCeb8uNvskkREREREpJ5RYCDioXq0a8TtgzoDsHTtIZasOWhyRSIiIiIiUp8oMBDxYBd3b8rIK9oBMPvH3fyaeMzkikREREREpL5QYCDi4Qae35Krz2sBwLTvtpOw95TJFYmIiIiISH2gwEDEwxmGwaj+7enbtQkOp4t35yWy92iG2WWJiIiIiIiHU2AgUg9YDIM7B3ehW+tQ8gsdvDEnnuMpOWaXJSIiIiIiHkyBgUg9YbNamHBDd1pFBpKVW8jkWXGkZeWbXZaIiIiIiHgoBQYi9Yivt41HRsYQEerLyfQ8Xp8dT05ekdlliYiIiIiIB1JgIFLPBPnbmTg6liB/O4eSsnh77mYKixxmlyUiIiIiIh5GgYFIPRQR4ssjI2PwsVvZfjCND7/ZitPpMrssERERERHxIAoMROqpVpGBPDC8OzarwYYdyXyxfCcul0IDERERERE5NwoMROqxLq3DuGtIVwzgx41HWPTrfrNLEhERERERD6HAQKSeO79LE8YM6ADAvJX7+CnuiMkViYiIiIiIJ1BgINIADOjTgsEXtgLgs6U72LQz2eSKRERERESkrlNgINJADL+0Lf16NMXlgvcXbmHX4TSzSxIRERERkTpMgYFIA2EYBrcN7ERMu0YUFjl5c85mjiRnmV2WiIiIiIjUUQoMRBoQq8XCvcOiadcsiJz8IibPjudUep7ZZYmIiIiISB2kwECkgfH2svLQiBiaNvIjNTOfybPjyMotNLssERERERGpYxQYiDRAAb5ePDo6ltBAb46dyuHNr+LJL3SYXZaIiIiIiNQhCgxEGqiwIB8mjorBz9vGniMZvD8/EYfTaXZZIiIiIiJSRygwEGnAmoUH8OCIHnjZLMTvOcWnS3bgcrnMLktEREREROoABQYiDVzHFiHcO7QbhgG/bD7G3J/3ml2SiIiIiIjUAQoMRISeHcK5bWBnAL797QDL1x8yuSIRERERETGbAgMRAeDSmChuuKQNAF8u38XabSdMrkhERERERMykwEBE3IZc1Jr+vZrhAj5etJVt+1PMLklEREREREyiwEBE3AzD4OYBHenTKZwih4spcxM4cDzT7LJERERERMQECgxEpASLxeDu67rSqUUIeQUOXp8TT1JartlliYiIiIhILVNgICKleNmsPHBjD1pEBJCRXcDkWXFkZBeYXZaIiIiIiNQiBQYiUiY/HxuPjIqhcbAPSam5vD4nntz8IrPLEhERERGRWqLAQETKFRLgzcTRsQT4enHgeCbvzkugyOE0uywREREREakFCgxE5Kwiw/x4eGQM3l5WtuxP5ZNvt+F0ucwuS0REREREapgCAxH5Q22jgrjvhmisFoM1W08w+4fduBQaiIiIiIjUawoMROScRLdtxJ3XdgFg2bpDLFl70OSKRERERESkJikwEJFzdmF0JKOuaA/AnB/3sCrhmMkViYiIiIhITVFgICIVMvCCllxzfgsApn23nc17TplckYiIiIiI1AQFBiJSYSOvaE/fbk1wuly8Oz+BPUfTzS5JRERERESqmQIDEakwi2Fw57Vd6NYmjIJCJ2/O2cyxU9lmlyUiIiIiItWoxgMDh8PB559/zvjx47nvvvuYM2dOTV9SRGqBzWrhvhuiaR0ZSFZuIZNnxZOamW92WSIiIiIiUk2qJTD46quv6NKlCw8//HCpYxMnTuTFF19kxYoVfP/99zzzzDM88sgj1XFZETGZj93GwyNjaBLqy6mMPF6fHUdOXqHZZYmIiIiISDWolsBg1apVAAwZMqTE62vWrGHp0qW4XC569uzJRRddBMCSJUtYvnx5dVxaREwW5G9n4uhYgv3tHE7O5q2vEygscphdloiIiIiIVFG1BAbbtm0DoFevXiVenz9/PgCjRo1i5syZTJ06lQceeACXy8W8efOq49IiUgeEh/jyyKgYfOxWdh5K48OFW3E6XWaXJSIiIiIiVVAtgUFqaip2u52wsLASr//2228YhsGtt97qfm3s2LEAJCYmVselRaSOaNkkkAdu7IHNarBhZzKf/2cnLpdCAxERERERT1UtgUF2djbe3t4lXktKSuL48eM0atSIDh06uF8PDg4mICCAlJSU6ri0iNQhXVqFMu66bhjAik1H+GbVfrNLEhERERGRSqqWwCAgIIDMzExyc3Pdr61btw6Anj17lnnO7wMGEakf+nSO4OarOgIw/5d9rIg7YnJFIiIiIiJSGdUSGBTPIFi8eLH7tfnz52MYBuedd16JtpmZmWRlZdG4cePquLSI1EFX9m7OkItaATBj6Q427kw2uSIREREREakoW3V0MmTIENatW8dzzz1HfHw8J0+eZOXKldjtdgYNGlSi7aZNmwBo3bp1dVxaROqoGy5pS3pWASs3H+P9BVt47KZYOrYIMbssERERERE5R9Uyw2DEiBFcdNFF5OXlMXv2bL7//nsMw+Dhhx8mPDy8RNslS5aUOfNAROoXwzD408BOxLZvTJHDyVtfbeZwcpbZZYmIiIiIyDmqlhkGVquVjz/+mEWLFrFp0yaCgoK49NJL6d27d4l2BQUFJCcn06dPHy699NLquLSI1GFWi4V7hnbjtX/HsftIOpNnxfHkrb1pHOxrdmkiIiIiIvIHDJf2PatxDoeTlJTsCp9ns1kIDfUnNTWboiJnDVQmUjuycgt5+YuNHD2ZTWSYH0/e2psAXy+zy5IapjFMRDyZxjAR8WRhYf5YrVV/oKBaHkkQETmbAF8vJo6KITTQm+MpObw5J578AofZZYmIiIiIyFnUSmDw448/8sILLzBp0iRWrVpVG5cUkTomLMiHiaNj8fexsedoBu8tSKTIoU9sRERERETqqmp5JGHZsmX83//9HxdffDHPPfdciWMvvfQSn332WYnXbr/9dh5//PFKXWv16tVMmzaN+Ph4cnJyiIqKYuDAgYwbNw4/P79K9elyufj222+ZN28e27ZtIyMjg5CQENq1a8ell17Kn//850r1W0yPJIj8z+7D6bzy700UFjm5ODqSOwd3wTAMs8uSGqAxTEQ8mcYwEfFkdeqRhB9++IGjR4/Sp0+fEq9v2bKFTz/9FJfLRdOmTWnZsiUul4vp06ezZs2aCl9nxowZ3H777axYsQJvb2/atWvHkSNHeO+99xgxYgRpaWkV7jM7O5s777yTRx99lF9++QU/Pz86d+6Ml5cX69at48MPP6xwnyJSvvbNgxk/NBqLYbAq8Thf/7TX7JJERERERKQM1RIYJCQkAHDhhReWeP3rr78G4KqrrmL58uUsXbqUsWPH4nK5mD17doWukZiYyKRJkwB47rnnWLFiBfPmzWP58uV069aNPXv28PTTT1eoT5fLxQMPPMCvv/7KJZdcwn/+8x+WL1/OV199xY8//sjq1avd1xSR6hPboTF/GtgJgO9WH+A/6w+ZXJGIiIiIiPxetQQGKSkpWK1WwsPDS7y+atUqDMPg7rvvxmI5fal77rkHgLi4uApd491338XpdDJ06FBGjx7tnsLcpEkTJk+ejMViYdmyZWzfvv2c+5w7dy6rVq0iJiaG999/n5YtW5Y4HhQUxJVXXlmhOkXk3FwaE8UNl7YF4N/Ld7Fm6wmTKxIRERERkTNVS2CQmZmJv79/iddSU1M5cOAAQUFB9OjRw/16REQEvr6+JCcnn3P/2dnZrFy5EoBRo0aVOt66dWv69u0LwJIlS8653+nTpwMwfvx4bDbbOZ8nItVjyIWtuLJXc1zAx4u2smV/itkliYiIiIjIf1XLXbKfnx+ZmZkUFhbi5XV6b/UNGzYAEBsbW6p9cZtztW3bNgoKCrDb7SXChzP17t2bX3/9lfj4+HPq8+DBg+zcuROLxcIFF1xAfHw8X3/9NQcPHsTPz4/Y2FhGjBhBWFhYhWoVkXNnGAZjBnQgPaeA9duTeHtuAk/c3ItWkYFmlyYiIiIi0uBVywyDtm3b4nK5+Omnn9yvLV68GMMw6N27d4m2ubm5ZGZmlnp84Wz27dsHQFRUVLlhQ/HjBMVt/0hiYiIAISEhfPHFF4wePZpZs2bx22+/8f333/Paa69xzTXXsHr16nOuU0QqzmIxuHtIVzq3DCG/wMHrs+NISs0xuywRERERkQavWmYYXHXVVcTFxfH3v/+dvXv3kpyczHfffYfFYmHQoEEl2iYkJOByuWjevPk595+eng5AcHBwuW2KjxW3/SNJSUkAZGRk8Oqrr3L55Zfzl7/8hZYtW7Jv3z4mTZrE6tWreeCBB/jmm2+IjIw853rLYrNVPJsp3gajOrbDEKnLbDYLD4+KZdKM9Rw8kcXkWfE8fXsfggO8zS5NqkBjmIh4Mo1hIuLJqmvX8moJDG655RYWLlzIjh07eP3113G5XO7XW7RoUaLtsmXLMAyj1BaMZ5Ofnw+c/VEGu91eou0fyck5/QlmUVERLVu25O2333b336lTJ95//32uuuoqkpOT+fTTT3n88cfPud7fs1gMQkP9/7hhOYKCfCt9roinCAWev/di/jplJSdScnjjq81MGn8xfj4Ve4RJ6h6NYSLiyTSGiUhDVi2Bgbe3NzNnzuTTTz8lLi6OwMBArrjiCoYMGVKiXUFBAevWraNp06b069evQv0DFBYWltumoKCgRNtz7RNg7NixpcIIX19fbrrpJqZMmcLKlSurFBg4nS4yMio+xdpqtRAU5EtGRi4Oh7PS1xfxFAbw6E2xPD99HXsOp/Pcx6t59KZYbPp0xyNpDBMRT6YxTEQ8WXCwr3unwqqotq0B/P39mTBhwlnb2O12FixYUOG+z+Vxg3N5bOFMQUFB7v9u165dmW2KXz98+PA59Xk2RUWV/4vG4XBW6XwRT9I4yIeHR8bwr5mb2LIvhffnJzLu+m5YqmteldQ6jWEi4sk0homIJ/rvpP8q84iP7Vq3bg3A0aNHy51lcPDgwRJt/0jbtm3d/13eow7FsxCcTv0lIVKb2jQN4r7h0VgtBmu3JfHv73e5H3USEREREZHaUSOBQVZWFmvXrmXx4sUsXryYtWvXkpWVVen+unTpgpeXFwUFBWzevLnMNmfbxrEsXbt2xcfHB4BDhw6V2aY4hKjqgociUnHRbRpx5+AuACxff5jFaw6aXJGIiIiISMNSbY8kAO5FD1euXFnqU3mLxcJll13GQw89RKdOnSrUb0BAAP369ePHH39k9uzZpbZq3L9/v3v7w4EDB55Tn76+vlxxxRUsXryY+fPnM3LkyBLHXS4X8+bNA6Bv374VqldEqseF3SLJyC5g1g+7+WrFHoL87PTr0dTsskREREREGoRqm2GwbNkyRo0axU8//YTD4cDlcpX4x+Fw8OOPPzJq1Cj+85//VLj/CRMmYBgGCxYsYNasWe7pyUlJSUycOBGn08mAAQPo3LlzifP69+9P//79WbJkSak+77//fmw2G+vXr+edd97B4XAAp3dOeOWVV9i+fTve3t7cfvvtFf+CiEi1uOb8lgw8vyUA0xdvJ373SZMrEhERERFpGAxXNTwYfOjQIQYPHkxBQQHNmjXjrrvu4uKLL3ZP5T9+/DirVq3ik08+4fDhw3h7e7No0aJSWy7+kenTp/Pyyy/jcrlo2rQpoaGh7N69m4KCAtq0acPMmTMJCwsrcU7xbIaXXnqJ4cOHl+pz3rx5PPXUUzgcDsLCwmjevDkHDx4kLS0NLy8vXn755VK7PVSUw+EkJSW7wufZbBZCQ/1JTc3WYjvSoDldLj5ZtI3fthzHbrPwlzE9adfs3BY4FfNoDBMRT6YxTEQ8WViYP9Zq2GmsWmYYfPLJJxQUFBAbG8vChQsZM2YMLVu2xG63Y7fbadmyJWPGjGHhwoXExsZSUFDAtGnTKnyd22+/nWnTpnHppZeSm5vL7t27iYqK4t577+Xrr78uFRacixtuuIFZs2YxcOBALBYL27Ztw8vLiyFDhvDVV19VOSwQkaqzGAZ3XNuZ6LZhFBQ5eWNOPMdOVTyEExERERGRc1ctMwyuueYaDh48yPz58/9wfYIdO3YwdOhQWrVqxdKlS6t6aY+gGQYi1SOvoIhXvtzEvmOZNAry5slb+xAa6G12WVIOjWEi4sk0homIJ6tTMwyOHz+Ov7//OS1m2KlTJwICAjh+/Hh1XFpEGhAfu42HRsbQJMyPUxn5TJ4dR3Ze2VutioiIiIhI1VRLYGCz2SgqKjqnti6Xi8LCQmy2at2gQUQaiCA/O4+OiiE4wM6R5GymfLWZgkKH2WWJiIiIiNQ71RIYtGrVivz8fFauXPmHbVeuXEl+fj6tWrWqjkuLSAPUOMSXiaNi8fW2svNwOh8s3ILTWeWnq0RERERE5AzVEhj0798fl8vF008/zZ49e8ptt3v3bp555hkMw+DKK6+sjkuLSAPVIiKAB4b3wGY12LTrJDOW7aAalmQREREREZH/qpZFD7Oyshg8eDAnTpzAy8uLgQMHcuGFF9KkSRPg9BoHv/32G0uXLqWwsJDIyEgWLVpEQEBAld+AJ9CihyI1Z/32JN6bn4gLuP7i1gy7pK3ZJcl/aQwTEU+mMUxEPFl1LXpYLYEBwK5du7j33ns5cuQIhmGU2cblctG8eXPee+89OnToUB2X9QgKDERq1o8bDzNj2U4Abr2mE1f0bGZyRQIaw0TEs2kMExFPVqd2SQDo0KEDCxcuZOLEiXTp0gWLxYLL5cLlcmGxWOjSpQuPPfYYCxYsaFBhgYjUvCt6Nee6i1oD8PmyHWzYkWRuQSIiIiIi9UC1zTD4vcLCQtLT0wEIDg7Gy8sLgMzMTP70pz9hGAZz586tiUvXOZphIFLzXC4Xny7Zwc/xR7FZLTw6OoZOLUPNLqtB0xgmIp5MY5iIeLI6N8Pg97y8vGjcuDGNGzd2hwUARUVFbNu2jW3bttXUpUWkATIMg1uv6UjPDo0pcjh56+sEDidlmV2WiIiIiIjHqrHAQESktlktFu65vhsdmgeTm1/Ea7PjOJmWa3ZZIiIiIiIeSYGBiNQrdi8rD47oQbPG/qRnFfDa7HgycwrMLktERERExOMoMBCResffx4tHRsUQFuTNiZQc3pizmfwCh9lliYiIiIh4FAUGIlIvhQX5MHFULP4+NvYdy+Dd+YkUObRolYiIiIjIuVJgICL1VlRjfx4aGYPdZiFh7ymmL95ODW0MIyIiIiJS7ygwEJF6rX2zYO4dFo3FMPg18ThfrdhjdkkiIiIiIh5BgYGI1Hux7Rtz26BOACxec5Bl6w6ZXJGIiIiISN1nq8xJXbp0qe46RERq1CU9osjILuDrn/by7+93EeTvRd+ukWaXJSIiIiJSZ1VqhoHL5arSPyIiZri2byuu7N0cgE8WbWPLvhSTKxIRERERqbsqNcPg/vvvr+46RERqnGEYjBnQgYzsAtZtT+LteQk8fnNPWkcGmV2aiIiIiEidY7j0kX+NczicpKRkV/g8m81CaKg/qanZFBVpOziR6lJY5OSNOfFsO5BKoJ8XT97amyahfmaXVe9oDBMRT6YxTEQ8WViYP1Zr1Zcs1KKHItLgeNks3D+8Oy2bBJCZU8jkWXGkZ+WbXZaIiIiISJ2iwEBEGiRfbxuPjIwhPMSH5LQ8Xp8dT25+kdlliYiIiIjUGQoMRKTBCg7wZuLoWIL8vDiYlMXbcxMo1LRTERERERFAgYGINHBNQv14eFQM3nYr2w6k8vGirTi1tIuIiIiIiAIDEZHWkUHcf0N3rBaDdduT+HL5Lm0BKyIiIiINngIDERGgW5sw/jykCwDfbzjMd6sPmFyRiIiIiIi5FBiIiPxX366R3HRlBwC+/mkvKzcfNbkiERERERHzKDAQETnD1ee1YNAFLQH4dPEO4nafNLkiERERERFzKDAQEfmdEZe346LoSJwuF+/PT2T3kXSzSxIRERERqXUKDEREfscwDG4f1JnubRtRUOTkzTnxHD2ZbXZZIiIiIiK1SoGBiEgZbFYLE4ZF0zYqiOy8IibPjiMlI8/sskREREREao0CAxGRcnjbrTw0ogeRYX6kZOTz+ux4svMKzS5LRERERKRWKDAQETmLQD87E0fHEBJg58jJbN76ajMFhQ6zyxIRERERqXEKDERE/kDjYF8mjorF19vGrsPpfLBwCw6n0+yyRERERERqlAIDEZFz0DwigAdv7I7NamHTrpPMWLoTl8tldlkiIiIiIjVGgYGIyDnq1DKUe67vimHAz/FHmb9yn9kliYiIiIjUGAUGIiIV0LtTBLdc3QmAb37dz48bD5tckYiIiIhIzVBgICJSQVf0bMb1F7cG4PNlO1m/PcncgkREREREaoACAxGRShjarw2XxUbhAj78ZgvbD6SaXZKIiIiISLVSYCAiUgmGYXDr1Z3o1TGcIoeLKXM3c/BEptlliYiIiIhUGwUGIiKVZLEY3HN9Vzo2DyY338Hrs+M5mZZrdlkiIiIiItVCgYGISBV42aw8OKIHzcL9Sc8u4LXZ8WTkFJhdloiIiIhIlSkwEBGpIj8fLyaOiqVRkDcnUnJ4c048eQVFZpclIiIiIlIlCgxERKpBaKA3E0fH4u9jY9+xTN6dl0iRw2l2WSIiIiIilabAQESkmjRt5M/DI2Ow2ywk7kth2nfbcbpcZpclIiIiIlIpCgxERKpRu2bBjB8WjcUw+G3Lcb5ascfskkREREREKkWBgYhINYtp35g7ru0MwJI1B1m69qDJFYmIiIiIVJwCAxGRGnBx96aMuLwdALN+2M1vW46bXJGIiIiISMUoMBARqSGDLmjJgD7NAZj67TYS950yuSIRERERkXOnwEBEpIYYhsFNV3bg/C4ROJwu3pmbyL5jGWaXJSIiIiJyThQYiIjUIIth8OfBXenaOpT8Qgevz47nREqO2WWJiIiIiPwhBQYiIjXMy2bhvhu606pJIFm5hbw2K470rHyzyxIREREROSsFBiIitcDX28bDo2KICPHlZHoek2fHk5NXZHZZIiIiIiLlUmAgIlJLgv3tTBwdQ5CfF4eSsnh77mYKi5xmlyUiIiIiUiYFBiIitSgi1I9HRsXibbey/WAaHy3aitPpMrssEREREZFSFBiIiNSyVpGB3D+8O1aLwfrtScxcvhOXS6GBiIiIiNQtCgxEREzQrXUYdw3pCsAPG4+w6LcDJlckIiIiIlKSAgMREZNc0LUJYwZ0AGDez3v5Of6oyRWJiIiIiPyPAgMRERNd1acF1/ZtBcCnS7YTt+ukyRWJiIiIiJymwEBExGQ3XtaWi7tH4nLBewsS2X043eySREREREQUGIiImM0wDG4b2Jke7RpRWOTkza/iOZKcZXZZIiIiItLAKTAQEakDbFYL44dF0y4qiOy8IibPjiclI8/sskRERESkAVNgICJSR3h7WXloZAxNG/mRmpnP5NnxZOUWml2WiIiIiDRQCgxEROqQAF8vJo6KJSTAztGT2bz11WbyCx1mlyUiIiIiDZACAxGROqZRsA8TR8fi521j95F0PliwBYfTaXZZIiIiItLAKDAQEamDmocH8OCIHtisFuJ2n+SzJTtwuVxmlyUiIiIiDYgCAxGROqpjixDuHdoNw4CVm48xb+U+s0sSERERkQZEgYGISB3Wq2M4t17TCYBFv+7n+w2HTa5IRERERBoKm9kFVNTq1auZNm0a8fHx5OTkEBUVxcCBAxk3bhx+fn4V6uuJJ55g3rx5Z23z0Ucfcemll1alZBGRKrk8thkZWQXM/2UfM/+zkyB/O+d1jjC7LBERERGp5zwqMJgxYwYvvvgiLpeLyMhImjZtyu7du3nvvfdYtmwZM2fOJCQkpML9Nm3alKZNm5Z5LDg4uIpVi4hU3XUXtyYtu4AVm47w0TdbCPD1okurULPLEhEREZF6zGMCg8TERCZNmgTAc889x6hRozAMgxMnTjB+/Hi2bNnC008/zZQpUyrc94033sgDDzxQ3SWLiFQbwzC45aqOZGYXsGFnMlO+3swTY3vRskmg2aWJiIiISD3lMWsYvPvuuzidToYOHcro0aMxDAOAJk2aMHnyZCwWC8uWLWP79u0mVyoiUjMsFoNx13elY4sQ8gocvD47nuS0XLPLEhEREZF6yiMCg+zsbFauXAnAqFGjSh1v3bo1ffv2BWDJkiW1WpuISG3ysll58MbuNA/3Jz27gMmz4sjIKTC7LBERERGphzzikYRt27ZRUFCA3W6nR48eZbbp3bs3v/76K/Hx8RXuf82aNezatYu0tDSCgoLo1q0b119/Pc2aNatq6SIi1c7Px4tHRsUyacYGTqTm8sbseP56c0987B4xpIuIiIiIh/CI3y737Tu993hUVBReXl5ltmnZsmWJthWxbt26Ev//n//8h3feeYeHHnqIu+++u8L9lcVmq/hkDqvVUuLfIiLFwkN9+evYnjw/fT37j2fy7vxEJo6OxVaHxguNYSLiyTSGiYgn++8T/FXmEYFBeno6cPYdC4qPFbc9F61ateKJJ56gb9++NGvWDLvdzo4dO5g6dSpLlizh1Vdfxc/Pj7Fjx1apfovFIDTUv9LnBwX5Vun6IlI/hYb68+zdfXnq/V9J3JvCZ0t38siYXlgs1fQ3RDXRGCYinkxjmIg0ZB4RGOTn5wOUO7sAwG63l2h7LsaPH1/qtZiYGN58803++c9/MnPmTN544w2GDRuGv3/lb/idThcZGTkVPs9qtRAU5EtGRi4Oh7PS1xeR+isiyJv7h3fnjdnxrNh4GF+7hTEDOppdFqAxTEQ8m8YwEfFkwcG+WCxVnyHlEYGBt7c3AIWFheW2KSgoKNG2qiZOnMicOXPIyMhg9erVXHnllVXqr6io8n/ROBzOKp0vIvVbt9Zh3D6oM598u43Fqw8S6Gtn4AUtzS7LTWOYiHgyjWEi4olcrurpxyMeyjqXxw3O5bGFiggMDKRDhw4AHDhwoFr6FBGpKRd3b8rIK9oBMPvH3fyaeMzkikRERETE03lEYNC6dWsAjh49Wu4sg4MHD5ZoWx2KH4EoKiqqtj5FRGrKwPNbcvV5LQCY9t12EvaeMrkiEREREfFkHhEYdOnSBS8vLwoKCti8eXOZbTZs2ABAbGxstVyzqKiIvXv3AhAZGVktfYqI1CTDMBjVvz19uzbB4XTx7rxE9h7NMLssEREREfFQHhEYBAQE0K9fPwBmz55d6vj+/ftZvXo1AAMHDqyWa86aNYvMzExsNht9+/atlj5FRGqaxTC4c3AXurUOJb/QwRtz4jmeUvFFV0VEREREPCIwAJgwYQKGYbBgwQJmzZqF67+rOCQlJTFx4kScTicDBgygc+fOJc7r378//fv3Z8mSJSVeX7VqFa+88gr79+8v8XpBQQEzZszgpZdeAuCmm24iIiKi5t6YiEg1s1ktTLihO60iA8nKLWTyrDjSss59BxkREREREQDD5aqu9RNr3vTp03n55ZdxuVw0bdqU0NBQdu/eTUFBAW3atGHmzJmEhYWVOKdTp04AvPTSSwwfPtz9+vLly7nvvvsAaNy4MU2aNAFg37595OSc/jTummuu4dVXX3Vv2VhZDoeTlJTsCp9ns1kIDfUnNTVbq/OKSIVlZBcw6fMNJKXm0jw8gCfG9sLPp/Y2x9EYJiKeTGOYiHiysDB/rNaqzw/wmBkGALfffjvTpk3j0ksvJTc3l927dxMVFcW9997L119/XSosOJtu3boxYcIELrroIry9vdm3bx87d+4kMDCQq6++mvfee4+33nqrymGBiIhZgvztTBwdS5C/ncPJWUz5ejOFRQ6zyxIRERERD+FRMww8lWYYiIiZDhzP5P9mbiSvwEHvTuGMHxqNxWLU+HU1homIJ9MYJiKerEHOMBARkYprFRnIA8O7Y7MabNiRzBfLd6KsWERERET+iAIDEZEGoEvrMO4a0hUD+HHjERb9ut/skkRERESkjlNgICLSQJzfpQljBnQAYN7KffwUd8TkikRERESkLlNgICLSgAzo04LBF7YC4LOlO9i0M9nkikRERESkrlJgICLSwAy/tC39ejTF5YL3F25h1+E0s0sSERERkTpIgYGISANjGAa3DexETLtGFBY5eXPOZo4kZ5ldloiIiIjUMQoMREQaIKvFwr3DomnXLIic/CImz47nVHqe2WWJiIiISB2iwEBEpIHy9rLy0IgYmjbyIzUzn8mz48jKLTS7LBERERGpIxQYiIg0YAG+Xjw6OpbQQG+Oncrhza/iyS90mF2WiIiIiNQBCgxERBq4sCAfJo6Kwc/bxp4jGbw/PxGH02l2WSIiIiJiMgUGIiJCs/AAHhzRAy+bhfg9p/h0yQ5cLpfZZYmIiIiIiRQY1FFOl5MdKbv55cA6dqTsxunSp30iUrM6tgjh3qHdMAz4ZfMx5v681+ySRERERMRENrMLkNLikhKYs2shafnp7tdCvIMZ2eF6YiO6m1iZiNR3PTuEc9vAzkxfvJ1vfztAsL+dAX1amF2WiIiIiJhAMwzqmLikBD5KnFEiLABIy0/no8QZxCUlmFSZiDQUl8ZEccMlbQD4cvku1m47YXJFIiIiImIGBQZ1iNPlZM6uhWdt89WuhXo8QURq3JCLWtO/VzNcwMeLtrJtf4rZJYmIiIhILVNgUIfsTttXambB76Xmp7M7bV8tVSQiDZVhGNw8oCN9OoVT5HAxZW4CB45nml2WiIiIiNQiBQZ1SEZ+xjm1W3HoFw5lHtUK5iJSoywWg7uv60qnFiHkFTh4fU48SWm5ZpclIiIiIrVEgUEdEuQddE7t4k9u4eV1b/Dsb//H3N2L2Jd+QI8piEiN8LJZeeDGHrSICCAju4DJs+LIyC4wuywRERERqQWGSx9T1ziHw0lKSvYftnO6nDz960tnfSzBz+ZL++A2bEvdSaGzyP16iHcwMeHdiA2Ppl1wG6wWa7XULiICkJaVz6QZGziZnkeryED+OqYnvt5n32jHZrMQGupPamo2RUUKNUXEs2gMExFPFhbmj9Va9fkBCgxqwbkGBvC/XRLKc3f0rcRGdCevKJ+tKTuIS0og8dQ28h3/+8QvwMufHo27EhvRnU6h7bFZtHumiFTd8ZQcJs3YQFZuId1ah/LQyBhsZ/mLSL9si4gn0xgmIp5MgYEHqUhgAKdDgzm7FpaYaRDqHcyIDtcTG9G9VPtCRyHbU3cRl5xIQvJWsoty3Md8rD50b9yF2PBoujbqhN1qr9qbEZEGbe/RDF75chP5hQ4u6NqEu6/risUwymyrX7ZFxJNpDBMRT6bAwINUNDCA048n7MvcT5GtAFuRnTaBrbEYf/wH7nA62JW2l7jkROKTE8ko+N+q5l4WL7o16kRseHeiG3fG1+Zb4fciIpK49xRvfrUZh9PF1ee1YHT/9hhlhAb6ZVtEPJnGMBHxZAoMPEhlAgOo+l9UTpeTfekHiUtOIC45kZS81P/1bVjpFNaB2PDu9GjclQC7f4X7F5GG67fE43y0aCsAI69ox6ALWpVqo1+2RcSTaQwTEU+mwMCDmBUYnMnlcnEo6whxSYnEJSdwIifZfcxiWGgf0pbY8GhiwrsR4h1cpWuJSMOwZM1BZv+4G4A/D+7Cxd2bljiuX7ZFxJNpDBMRT6bAwIPUhcDg945ln3CHB4ezjpY41iaoFbER0cSGd6exb1i1XldE6pdZP+xi6dpDWAyDB0f0oEe7Ru5j+mVbRDyZxjAR8WQKDDxIXQwMznQy9xRxyYnEJSWyL+NAiWMtAqKICe9Oz4hoIv2b1FgNIuKZnC4XHy/ayuotJ7B7WfjLmJ60izo9S0m/bIuIJ9MYJiKeTIGBB6nrgcGZ0vLTiU/eQlxSArvS9uLif98eTfwi6BkeTUxENC0CmpW5yJmINDxFDidvfrWZLftSCPD14m+39KJpI3/9si0iHk1jmIh4MgUGHsSTAoMzZRZkkXByK3HJiWxP2YXD5XAfa+QTSkx4ND0jutM6qOU57eAgIvVXXkERr3y5iX3HMmkU5MMTY3uRkplHocvAy3DRLioYi0Uho4h4DrN/DxMRqQoFBh7EUwODM+UW5ZJ4cjtxyQlsObWDQmeh+1iwPZCY8GhiwqPpENIWq8VqYqUiYpaMnAJemrGBE6m5WC0GDuf//noJDfTm5gEd6N0pwsQKRUTOXV36PUxEpKIUGHiQ+hAYnKnAUcDWUzvYlJxA4snt5Dny3Mf8bX50D+9Kz/DudArrgJfFZmKlIlLbftx4mBnLdpZ7/L4bohUaiIhHqKu/h4mInAsFBh6kvgUGZyp0FrEzdTdxSQnEn9xCdmGO+5iP1Zvoxl2ICY+mW6POeFvtJlYqIjXN6XTxl/d+JTUzv9w2YYHe/Gv8RXo8QUTqPE/4PUxEpDzVFRjo41+pEi+LjW6NOtOtUWducg5nT/o+944L6QUZrD8Rx/oTcXhZbHQN60RsRHeiG3XBz8vX7NJFpJrtPJR21rAAICUzn52H0ujcKrSWqhIRERGRylJgINXGarHSMbQ9HUPbM6LD9ezPOERccgJxSYmcyksh/uQW4k9uwWpY6RTantjwaHqEdyPQHmB26SJSDdKyzx4WVLSdiIiIiJhLgYHUCIthoW1wK9oGt+KGdoM5nHXsdHiQnMjx7BNsTdnB1pQdfLljLu1D2hAb3p2Y8G6E+oSYXbqIVFKIv3e1thMRERERcykwkBpnGAYtAqNoERjFdW2v4Xh20unHFpITOJR5hF1pe9mVtpc5uxbQOqglseHRxIZ3J9yvkdmli0gFdGwRQmig9x+uYdCxRUjtFSUiIiIilaZFD2tBfV70sKpO5ab8NzxIZF/6AVz879uxWUBTd3jQ1L8JhqFF0kTqug07knhnXmK5x7VLgoh4iobwe5iI1F/aJcGDKDA4N+n5GcQnbyEuOYFdaXtxuv73niP8GhMb3p3Y8GhaBjZXeCBSh23YkcTM5btKzDQIC/RmzIAOCgtExGM0tN/DRKR+UWDgQRQYVFxWYTYJyVuJS05ke8pOilwO97FQ7xBiI07PPGgb3AqLUfUfBBGpXk6niz1H0yl0GXgZLtpFBWsrRRHxKA359zAR8XwKDDyIAoOqyS3KY8up7cQlJbDl1HYKnIXuY4H2AGIadyM2ojsdQ9phtVhNrFREzqQxTEQ8mcYwEfFkCgw8iAKD6lPgKGRbyg7ikhNJOLmV3KI89zE/my/dG3clNjyaLmEd8bJ6mVipiGgMExFPpjFMRDyZAgMPosCgZhQ5i9iZuoe45ATik7eQVfi/r7G31U63Rp2JDe9Ot0ad8LH5mFipSMOkMUxEPJnGMBHxZAoMPIgCg5rndDnZk7bPveNCWn66+5jNYqNLWEdiw6Pp0bgrfl5+JlYq0nBoDBMRT6YxTEQ8mQIDD6LAoHY5XU4OZh4mLimRTckJnMw95T5mMSx0DGlHbER3YsK7EWQPNLFSkfpNY5iIeDKNYSLiyRQYeBAFBuZxuVwczT5OXFICccmJHM0+7j5mYNA2uDU9/xsehPmEmlipSP2jMUxEPJnGMBHxZAoMPIgCg7rjRE4y8UmnH1s4kHmoxLGWgc3pGd6dmIhomviFm1ShSP2hMUxEPJnGMBHxZAoMPIgCg7opJS+V+OQtbEpKYG/6flz870chyj+S2PBoYiO6E+UfiWFo/3iRitIYJiKeTGOYiHgyBQYeRIFB3ZdRkEl88hbikxPZkbobp+t/X+9w30bEhncnNiKaVoEtFB6InCONYSLiyTSGiYgnU2DgQRQYeJbswhwST25jU3IC21J2UuQsch8L8Q4mJjyanuHRtAtpg8Wo+g+hSH2lMUxEPJnGMBHxZAoMPIgCA8+VV5TPllPbiU9OJPHUNvIdBe5jAV7+xIR3Iza8Ox1D22Gz2EysVKTu0RgmIp5MY5iIeDIFBh5EgUH9UOgoZHvqLjYlJZBwcis5RbnuY742X7o37kJseDRdwjpht3qZWKlI3aAxTEQ8mcYwEfFkCgw8iAKD+sfhdLAzbQ9xyYnEJyeSWZDlPma3eNGtUWdiI7rTrVFnfG0+JlYqYh6NYSLiyTSGiYgnU2DgQRQY1G9Ol5O96QeIS04gLimR1Pw09zGbYaVzWEdiw6PpHt6VAC9/8woVqWUaw0TEk2kMExFPpsDAgygwaDhcLhcHMw8Tl5xIXFICSbkn3ccshoUOIW2JDe9OTHg3gr2DTKxUpOZpDBMRT6YxTEQ8mQIDD6LAoGFyuVwcyz5xeuZBciJHso65jxkYtAluRWx4NLHh0TTyDTOxUpGaoTFMRDyZxjAR8WQKDDyIAgMBSMo5SXxyInHJiezPOFjiWIvAZsSGdyc2PJpI/wiTKhSpXhrDRMSTaQwTEU+mwMCDKDCQ30vNSyM+eQtxyQnsTtuHi//9GEb6N/nvzIPuNA9oimEYJlYqUnkaw0TEk2kMExFPpsDAgygwkLPJLMhic/IW4pIT2ZG6G4fL4T7W2CeMmIjT4UHroBZYjKr/0IvUFo1hIuLJNIaJiCdTYOBBFBjIucopzCXx1DbikhLYmrKDQmeR+1iwPYiY8Gh6RkTTLrgNVovVxEpF/pjGMBHxZBrDRMSTKTDwIAoMpDLyHQVsObWd+OREEk9uI8+R7z4W4OVPj8ZdiQmPplNYB7wsNhMrFSmbxjAR8WQaw0TEkykw8CAKDKSqCh2F7EjdzabkBBKSt5JdlOM+5mP1IbpxZ3qGd6dLo054W+0mViryPxrDRMSTaQwTEU+mwMCDKDCQ6uRwOtiVtpf45ETikxNJL8h0H/OyeNGtUSdiwqPp3rgLvjZfEyuVhk5jmIh4Mo1hIuLJFBh4EAUGUlOcLif7Mw6yKSmB+ORETuWluo9ZDSudwtrTM7w73Rt3JdAeYGKl0hBpDBMRT6YxTEQ8mQIDD6LAQGqDy+XiUNYR4pMS2ZScyImcJPcxA4MOIW3/u+NCNCHewSZWKg2FxjAR8WQaw0TEkykw8CAKDMQMx7NPsCkpkfjkBA5lHS1xrE1QS2IjuhMbHk1j30YmVSj1ncYwEfFkGsNExJMpMPAgCgzEbCdzTxGXnEhcUiL7Mg6UONY8IIrY8GhiI7rT1L+JSRVKfaQxTEQ8mcYwEfFkCgw8iAIDqUvS8tOJT95CXHIiu1L34OJ/Q0ATv4j/hgfRtAhohmEYJlYqnk5jmIh4Mo1hIuLJFBh4EAUGUldlFWSz+eRW4pIT2J6yC4fL4T7WyCeUmPBoYsO70ya4JRaj6gOONCwaw0TEk2kMExFP1mADg9WrVzNt2jTi4+PJyckhKiqKgQMHMm7cOPz8/Krc/xdffMFzzz0HwPnnn8+MGTOq3KcCA/EEuUW5JJ7cTlxyIltPbafAWeg+FmQP/G94EE2HkLZYLVYTKxVPoTFMRDyZxjAR8WQNMjCYMWMGL774Ii6Xi8jISMLCwti9ezcFBQW0a9eOmTNnEhISUun+T5w4wbXXXktWVhagwEAargJHAVtTdhKXlEDCyW3kOfLcx/xtfnQP70pseDSdwzriZbGZWKnUZRrDRMSTaQwTEU9WXYGBx/ymn5iYyKRJkwB47rnnGDVqFIZhcOLECcaPH8+WLVt4+umnmTJlSqWv8eyzz5Kbm8sVV1zBjz/+WF2li3gcu9V+ei2D8GiKnEXsSN1NXFIim09uIaswm9XH1rP62Hp8rN50a9SZ2IjudA3rhI/N2+zSRURERESkmnhMYPDuu+/idDoZNmwYo0ePdr/epEkTJk+ezKBBg1i2bBnbt2+nc+fOFe7/u+++44cffuBPf/rT/7d378FR1/f+x1+7m90k5LpJNuGqCbeEkIUl9IBVT21TKnJokWORHodxdLy2Tr1Mta1UoNWMA390ahVrpZ1SqNdftYr+rIozIg4HabAxS7JJCBCuuUA29wvktrvnD2SV2QRCsrD5wvMxw0yyn+/3k3cyw5vllc/381FiYiKBAfClKHOUZqbmaGZqjv7H/9+qbjsst7dM7gaP2nrbVdywR8UNe2Q1R2lGSrZcjjw503I1xhob6dIBAAAAjIAhAoOuri7t2LFDkrR8+fKQ8czMTF1zzTX67LPP9OGHH15wYNDW1qann35aY8eO1SOPPKKNGzeGpW7gcmMxWzTdPkXT7VO0bNoSHWk/9uVxjWVq7G5WaWO5ShvLZTaZlW2fqjkOp2Y5ZirBFh/p0gEAAABcIEMEBpWVlert7ZXNZtOsWbMGvGbu3Ln67LPPtGfPnguef926dWpsbNQf/vAHxcXFjbRc4IpgNpmVlXS1spKu1tIp/6Xaznq5vWUq8Xp0vOuEKpv3qbJ5n16rektTk7OCmybaY5IjXToAAACAITBEYHDo0CFJ0vjx42W1Wge85qqrrjrr2qHatWuX3nrrLRUUFGjBggUjKxS4QplMJk1MGK+JCeP1/ckLdbyrQXu8Hrm9ZTraUav9rQe1v/Wg3tz/rq5OnKQ5DqdmO/KUPiYt0qUDAAAAGIQhAoO2tjZJUlJS0qDXnBk7c+1QdHd3a82aNRozZozWrFkzsiLPIyrqwneoPLOrZTh2twQupYlJYzUxaawWT12gplPNKmnwqOREmapbD+tI+zEdaT+mLdXva2L8OLkynMpPd2p8/FiZTKZIl44woocBMDJ6GAAjC9fbakMEBj09PZI06OoCSbLZbGddOxTPPfecjh49qpUrV2rcuHEjK/IczGaT7PbhP+qQmMjmcTAuuz1OU8dP0q1apJZTbfq81q2iGrfKG/apprNeNZ31eq/6I42LT9f8SXM0b4JLU1KuJjy4jNDDABgZPQzAlcwQgUF09Omj2vr6+ga9pre396xrz6eiokKbN29Wbm6ubr/99pEXeQ5+f0Dt7Scv+D6LxazExFi1t5+Sz8f5v7gcROk/Ur+h/0j9hjp7u1TqrVBJQ5kqmvapvrNBWyq3akvlVtljkjUn3an8DKemJGfKbOK3O0ZEDwNgZPQwAEaWlBQrs3nk76ENERgM5XGDoTy28HVPPPGE/H6/nnrqKVkslpEXeR79/cP/h8bn84/ofmA0ijHHal7GXM3LmKvu/m6VN+1Videj8qa9aulu1bajO7Tt6A4l2OI1O22mXA6nptunyGK++H9fEV70MABGRg8DYESBQHjmMURgkJmZKUmqq6tTX1/fgI8mHD169Kxrz6eiokIWi0U//vGPQ8ZOnjy9GqCkpETXXXedJOnNN9+8qI8tAFeymKgYzc1waW6GS72+PlU275PbW6ayxgp19Hbqf+uK9L91RYqNitWstFy5HHnKSZkum2Xwx5QAAAAAjIwhAoMZM2bIarWqt7dXpaWlmjt3bsg1xcXFkiSXyzXkeX0+nxobGwcd7+vrC477fL4LKxrAsNgsVs12zNRsx0z1+/u1r6Vabq9Hpd5ydfR1quh4sYqOF8tmsSkvNUcuR55mpuYoJiom0qUDAAAAlxVDBAbx8fG6/vrr9cknn+jvf/97SGBw+PBh/etf/5Ik3XTTTUOas6qqatCx9evX6/nnn9e8efP00ksvDb9wACMSZY5Sbmq2clOz9T/Z/63q1sNye8vk9nrU2tOmLxpK9UVDqaLMUZqRMk0uh1POtFzFWcdEunQAAADA8AwRGEjSAw88oO3bt+udd95Rfn6+li9fLpPJpIaGBv3sZz+T3+/XggULlJOTc9Z9BQUFkqRf/OIXQw4TAIw+ZpNZ0+yTNc0+WcumLdGRjmNyN3jk9pbJe6pJZY2VKmuslNlk1vTkKXKl52lWWp6SohMiXToAAABgSIYJDGbNmqXHH39c69at05o1a/THP/5RdrtdBw4cUG9vr7KyslRYWBhyX21traSv9iUAYHwmk0mZiVcpM/Eq3Txlkeq6jsvdcHrlQV3Xce1t2a+9Lfv1/6q2aHLS1XKlOzU7LU+psfZIlw4AAAAYhmECA0m68847lZ2drY0bN6q0tFRNTU0aP368brrpJt13332Ki4uLdIkALjGTyaQJ8eM0IX6cFk++USdOerXH65G7waMjHcdU3XZY1W2H9Y/9/19XJUyUy5EnV7pTGWMckS4dAAAAGNVMgUC4DlzAYHw+v5qbuy74vqgos+z2OLW0dHGcDzAMzd0t2uMtl9tbpurWwwroq3Y3Li5DLodTLkeeJsSPk8lkimCllyd6GAAjo4cBMLKUlDhZLOYRz0NgcAkQGACR197boVJvudxej6paDsgf+OrvVFps6umVBw6nrk6cKLNp5M0V9DAAxkYPA2BkBAYGQmAAjC4n+06qrLFSbq9Hlc1V6vP3B8eSo5M025EnlyNPU5OzCA9GgB4GwMjoYQCMjMDAQAgMgNGru79HFc1VcjeUydNUqR5fb3As3hqnWWkz5Up3Kts+RVFmQ237EnH0MABGRg8DYGQEBgZCYAAYQ5+vT3tb9svd4FFpY7lO9p8KjsVGxSgvNVdz0vM0I2W6bBZbBCs1BnoYACOjhwEwMgIDAyEwAIzH5/dpf+tBlXjLtMfrUUdvZ3DMZrZqZmqOXI48zUybodiomAhWOnrRwwAYGT0MgJERGBgIgQFgbP6AXwfbjmiP16OShjK19LQGx6JMFuWkTJPL4ZTTkat4K8e7nkEPA2Bk9DAARkZgYCAEBsDlIxAI6FhHrUq8ZXJ7y9RwsjE4ZjaZNS15slyOPM125CkpOjGClUYePQyAkdHDABgZgYGBEBgAl6dAIKD6rhNye8vk9npU21kfHDPJpKykq+RyODXbkae02JQIVhoZ9DAARkYPA2BkBAYGQmAAXBm8J5vk/nLPg0PtR88amxQ/Xq50p1wOp8bGpUeowkuLHgbAyOhhAIyMwMBACAyAK09Ld6v2NJbL3VCmA62HFNBXrXbsmPQvw4M8TYwfL5PJFMFKLx56GAAjo4cBMDICAwMhMACubB29nSptLJe7waOqlgPyBXzBsdSYFLkceXKlO5WZOElm08gb+2hBDwNgZPQwAEZGYGAgBAYAzjjZd0qepkq5vR5VNFWpz98XHEuyJWq2I08uR56mJmfJYrZEsNKRo4cBMDJ6GAAjIzAwEAIDAAPp8fWqoqlKbm+ZPI2V6vb1BMfirGM0K22mXI48ZadMk9UcFcFKh4ceBsDI6GEAjIzAwEAIDACcT5+/X1XN++X2elTaWK6uvpPBsRhLjPLScuRyOJWbmq1oiy2ClQ4dPQyAkdHDABgZgYGBEBgAuBA+v08HWg/J7fVoj7dMbb0dwTGr2arc1Gy5HHlyps1QbFRsBCs9N3oYACOjhwEwMgIDAyEwADBc/oBfh9uPyd1QJre3TE3dLcExi8mi7JSpcjnyNCttphJs8RGsNBQ9DICR0cMAGBmBgYEQGAAIh0AgoJrOui/DA4+On2wIjplk0tTkrOBxjcnRSRGs9DR6GAAjo4cBMDICAwMhMABwMRzvOiG31yN3Q5mOddadNZaVeJVmO/I0J92ptNjUiNRHDwNgZPQwAEZGYGAgBAYALrbGU81ye8u0x+vRwbYjZ41NiB+nOQ6nZjvyNC4uQyaT6ZLURA8DYGT0MABGRmBgIAQGAC6l1p42lXrLVeL16EDrQfkDX/WPjDGO0ysPHE5NSphwUcMDehgAI6OHATAyAgMDITAAECmdvV0qbazQHm+Z9jbvV3/AFxxLibHL5cjTbEeeJiddLbNp5P+ofB09DICR0cMAGBmBgYEQGAAYDU71n1J5416VeD2qaNqrXn9fcCzRlqBZjpma43BqWvJkWcyWEX89ehgAI6OHATAyAgMDITAAMNr0+npV0bxP7gaPPE0VOtXfHRyLixojZ1quXOl5yrFPk9ViHdbXoIcBMDJ6GAAjIzAwEAIDAKNZv79fVS3VcjeUqbSxXJ19X/WraItNeakz5Ep3KjclWzFR0UOelx4GwMjoYQCMjMDAQAgMABiFz+9Tddthub0e7fF61NrTFhyzmqM0IyVbLkeenGkzNMY6ZtB5/AG/DnUcVn9Ur6L6bcpKyAz7HgkAcDHxPgyAkREYGAiBAQAj8gf8OtJeI7e3TO6GMjV2NwfHzCazsu1Tg5smJtjig2PuhjK9sf/ds8KG5Ogk3TptiVzpzkv6PQDAcPE+DICRERgYCIEBAKMLBAKq7ayX2+uR21um+q4TwTGTTJqSnCmXwymrOUqvVb016Dz35t1OaADAEHgfBsDICAwMhMAAwOXmRFfDl+GBR0c7aoZ8nz06SU9du5LHEwCMerwPA2Bk4QoMosJQCwDgCpMRl66FcQVamFmgplMt2tPo0a663ar72sqDgbT0tOlA6yFNt0+5RJUCAABguPgVDwBgRFJj7SqY9J9aeHXBkK5v72m/yBUBAAAgHAgMAABhkRidGNbrAAAAEFkEBgCAsJianKXk6KRzXmOPTtLU5KxLVBEAAABGgsAAABAWZpNZt05bcs5rlk1bwoaHAAAABsG7NgBA2LjSnbo37/aQlQb26CSOVAQAADAYTkkAAISVK92pWY6ZOtRxWP1RvYrqtykrIZOVBQAAAAZDYAAACDuzyazslKmcYQ4AAGBg/LoHAAAAAACEIDAAAAAAAAAhCAwAAAAAAEAIAgMAAAAAABCCwAAAAAAAAIQgMAAAAAAAACEIDAAAAAAAQAgCAwAAAAAAEILAAAAAAAAAhCAwAAAAAAAAIQgMAAAAAABACAIDAAAAAAAQgsAAAAAAAACEMAUCgUCki7jcBQIB+f3D+zFbLGb5fP4wVwQAlwY9DICR0cMAGJXZbJLJZBrxPAQGAAAAAAAgBI8kAAAAAACAEAQGAAAAAAAgBIEBAAAAAAAIQWAAAAAAAABCEBgAAAAAAIAQBAYAAAAAACAEgQEAAAAAAAhBYAAAAAAAAEIQGAAAAAAAgBAEBgAAAAAAIASBAQAAAAAACEFgAAAAAAAAQhAYAAAAAACAEAQGAAAAAAAgRFSkC8DZvF6vdu7cKY/Ho7KyMlVWVqqnp0fz5s3TSy+9FOnyAGBQgUBAJSUl2rZtm4qLi3Xw4EF1dnYqISFBubm5Wrp0qX7wgx/IZDJFulQAGNAHH3ygzz77TOXl5WpoaFBra6usVqsyMzN1ww036I477pDdbo90mQAwJJ9++qnuu+8+SdKECRO0bdu2C57DFAgEAuEuDMO3adMmrV27NuR1AgMAo92uXbt05513Bj+fNGmSEhMTVVtbq9bWVknSt7/9ba1fv142my0yRQLAOdx8883au3evbDabHA6H7Ha7mpubVVdXJ0lKTU3Vxo0blZOTE+FKAeDcurq69P3vfz/Yv4YbGLDCYJSJj4/XtddeK6fTKafTqYqKCr3wwguRLgsAzisQCGjixIm64447tHjxYqWmpgbHtmzZotWrV2v79u169tln9fOf/zyClQLAwFasWKGsrCy5XC5Zrdbg61VVVXrssce0b98+Pfroo/rnP/8ZwSoB4PyeeeYZ1dXV6bvf/a4+/vjjYc/DCoNR7uWXX1ZhYSErDACMep2dnYqOjj7rTfbXvfjii3rmmWeUnJysXbt2yWxmGx0AxlFaWqpbb71VkvT+++9rypQpEa4IAAbmdrt122236Tvf+Y4WLFiglStXDnuFAe/WAABhER8fP2hYIEnf+ta3JEmtra1qbm6+VGUBQFhMnjw5+PGpU6ciWAkADK6vr0+rV69WTEyM1qxZM+L5CAwAAJdEd3d38OOYmJgIVgIAF664uFiSNGbMGGVlZUW4GgAY2IYNG7Rv3z49/PDDGjt27IjnYw8DAMAlceaZ35ycHMXHx0e4GgA4P7/fHzzB6re//a0k6bHHHlNcXFyEKwOAUNXV1dqwYYNmzpyp22+/PSxzEhgAAC46j8ej119/XZKCx/sAwGg10KlVs2bN0rp164KPVwHAaBIIBLRq1Sr19/frySeflMViCcu8PJIAALioGhsb9eCDD6q/v1/f+973tHjx4kiXBADnlJGRofz8fM2ePVsOh0Mmk0mVlZV655131N7eHunyACDEq6++qi+++EIrVqyQ0+kM27ysMAAAXDQdHR269957VVdXp5kzZ2rdunWRLgkAzmvRokVatGhR8PO9e/eqsLBQ7733nqqrq/WPf/wjbL+9A4CROnHihH73u98pIyNDjzzySFjnZoUBAOCi6Orq0j333KOKigpNmzZNf/nLX9i7AIAh5eTkaMOGDbLb7aqsrAzuyQIAo0FhYaE6Ozu1atWqsL/XYoUBACDsTp06pfvvv19ut1uZmZn661//KrvdHumyAGDY4uPjNW/ePG3dulXl5eVasmRJpEsCAElSRUWFJOnJJ5/Uk08+edbYmVOq6uvrdd1110mS1q9fr/z8/CHNTWAAAAirnp4e/eQnP9Hnn3+uCRMmaNOmTXI4HJEuCwBGrL+/X5Lk8/kiXAkAhGpsbBx0zO/3B8f7+vqGPCeBAQAgbPr6+vTggw9q165dysjI0ObNmzVu3LhIlwUAI9ba2qrdu3dLkmbMmBHhagDgK9u2bRt07K233tLKlSs1YcKEc143GPYwAACEhc/n06OPPqpPP/1UDodDmzdv1qRJkyJdFgAMye7du/XCCy+opqYmZKy8vFx33323Ojo6lJGRoZtuuikCFQLApccKg1Gmvr5eS5cuDX7e29srSfriiy80f/784Ov33HOP7r333ktdHgAM6oMPPtDWrVslSTabTb/61a8GvXb16tXKzc29VKUBwHm1t7fr2Wef1bPPPiuHw6H09HRZLBbV19fL6/VKOn3c4oYNGxQXFxfhagHg0iAwGGV8Pp9aW1tDXu/v7z/r9TObVwDAaHEm4JSk2tpa1dbWDnptR0fHpSgJAIZszpw5WrlypYqKinTgwAEdPnxYvb29SkxM1Pz581VQUKBly5Zx2guAK4opEAgEIl0EAAAAAAAYXdjDAAAAAAAAhCAwAAAAAAAAIQgMAAAAAABACAIDAAAAAAAQgsAAAAAAAACEIDAAAAAAAAAhCAwAAAAAAEAIAgMAAAAAABCCwAAAAAAAAIQgMAAAAFes7OxsZWdnq6ioKNKlAAAw6kRFugAAADB6rF+/Xs8///yQr6+qqrqI1QAAgEgiMAAAAANKS0uLdAkAACCCCAwAAMCAdu7cGekSAABABLGHAQAAAAAACMEKAwAAEBYFBQWqra3V2rVrdeONN2rDhg366KOPVF9fr9jYWM2dO1f333+/Zs+ePegcPp9Pb7/9tt59911VVVWpq6tLdrtdc+bM0YoVKzR//vxz1lBfX6+XXnpJO3fuVE1Njfr6+pSenq5p06Zp4cKFWrRokaKjowe8t7OzU3/+85+1detW1dXVKTY2Vi6XSw888MA5awYA4HJFYAAAAMKqvb1dy5Yt06FDh2S1WhUdHa3W1lZ9/PHH+uSTT1RYWKhly5aF3NfR0aEHHnhAu3fvliRZLBbFxcXJ6/Vq69at2rp1q+666y798pe/HPDrbtmyRWvWrFFPT48kyWq1Ki4uTvX19Tp27Ji2bdum7OxszZgxI+Rer9erW265RUeOHFF0dLTMZrNaW1u1fft27dy5Uy+++KKuv/76MP6UAAAY/XgkAQAAhNXzzz+v5uZm/f73v5fb7VZxcbHef/99zZs3T36/X7/+9a9VXl4ect8TTzyh3bt3y2q1atWqVSouLtbnn3+uHTt26Ic//KEkaePGjXrttddC7t2+fbsef/xx9fT0KD8/X6+88opKS0tVVFSkkpISvfLKK1q+fLmsVuuANT/11FOyWq3avHmz3G63SkpK9MYbbygrK0t9fX1as2aN/H5/eH9QAACMcqZAIBCIdBEAAGB0+Pqxiuc7JWHRokVatWpV8PMzjyRI0qZNm/TNb37zrOu7u7t188036/Dhw7rhhhv0pz/9KTi2Z88eLV++XNLp/7z/6Ec/Cvl6Dz30kLZu3Sq73a5PP/00+GhBf3+/Fi5cqJqaGs2dO1ebNm2SzWYb0vebnZ0tSUpJSdF7772n1NTUs8arqqq0ZMkSSdKrr76quXPnDmleAAAuB6wwAAAAA2psbDznn87OzgHvy8/PDwkLJCkmJkZ33323JGnHjh3q6OgIjr3//vuSpLFjx+rWW28dcN6HH35YktTS0nLWCQ5FRUWqqamRJK1cuXLIYcHXLV++PCQskE4HChMnTpR0OjwAAOBKwh4GAABgQMP9D/I111xz3jG/36/y8vLg5x6PR5I0f/58mc0D/z5jypQpysjI0IkTJ+TxeFRQUCBJKikpkSQ5HA45nc5h1XyuTQ3T09NVU1Ojtra2Yc0NAIBRscIAAACEVUZGxpDGmpubgx83NTWd917p9AqEr18vnd6wUJLGjx9/4cV+KS4ubtCxqKjTv1/p7+8f9vwAABgRgQEAADA0k8kU6RIAALgsERgAAICwOnHixJDGUlJSgh+f2T/g+PHj55z7zPjX9xs4szljXV3dhRcLAAAGRWAAAADCqqio6LxjZrNZubm5wdfz8vKC44MdX1hdXR0MHL6+V0F+fr6k048mlJWVjax4AAAQRGAAAADCqri4eMDQoKenRxs3bpQkXX/99UpMTAyOLV68WNLpFQhvvPHGgPM+99xzkiS73a5rr702+Pr8+fM1adIkSdLatWvV29sbnm8EAIArHIEBAAAIq4SEBD300EP68MMPgxsFVldX67777tPBgwdlsVj00EMPnXXPrFmztHDhQklSYWGhXn75ZZ06dUrS6ZUDq1at0ocffijp9PGK0dHRwXstFotWr14tk8mk4uJi3Xnnnfr3v/8dXKnQ29uroqIiPfbYYzpw4MBF//4BALhccKwiAAAY0HXXXXfea9avXx98JOCMn/70p3r99df18MMPy2azKTo6Wh0dHZJOb1D4m9/8ZsDjD59++mm1tLRo9+7dKiws1Nq1axUXF6f29nYFAgFJ0l133aXbbrst5N4bbrhB69at0+rVq1VcXKwVK1bIZrNpzJgx6uzsDAYXd9999wX/HAAAuFIRGAAAgAE1Njae95q+vr6Q1xITE/Xmm29qw4YN+uijj1RfX6/k5GTNmTNH999/v+bMmTPgXAkJCdq0aZPefvttvfPOO6qqqtLJkyeVlpam/Px8rVixQvPnzx+0lqVLl+ob3/iG/va3v2nnzp2qq6tTT0+Pxo8fr+nTp+vGG2/UlClThv4DAADgCmcKnInsAQAARqCgoEC1tbVau3atbrnllkiXAwAARog9DAAAAAAAQAgCAwAAAAAAEILAAAAAAAAAhCAwAAAAAAAAIdj0EAAAAAAAhGCFAQAAAAAACEFgAAAAAAAAQhAYAAAAAACAEAQGAAAAAAAgBIEBAAAAAAAIQWAAAAAAAABCEBgAAAAAAIAQBAYAAAAAACAEgQEAAAAAAAjxf7FHckqcrn9wAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}